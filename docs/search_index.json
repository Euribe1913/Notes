[["index.html", "Stat 255: Statistics for Data Science Notes Preface", " Stat 255: Statistics for Data Science Notes Andrew Sage - Lawrence University 2023-08-13 Preface These notes serve as the primary textual resource for Stat 255: Statistics for Data Science at Lawrence University. What is this course about? Stat 255 provides an introduction to essential statistical tasks including modeling, inference, prediction, and computation. The course employs a modern approach, intended to equip students with skills needed for working with today’s complex data. Traditional concepts, like interval estimation, and hypothesis testing, are introduced through the lens of multivariate models and simulation. Data computation in R plays a central role throughout the course. The course’s overarching learning outcomes are: Visualize and wrangle data using statistical software R. Build and assess multivariate models to predict future outcomes. Quantify uncertainty associated with estimates and predictions. Explain the assumptions associated with statistical models, and evaluate whether these assumptions are reasonably satisfied in context. Write reproducible analyses, using statistical software. Work with data in an ethical and responsible manner. More specific learning tasks, related to these outcomes are provided in each chapter. Who is this course intended for? This course is intended for students who are interested in learning statistical modeling and data computation skills that might prove useful in further courses, research, or career. Stat 255 can serve as either: a first course in statistics for students with a strong quantitative background, typically including calculus. a second course in statistics, building on introductory topics taught in courses like Lawrence’s Stat 107: Principles of Statistics, or AP Statistics. At Lawrence, this course is required for the Statistics Track of the Mathematics Major, the Economics and Mathematics-Economics Majors, the Business Analytics Track of the Business Major, and the Statistics and Data Science Minor. The prerequisite for the course is either 1) a prior college-level course in statistics (i.e. STAT 107, BIOL 170 or 280, ANTH 207, AP Stats) OR 2) Calculus. (Math 140, AP Calculus, or equivalent). The course does not assume any prior knowledge of statistics, but does move more rapidly than a typical introductory statistics course. Students engage rigorously in statistical thinking and computation, intended to equip them with essential skills for further study in statistics and data science. "],["exploratory-data-analysis.html", "Chapter 1 Exploratory Data Analysis 1.1 Getting Started in R 1.2 Data Visualization 1.3 Summary Tables", " Chapter 1 Exploratory Data Analysis Learning Outcomes: Interpret graphical summaries of data, including boxplots, histograms, violin plots, density plots, scatterplots, and correlation plots. Read data from a .csv file into R. Preview data in R. Create graphical summaries of data using R. Calculate summary statistics for entire datasets and grouped summaries. Create reproducible documents using R Markdown. 1.1 Getting Started in R This section provides examples of how to read data into R, create graphics, like those in the previous section, and calculate summary statistics. We’ll work with data on houses that sold in King County, WA, (home of Seattle) between 2014 and 2015. We begin by loading the tidyverse package which can be used to create professional data graphics and summaries. library(tidyverse) 1.1.1 Previewing the Data head() The head() function displays the first 5 rows of the dataset. head(Houses) ## # A tibble: 6 × 9 ## Id price bedrooms bathrooms sqft_living sqft_lot condition waterfront ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 1 1225 4 4.5 5420 101930 average No ## 2 2 885. 4 2.5 2830 5000 average No ## 3 3 385. 4 1.75 1620 4980 good No ## 4 4 253. 2 1.5 1070 9643 average No ## 5 5 468. 2 1 1160 6000 good No ## 6 6 310. 3 1 1430 19901 good No ## # ℹ 1 more variable: yr_built &lt;dbl&gt; The rows of the dataset are called observations. In this case, the observations are the houses. The columns of the dataset, which contain information about the houses, are called variables. glimpse The glimpse() command shows the number of observations (rows), and the number of variables, (columns). We also see the name of each variable and its type. Variable types include Categorical variables, which take on groups or categories, rather than numeric values. In R, these might be coded as logical &lt;logi&gt;, character &lt;chr&gt;, factor &lt;fct&gt; and ordered factor &lt;ord&gt;. Quantitative variables, which take on meaningful numeric values. These include numeric &lt;num&gt;, integer &lt;int&gt;, and double &lt;dbl&gt;. Date and time variables take on values that are dates and times, and are denoted &lt;dttm&gt; glimpse(Houses) ## Rows: 100 ## Columns: 9 ## $ Id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,… ## $ price &lt;dbl&gt; 1225.00, 885.00, 385.00, 252.70, 468.00, 310.00, 550.00, 4… ## $ bedrooms &lt;dbl&gt; 4, 4, 4, 2, 2, 3, 4, 4, 3, 3, 3, 4, 5, 3, 4, 4, 3, 4, 3, 3… ## $ bathrooms &lt;dbl&gt; 4.50, 2.50, 1.75, 1.50, 1.00, 1.00, 1.00, 1.00, 1.00, 2.25… ## $ sqft_living &lt;dbl&gt; 5420, 2830, 1620, 1070, 1160, 1430, 1660, 1600, 960, 1660,… ## $ sqft_lot &lt;dbl&gt; 101930, 5000, 4980, 9643, 6000, 19901, 34848, 4300, 6634, … ## $ condition &lt;fct&gt; average, average, good, average, good, good, poor, good, a… ## $ waterfront &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, No… ## $ yr_built &lt;dbl&gt; 2001, 1995, 1947, 1985, 1942, 1927, 1933, 1916, 1952, 1979… There are 100 houses in the dataset, and 9 variables on each house. summary summary displays the mean, minimum, first quartile, median, third quartile, and maximum for each numeric variable, and the number of observations in each category, for categorical variables. summary(Houses) ## Id price bedrooms bathrooms ## Min. : 1.00 Min. : 180.0 Min. :1.00 Min. :0.750 ## 1st Qu.: 25.75 1st Qu.: 322.9 1st Qu.:3.00 1st Qu.:1.500 ## Median : 50.50 Median : 507.5 Median :3.00 Median :2.000 ## Mean : 50.50 Mean : 735.4 Mean :3.39 Mean :2.107 ## 3rd Qu.: 75.25 3rd Qu.: 733.8 3rd Qu.:4.00 3rd Qu.:2.500 ## Max. :100.00 Max. :5300.0 Max. :6.00 Max. :6.000 ## sqft_living sqft_lot condition waterfront yr_built ## Min. : 440 Min. : 1044 poor : 1 No :85 Min. :1900 ## 1st Qu.:1410 1st Qu.: 5090 fair : 1 Yes:15 1st Qu.:1948 ## Median :2000 Median : 7852 average :59 Median :1966 ## Mean :2291 Mean : 13205 good :30 Mean :1965 ## 3rd Qu.:2735 3rd Qu.: 12246 very_good: 9 3rd Qu.:1991 ## Max. :8010 Max. :101930 Max. :2014 1.1.2 Modifying the Data Next we’ll look at how to manipulate the data and create new variables. Adding a New Variable We can use the mutate() function to create a new variable based on variables already in the dataset. Let’s add a variable giving the age of the house, as of 2015. Houses &lt;- Houses %&gt;% mutate(age = 2015-yr_built) head(Houses) ## # A tibble: 6 × 10 ## Id price bedrooms bathrooms sqft_living sqft_lot condition waterfront ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 1 1225 4 4.5 5420 101930 average No ## 2 2 885. 4 2.5 2830 5000 average No ## 3 3 385. 4 1.75 1620 4980 good No ## 4 4 253. 2 1.5 1070 9643 average No ## 5 5 468. 2 1 1160 6000 good No ## 6 6 310. 3 1 1430 19901 good No ## # ℹ 2 more variables: yr_built &lt;dbl&gt;, age &lt;dbl&gt; Selecting Columns If the dataset contains a large number of variables, narrow down to the ones you are interested in working with. This can be done with the select() command. If there are not very many variables to begin with, or you are interested in all of them, then you may skip this step. Let’s create a smaller version of the dataset, with only the columns price, sqft_living, and waterfront. We’ll call this Houses_3var. Houses_3var &lt;- Houses %&gt;% select(price, sqft_living, waterfront) head(Houses_3var) ## # A tibble: 6 × 3 ## price sqft_living waterfront ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 1225 5420 No ## 2 885. 2830 No ## 3 385. 1620 No ## 4 253. 1070 No ## 5 468. 1160 No ## 6 310. 1430 No 1.1.2.1 Filtering by Row The filter() command narrows a dataset down to rows that meet specified conditions. We’ll filter the data to include only houses built after 2000. New_Houses &lt;- Houses %&gt;% filter(yr_built&gt;=2000) head(New_Houses) ## # A tibble: 6 × 10 ## Id price bedrooms bathrooms sqft_living sqft_lot condition waterfront ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 1 1225 4 4.5 5420 101930 average No ## 2 16 3075 4 5 4550 18641 average Yes ## 3 23 862. 5 2.75 3595 5639 average No ## 4 24 360. 4 2.5 2380 5000 average No ## 5 25 625. 4 2.5 2570 5520 average No ## 6 27 488. 3 2.5 3160 13603 average No ## # ℹ 2 more variables: yr_built &lt;dbl&gt;, age &lt;dbl&gt; Now, we’ll filter the data to include only houses on the waterfront. New_Houses &lt;- Houses %&gt;% filter(waterfront == &quot;Yes&quot;) head(New_Houses) ## # A tibble: 6 × 10 ## Id price bedrooms bathrooms sqft_living sqft_lot condition waterfront ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 16 3075 4 5 4550 18641 average Yes ## 2 19 995. 3 4.5 4380 47044 average Yes ## 3 34 825. 2 1 1150 12775 good Yes ## 4 40 2400. 4 2.5 3650 8354 average Yes ## 5 42 290. 2 0.75 440 8313 good Yes ## 6 46 5111. 5 5.25 8010 45517 average Yes ## # ℹ 2 more variables: yr_built &lt;dbl&gt;, age &lt;dbl&gt; 1.2 Data Visualization 1.2.1 Histogram Next, we’ll create graphics to help us visualize the distributions and relationships between variables. We’ll use the ggplot() function, which is part of the tidyverse package. Histograms are useful for displaying the distribution of a single quantitative variable. In a histogram, the x-axis breaks the variable into ranges of values, and the y-axis displays the number of observations with a value falling in that category (frequency). General Template for Histogram ggplot(data=DatasetName, aes(x=VariableName)) + geom_histogram(fill=&quot;colorchoice&quot;, color=&quot;colorchoice&quot;) + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;x-axis label&quot;) + ylab(&quot;y-axis label&quot;) Histogram of House Prices ggplot(data=Houses, aes(x=price)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + ggtitle(&quot;Distribution of House Prices&quot;) + xlab(&quot;Price (in thousands)&quot;) + ylab(&quot;Frequency&quot;) We see that the distribution of house prices is right-skewed. Most houses cost less than $1,000,000, though there are a few houses that are much more expensive. The most common price range is around $400,000 to $500,000. 1.2.2 Density Plot Density plots show the distribution for a quantitative variable price. Scores can be compared across categories, like whether or not the house is on a waterfront. General Template for Density Plot ggplot(data=DatasetName, aes(x=QuantitativeVariable, color=CategoricalVariable, fill=CategoricalVariable)) + geom_density(alpha=0.2) + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;Axis Label&quot;) + ylab(&quot;Frequency&quot;) alpha, ranging from 0 to 1 dictates transparency. Density Plot of House Prices ggplot(data=Houses, aes(x=price, color=waterfront, fill=waterfront)) + geom_density(alpha=0.2) + ggtitle(&quot;Distribution of Prices&quot;) + xlab(&quot;House price (in thousands)&quot;) + ylab(&quot;Frequency&quot;) We see that on average, houses on the waterfront tend to be more expensive and have a greater price range than houses not on the waterfront. 1.2.3 Boxplot Boxplots can be used to compare a quantitative variable with a categorical variable. The middle 50% of observations are contained in the “box”, with the upper and lower 25% of the observations in each tail. General Template for Boxplot ggplot(data=DatasetName, aes(x=CategoricalVariable, y=QuantitativeVariable)) + geom_boxplot() + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;Variable Name&quot;) + ylab(&quot;Variable Name&quot;) You can make the plot horizontal by adding + coordflip(). You can turn the axis text vertical by adding theme(axis.text.x = element_text(angle = 90)). Boxplot Comparing Price by Waterfront Status ggplot(data=Houses, aes(x=waterfront, y=price)) + geom_boxplot() + ggtitle(&quot;House Price by Waterfront Status&quot;) + xlab(&quot;Waterfront&quot;) + ylab(&quot;Price (in thousands)&quot;) + coord_flip() For houses not on the waterfront, the median price is about $400,000, and the middle 50% of prices range from about $300,000 to $600,000. For waterfront houses, the median price is about $1,500,000, and the middle 50% of prices range from about $900,000 to $1,900,000. 1.2.4 Violin Plot Violin plots are an alternative to boxplots. The width of the violin tells us the density of observations in a given range. General Template for Violin Plot ggplot(data=DatasetName, aes(x=CategoricalVariable, y=QuantitativeVariable, fill=CategoricalVariable)) + geom_violin() + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;Variable Name&quot;) + ylab(&quot;Variable Name&quot;) Violin Plot Comparing Prices by Waterfront ggplot(data=Houses, aes(x=waterfront, y=price, fill=waterfront)) + geom_violin() + ggtitle(&quot;Price by Waterfront Status&quot;) + xlab(&quot;Waterfront&quot;) + ylab(&quot;Price (in thousands)&quot;) + theme(axis.text.x = element_text(angle = 90)) Again, we see that houses on the waterfront tend to be more expensive than those not on the waterfront, and have a wider range in prices. 1.2.5 Scatterplot Scatterplots are used to visualize the relationship between two quantitative variables. Scatterplot Template ggplot(data=DatasetName, aes(x=CategoricalVariable, y=QuantitativeVariable)) + geom_point() + ggtitle(&quot;Plot Title&quot;) + ylab(&quot;Axis Label&quot;) + xlab(&quot;Axis Label&quot;) Scatterplot Comparing Price and Square Feet of Living Space ggplot(data=Houses, aes(x=sqft_living, y=price)) + geom_point() + ggtitle(&quot;Price and Living Space&quot;) + ylab(&quot;Price (in thousands)&quot;) + xlab(&quot;Living Space in sq. ft. &quot;) We see that there is an upward trend, indicating that houses with more living space tend to, on average, be higher priced than those with less living space. The relationship appears to be roughly linear, though there might be some curvature, as living space gets very large. There are some exceptions to this trend, most notably a house with more than 7,000 square feet, priced just over $1,000,000. We can also add color, size, and shape to the scatterplot to display information about other variables. We’ll use color to illustrate whether the house is on the waterfront, and size to represent the square footage of the entire lot (including the yard and the house). ggplot(data=Houses, aes(x=sqft_living, y=price, color=waterfront, size=sqft_lot)) + geom_point() + ggtitle(&quot;Price of King County Houses&quot;) + ylab(&quot;Price (in thousands)&quot;) + xlab(&quot;Living Space in sq. ft. &quot;) We notice that many of the largest and most expensive houses are on the waterfront. 1.2.6 Bar Graph Bar graphs can be used to visualize one or more categorical variables. A bar graph is similar to a histogram, in that the y-axis again displays frequency, but the x-axis displays categories, instead of ranges of values. Bar Graph Template ggplot(data=DatasetName, aes(x=CategoricalVariable)) + geom_bar(fill=&quot;colorchoice&quot;,color=&quot;colorchoice&quot;) + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;Variable Name&quot;) + ylab(&quot;Frequency&quot;) Bar Graph by Condition ggplot(data=Houses, aes(x=condition)) + geom_bar(fill=&quot;lightblue&quot;,color=&quot;white&quot;) + ggtitle(&quot;Number of Houses by Condition&quot;) + xlab(&quot;Condition&quot;) + ylab(&quot;Frequency&quot;) + theme(axis.text.x = element_text(angle = 90)) We see that the majority of houses are in average condition. Some are in good or very good condition, while very few are in poor or very poor condition. 1.2.7 Stacked and Side-by-Side Bar Graphs Stacked Bar Graph Template ggplot(data = DatasetName, mapping = aes(x = CategoricalVariable1, fill = CategoricalVariable2)) + stat_count(position=&quot;fill&quot;) + theme_bw() + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;Variable 1&quot;) + ylab(&quot;Proportion of Variable 2&quot;) + theme(axis.text.x = element_text(angle = 90)) Stacked Bar Graph Example The stat_count(position=\"fill\") command creates a stacked bar graph, comparing two categorical variables. Let’s explore whether waterfront status is related to condition. ggplot(data = Houses, mapping = aes(x = waterfront, fill = condition)) + stat_count(position=&quot;fill&quot;) + theme_bw() + ggtitle(&quot;Condition by Waterfront Status&quot;) + xlab(&quot;Waterfront Status&quot;) + ylab(&quot;Condition&quot;) + theme(axis.text.x = element_text(angle = 90)) We see that a higher proportion of waterfront houses are in good or excellent condition than non-waterfront houses. Side-by-side Bar Graph Template We can create a side-by-side bar graph, using position=dodge. ggplot(data = DatasetName, mapping = aes(x = CategoricalVariable1, fill = CategoricalVariable2)) + geom_bar(position = &quot;dodge&quot;) + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;Genre&quot;) + ylab(&quot;Frequency&quot;) Side-by-side Bar Graph Example ggplot(data = Houses, mapping = aes(x = waterfront, fill = condition)) + geom_bar(position = &quot;dodge&quot;) + ggtitle(&quot;Condition by Waterfront Status&quot;) + xlab(&quot;Waterfront Status&quot;) + ylab(&quot;Condition&quot;) + theme(axis.text.x = element_text(angle = 90)) In this case, since there are so few waterfront houses, the graph is hard to read and not very useful. The stacked bar graph is a better way to convey information in this instance, though you may find that for a different dataset, the side-by-side bar graph could be a better choice. 1.2.8 Correlation Plot Correlation plots can be used to visualize relationships between quantitative variables. Correlation is a number between -1 and 1, describing the strength of the linear relationship between two variables. Variables with strong positive correlations will have correlation close to +1, while variables with strong negative correlations will have correlations close to -1. Variables with little to no relationship will have correlation close to 0. The cor() function calculates correlations between quantitative variables. We’ll use select_if to select only numeric variables. The `use=“complete.obs” command tells R to ignore observations with missing data. cor(select_if(Houses, is.numeric), use=&quot;complete.obs&quot;) %&gt;% round(2) ## Id price bedrooms bathrooms sqft_living sqft_lot yr_built age ## Id 1.00 0.03 -0.06 -0.01 -0.03 -0.07 -0.02 0.02 ## price 0.03 1.00 0.40 0.67 0.81 0.42 0.17 -0.17 ## bedrooms -0.06 0.40 1.00 0.58 0.58 0.15 0.26 -0.26 ## bathrooms -0.01 0.67 0.58 1.00 0.85 0.45 0.50 -0.50 ## sqft_living -0.03 0.81 0.58 0.85 1.00 0.54 0.36 -0.36 ## sqft_lot -0.07 0.42 0.15 0.45 0.54 1.00 0.14 -0.14 ## yr_built -0.02 0.17 0.26 0.50 0.36 0.14 1.00 -1.00 ## age 0.02 -0.17 -0.26 -0.50 -0.36 -0.14 -1.00 1.00 The corrplot() function in the corrplot() package provides a visualization of the correlations. Larger, thicker circles indicate stronger correlations. library(corrplot) Corr &lt;- cor(select_if(Houses, is.numeric), use=&quot;complete.obs&quot;) corrplot(Corr) We see that price has a strong positive correlation with square feet of living space, and is also positively correlated with number of bedrooms and bathrooms. Living space, bedrooms, and bathrooms are all positively correlated, which makes sense, since we would expect bigger houses to have more bedrooms and bathrooms. Price does not show much correlation with the other variables. We notice that bathrooms is negatively correlated with age, which means older houses tend to have fewer bathrooms than newer ones. Not surprisingly, age is very strongly correlated with year built. 1.2.9 Scatterplot Matrix A scatterplot matrix is a grid of plots. It can be created using the ggpairs() function in the GGally package. The scatterplot matrix shows us: Along the diagonal are density plots for quantitative variables, or bar graphs for categorical variables, showing the distribution of each variable. Under the diagonal are plots showing the relationships between the variables in the corresponding row and column. Scatterplots are used when both variables are quantitative, bar graphs are used when both variables are categorical, and boxplots are used when one variable is categorical, and the other is quantitative. Above the diagonal are correlations between quantitative variables. Including too many variables can make these hard to read, so it’s a good idea to use select to narrow down the number of variables. library(GGally) ggpairs(Houses %&gt;% select(price, sqft_living, condition, age)) The scatterplot matrix is useful for helping us notice key trends in our data. However, the plot can hard to read as it is quite dense, especially when there are a large number of variables. These can help us look for trends from a distance, but we should then focus in on more specific plots. 1.3 Summary Tables 1.3.1 Calculating Summary Statistics summarize() The summarize command calculates summary statistics for variables in the data. For a set of \\(n\\) values \\(y_i, \\ldots, y_n\\): mean (\\(\\bar{y}\\)) is calculated by \\(\\bar{y} =\\frac{1}{n}\\displaystyle\\sum_{i=1}^n y_i\\). standard deviation (\\(s\\)), a measure of the spread is calculated by \\(s =\\sqrt{\\displaystyle\\sum_{i=1}^n \\frac{(y_i-\\bar{y})^2}{n-1}}\\). The square of the standard deviation, called the variance is denoted \\(s^2\\). Let’s calculate the mean, median, and standard deviation, in prices. Houses_Summary &lt;- Houses %&gt;% summarize(Mean_Price = mean(price, na.rm=TRUE), Median_Price = median(price, na.rm=TRUE), StDev_Price = sd(price, na.rm = TRUE), Number_of_Houses = n()) Houses_Summary ## # A tibble: 1 × 4 ## Mean_Price Median_Price StDev_Price Number_of_Houses ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 735. 507. 835. 100 Notes: 1. The n() command calculates the number of observations. 2. The na.rm=TRUE command removes missing values, so that summary statistics can be calculated. It’s not needed here, since this dataset doesn’t include missing values, but if the dataset does include missing values, you will need to include this, in order to do the calculation. The kable() function in the knitr() package creates tables with professional appearance. library(knitr) kable(Houses_Summary) Mean_Price Median_Price StDev_Price Number_of_Houses 735.3525 507.5 835.1231 100 1.3.2 Grouped Summaries group_by() The group_by() command allows us to calculate summary statistics, with the data broken down by by category.We’ll compare waterfront houses to non-waterfront houses. Houses_Grouped_Summary &lt;- Houses %&gt;% group_by(waterfront) %&gt;% summarize(Mean_Price = mean(price, na.rm=TRUE), Median_Price = median(price, na.rm=TRUE), StDev_Price = sd(price, na.rm = TRUE), Number_of_Houses = n()) kable(Houses_Grouped_Summary) waterfront Mean_Price Median_Price StDev_Price Number_of_Houses No 523.7595 450 295.7991 85 Yes 1934.3800 1350 1610.7959 15 Note: arrange(desc(Mean_Gross)) arranges the table in descending order of Mean_Gross. To arrange in ascending order, use arrange(Mean_Gross). "],["introduction-to-statistical-models.html", "Chapter 2 Introduction to Statistical Models 2.1 Fitting Models to Data 2.2 Variability Explained by a Model 2.3 Models with Interaction 2.4 Least Squares Estimation (LSE) 2.5 ANalysis Of VAriance", " Chapter 2 Introduction to Statistical Models Learning Outcomes: Calculate sums of squares related to variability explained, including SST, SSR, and SSM., when given small datasets and/or summary statistics. Explain the meaning of SST, SSR, and SSM in a given context. Calculate \\(R^2\\) and ANOVA F-Statistics, when given small datasets and/or summary statistics. Interpret \\(R^2\\) and F-statistics in context. Explain the process for estimating least-squares regression coefficients. Calculate predictions from linear regression models. Interpret regression coefficients for models involving quantitative and/or categorical variables in context, or explain why it is inappropriate to do so. Explain the meaning of interaction between quantitative and categorical explanatory variables. Apply graphical methods, statistical summaries, and background knowledge to argue for whether or not interaction term(s) should be used in a statistical model. Determine slopes, intercepts, and other regression coefficients for specific categories or values of an explanatory variable in models that involve interaction. 2.1 Fitting Models to Data 2.1.1 Terminology In this section, we’ll use statistical models to predict the prices of houses in King County, WA. In a statistical model, The variable we are trying to predict (price) is called the response variable (denoted \\(Y\\)). Variable(s) we use to help us make the prediction is(are) called explanatory variables (denoted \\(X\\)). These are also referred to as predictor variables or covariates. 2.1.2 Model with Quantitative Explanatory Variable We’ll first predict the price of the house, using the number of square feet of living space as our explanatory variable. We’ll assume that price changes linearly with square feet, and fit a trend line to the data. ggplot(data=Houses, aes(x=sqft_living, y=price)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) + ggtitle(&quot;Price and Living Space&quot;) + ylab(&quot;Price&quot;) + xlab(&quot;Living Space in sq. ft. &quot;) The model equation is \\[ \\widehat{\\text{Price}} = b_0 + b_1\\text{Sq.Ft.} \\] Note, the symbol over the response variable (Price) is read as “hat”, and means “predicted price”. We fit the model in R, using the lm (linear model) command. The output gives the estimates of \\(b_0\\) and \\(b_1\\). M_House_sqft &lt;- lm(data=Houses, price~sqft_living) M_House_sqft ## ## Call: ## lm(formula = price ~ sqft_living, data = Houses) ## ## Coefficients: ## (Intercept) sqft_living ## -484.9575 0.5328 The estimates are \\(b_0=-484.9575\\) and \\(b_1=0.5328\\). The model equation is \\[ \\widehat{\\text{Price}} = -484.9575 + 0.5328\\times\\text{Sq.Ft.} \\] Interpretations The intercept \\(b_0\\) represents the expected (or average) value of the response variable, when the explanatory variable is equal to 0. This is not always a meaningful interpretation in context. The slope \\(b_1\\) represents the expected (or average) change in the response variable for each one-unit increase in the explanatory variable. On average, a house with 0 square feet is expected to cost -485 thousand dollars. This is not a sensible interpretation, as there are no houses with 0 square feet. For each additional square foot in living space, the price of the house is expected to increase by 0.5328 thousand dollars (or $533). - Since a 1 square ft. increase is very small, it makes more sense to give the interpretation in terms of a 100-square foot increase. For each additional 100 square feet in living space, the price of the house is expected to increase by 53.28 thousand dollars. Prediction We can predict the price of a house with a given number of square feet by plugging the square feet into the model equation. The predicted price of a house with 1,500 square feet is \\[ \\widehat{\\text{Price}} = -484.9575 + 0.5328\\times 1500 = \\$314{ \\text{ thousand}} \\] We can calculate this directly in R using the predict command. predict(M_House_sqft, newdata=data.frame(sqft_living=1500)) ## 1 ## 314.1803 We should only try to make predictions on houses within the range of the observed data. Since the largest house in the dataset is 8,000 square feet we should not try to predict the price of house with 10,000 square feet. 2.1.3 Model with Categorical Variable Next, we’ll predict the price of a house based on whether or not it is on the waterfront. The boxplot shows the distribution of prices for waterfront and nonwaterfront houses. The red dots indicate the mean. ggplot(data=Houses, aes(x=waterfront, y=price)) + geom_boxplot() + ggtitle(&quot;House Price by Waterfront Status&quot;) + xlab(&quot;Waterfront&quot;) + ylab(&quot;Price&quot;) + coord_flip() + stat_summary(fun.y=mean, geom=&quot;point&quot;, shape=20, color=&quot;red&quot;, fill=&quot;red&quot;) The table displays the price summary by waterfront status. Houses_Grouped_Summary &lt;- Houses %&gt;% group_by(waterfront) %&gt;% summarize(Mean_Price = mean(price, na.rm=TRUE), Median_Price = median(price, na.rm=TRUE), StDev_Price = sd(price, na.rm = TRUE), Number_of_Houses = n()) kable(Houses_Grouped_Summary) waterfront Mean_Price Median_Price StDev_Price Number_of_Houses No 523.7595 450 295.7991 85 Yes 1934.3800 1350 1610.7959 15 The model equation is \\[ \\widehat{\\text{Price}} = b_0 + b_1\\text{Waterfront} \\] The waterfront variable takes on value of 1 if the house is on the waterfront, and 0 otherwise. M_House_wf &lt;- lm(data=Houses, price~waterfront) M_House_wf ## ## Call: ## lm(formula = price ~ waterfront, data = Houses) ## ## Coefficients: ## (Intercept) waterfrontYes ## 523.8 1410.6 The estimates are \\(b_0=523.8\\) and \\(b_1=1410.6\\). The model equation is \\[ \\widehat{\\text{Price}} = 523.8 + 1410.6\\times \\text{Sq.Ft.} \\] Interpretations The intercept \\(_0\\) represents the expected (or average) value of the response variable in the “baseline” category (in this case non-waterfront). The coefficient \\(b_1\\) represents the expected (or average) difference in response between the a category and the “baseline” category. On average, a house that is not on the waterfront is expected to cost 523.8 thousand dollars. On average a house that is not on the waterfront is expected to cost 1410.6 thousand (or 1.4 million) dollars more than a house that is not on the waterfront. Prediction We can predict the price of a house with a given number of square feet by plugging in either 1 or 0 for the waterfront variable. The predicted price of a house on the waterfront is: \\[ \\widehat{\\text{Price}} = 523.8 + 1410.6\\times 1 = \\$1934.6{ \\text{ thousand (or 1.9 million)}} \\] The predicted price of a house not on the waterfront is: \\[ \\widehat{\\text{Price}} = 523.8 + 1410.6\\times 0 = \\$523.8{ \\text{ thousand}} \\] Calculations in R: predict(M_House_wf, newdata=data.frame(waterfront=&quot;Yes&quot;)) ## 1 ## 1934.38 predict(M_House_wf, newdata=data.frame(waterfront=&quot;No&quot;)) ## 1 ## 523.7595 Notice that the predicted prices for each category correspond to the average price for that category. 2.1.4 Model with Multiple Explanatory Variables We’ve used square feet and waterfront status as explanatory variables individually. We can also build a model that uses both of these variables at the same time. A model with two or more explanatory variables is called a multiple regression model. The model equation is \\[ \\widehat{\\text{Price}} = b_0 + b_1\\times\\text{Sq. Ft} + b_2\\times\\text{Waterfront} \\] For a house not on the waterfront, \\(b_2=0\\), so the model equation is: \\[ \\widehat{\\text{Price}} = b_0 + b_1\\text{Sq. Ft} \\] For a house on the waterfront, \\(b_2=1\\), so the model equation is: \\[ \\widehat{\\text{Price}} = (b_0 + b_2) + b_1\\times\\text{Sq. Ft} \\] Notice that the slope is the same, regardless of whether the house is on the waterfront (\\(b_1\\)). The intercept, however, is different (\\(b_0\\) for houses not on the waterfront, and \\(b_0 + b_2\\) for houses on the waterfront). Thus, the model assumes that price increases at the same rate, with respect to square feet, regardless of whether or not it is on the waterfront, but allows the predicted price for a waterfront house to differ from a non-waterfront house of the same size. We fit the model in R. M_wf_sqft &lt;- lm(data=Houses, price~sqft_living+waterfront) M_wf_sqft ## ## Call: ## lm(formula = price ~ sqft_living + waterfront, data = Houses) ## ## Coefficients: ## (Intercept) sqft_living waterfrontYes ## -407.6549 0.4457 814.3613 The model equation is \\[ \\widehat{\\text{Price}} = -407.7 + 0.4457\\times\\text{Sq. Ft} + 814.36\\times\\text{Waterfront} \\] Interpretations The intercept \\(b_0\\) represents the expected (or average) value of the response variable, when all quantitative explanatory variables are equal to 0, and all categorical variables are in the “baseline” category. This interpretion is not always sensible. We interpret coefficients \\(b_j\\) for categorical or quantitative variables, the same way we would in a regression model with only one variable, but we need to state that all other explanatory variables are being held constant. On average, a house that is not on the waterfront with 0 square feet is expected to cost -407.7 thousand dollars. This is not a sensible interpretation, since there are no houses with 0 square feet. For each 1-square foot increase in size, the price of a house is expected to increase by 0.4457 thousand (or 446 hundred) dollars, assuming waterfront status is the same. Equivalently, for each 100-square foot increase in size, the price of a house is expected to increase by 44.57 thousand dollars, assuming waterfront status is the same. On average, a house on the waterfront is expected to cost 814 thousand dollars more than a house that is not on the waterfront, assuming square footage is the same. Prediction The predicted price of a 1,500 square foot house on the waterfront is: \\[ \\widehat{\\text{Price}} = -407.7 + 0.4457\\times1500 + 814.36\\times1 = \\$1075{ \\text{ thousand (or 1.075 million)}} \\] The predicted price of a 1,500 square foot not on the waterfront is: \\[ \\widehat{\\text{Price}} = -407.7 + 0.4457\\times1500 = \\$260.9{ \\text{ thousand}} \\] Calculations in R: predict(M_wf_sqft, newdata=data.frame(waterfront=&quot;Yes&quot;, sqft_living=1500)) ## 1 ## 1075.227 predict(M_wf_sqft, newdata=data.frame(waterfront=&quot;No&quot;, sqft_living=1500)) ## 1 ## 260.8657 2.1.5 Model with No Explanatory Variable Finally, we’ll consider a model that makes use of no explanatory variables at all. Although this might seem silly, its relevance will be seen in the next section. The histogram shows the distribution of prices, without any information about explanatory variables. The mean price is indicated in red. ggplot(data=Houses, aes(x=price)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + ggtitle(&quot;Distribution of House Prices&quot;) + xlab(&quot;Price&quot;) + ylab(&quot;Frequency&quot;) + geom_point(aes(x=mean(Houses$price), y=0), color=&quot;red&quot;, shape=24, fill=&quot;red&quot;) The mean, median, and standard deviation in prices is shown below. library(knitr) kable(Houses_Summary) Mean_Price Median_Price StDev_Price Number_of_Houses 735.3525 507.5 835.1231 100 Suppose we know that a house sold in King County during this time, and want to predict the price, without knowing anything else about the house. The best we can do is to use the mean price for our prediction. (We’ll define what we mean by “best” later in the chapter.) The model equation is \\[ \\widehat{\\text{Price}} = b_0 \\] We fit a statistical model in R using the lm command. # syntax for lm command # lm(data=DatasetName, ResponseVariable~ExplanatoryVariable(s)) M0_House &lt;- lm(data=Houses, price ~ 1) # when there are no explanatory variables, use ~1 M0_House ## ## Call: ## lm(formula = price ~ 1, data = Houses) ## ## Coefficients: ## (Intercept) ## 735.4 The model equation is \\[ \\widehat{\\text{Price}} = 735.4 \\] Interpretation The expected price of a house in King County is 735.4 thousand dollars. Predictions Without knowing anything about any explanatory variables, we would predict the price of any house sold in King County, WA to cost 735.4 thousand dollars. 2.2 Variability Explained by a Model 2.2.1 Quantifying Variability We’ve seen four different models for predicting house price. It would be nice to have a way to assess how well the models are predicting prices, and determine which model appears to be the best. Of course we won’t know the price of the house we are trying to predict, so we can’t be sure how close or far our prediction is. We do, however, know the prices of the original 100 houses in our dataset. We can assess the models by measuring how far the actual prices of the 100 houses differ from the predicted (mean) price, and by calculating the proportion of total variation in sale price explained by each model. 2.2.2 Total Variability Let’s start with our most basic model, which uses no explanatory variables and predicts the price of each simply using the average of all houses in the dataset. We measure the total variability in the response variable by calculating the square difference between each individual response value and the overall average. This quantity is called the total sum of squares (SST). \\[ \\text{SST} = \\displaystyle\\sum_{i=1}^n (y_i - \\bar{y})^2 \\] The plot below shows a horizontal line at the mean sale price (785 thousand). The points represent prices of individual houses, and the red lines represent the differences between the price of each house and the overall average. The first three houses in the dataset are shown below. First3Houses &lt;- Houses %&gt;% select(Id, price, waterfront, sqft_living) %&gt;% head(3) kable(First3Houses) Id price waterfront sqft_living 1 1225 No 5420 2 885 No 2830 3 385 No 1620 \\[ \\begin{aligned} \\text{SST} &amp; = \\displaystyle\\sum_{i=1}^{100} (y_i - \\bar{y})^2 \\\\ &amp; = (1225-785)^2 + (885-785)^2 + (385-785)^2 + \\ldots \\end{aligned} \\] We could calculate SST by hand for small datasets. For larger datasets, we’ll use R to perform the calculation. meanprice &lt;- mean(Houses$price) #calculate mean price SST &lt;- sum((Houses$price - meanprice)^2) ## calculate SST SST ## [1] 69045634 By itself, the size of SST does not have much meaning. We cannot say whether a SST value like the one we see here is large or small, since it depends on the size and scale of the variable being measured. An SST value that is very large in one context might be very small in another. SST does, however, give us a baseline measure of the total variability in the response variable. We’ll assess the performance of a model with a given explanatory variable by measuring how much of this variability the model accounts for. 2.2.3 Residuals Now let’s consider our model that uses the size of the house in square feet as the explanatory variable. The figure on the left shows difference between actual and predicted prices, using this linear model. We compare the size of the differences to those resulting from the basic model that does not use any explanatory variables, and predicts each price using the overall average (shown on the right). Residplot_sqft &lt;- ggplot(data=Houses, aes(x = sqft_living, y = price)) + geom_point() + geom_segment(aes(xend = sqft_living, yend = M_House_sqft$fitted.values), color=&quot;red&quot;) + geom_point(aes(y = M_House_sqft$fitted.values), shape = 1) + stat_smooth(method=&quot;lm&quot;, se=FALSE) + ylim(c(0,5500)) + theme_bw() Notice that the red lines are shorter in the figure on the left, indicating the predictions are closer to the actual values. The difference between the actual and predicted values is called the residual. The residual for the \\(ith\\) case is \\[ r_i = (y_i-\\hat{y}_i)^2 \\] We’ll calculate the residuals for the first three houses in the dataset, shown below. kable(First3Houses) Id price waterfront sqft_living 1 1225 No 5420 2 885 No 2830 3 385 No 1620 The model equation is \\[ \\widehat{\\text{Price}} = -484.9575 + 0.5328\\times \\text{Sq. Ft} \\] The predicted prices for these three houses are: \\[ \\widehat{\\text{Price}_1} = -484.9575 + 0.5328\\times 5420 = 2402.6 \\text{ thousand dollars} \\] \\[ \\widehat{\\text{Price}_2} = -484.9575 + 0.5328\\times 2830 = 1022.7 \\text{ thousand dollars} \\] \\[ \\widehat{\\text{Price}_3} = -484.9575 + 0.5328\\times 1620 = 378.1 \\text{ thousand dollars} \\] To calculate the residuals, we subtract the predicted price from the actual price. \\[r_1 = y_1-\\hat{y}_1 = 1225 - 2402.6 = -1177.6 \\text{ thousand dollars}\\] \\[r_2 = y_2-\\hat{y}_2 = 885 - 1022.7 = -137.7 \\text{ thousand dollars}\\] \\[r_2 = y_2-\\hat{y}_2 = 385 - 378.1 = 6.9 \\text{ thousand dollars}\\] The fact that the first two residuals are negative indicates that these houses sold for less than the model predicts. The predicted values and residuals from a model can be calculated automatically in R. The predicted values and residuals for the first 5 houses are shown below. Predicted &lt;- predict(M_House_sqft) head(Predicted, 3) ## 1 2 3 ## 2402.5937 1022.7491 378.1113 Residual &lt;- M_House_sqft$residuals head(Residual, 3) ## 1 2 3 ## -1177.593665 -137.749128 6.888668 2.2.4 Variability Explained by Sq. Ft. Model The sum of squared residuals (SSR) measures the amount of unexplained variability in the response variable after accounting for all explanatory variables in the model. \\[ \\text{SSR} = \\displaystyle\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2. \\] Note that SSR is similar to SST, except we subtract the model’s predicted values, rather than the overall average. In the special case of a model with no explanatory variables, the predicted values are equal to the overall average, so SSR is equal to SST. We calculate SSR for the model using square feet as the explanatory variable. \\[ \\begin{aligned} \\text{SSR} &amp; = \\displaystyle\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2. \\\\ &amp; = (1225 - 2402.6)^2 + (885 - 1022.7)^2 + (385 - 378.1)^2 + \\ldots \\end{aligned} \\] We can calculate the model’s SSR directly in R. SSR_sqft &lt;- sum(M_House_sqft$residuals^2) SSR_sqft ## [1] 23767280 SSR represents the amount of total variability in saleprice remaining after accounting for the house’s size in square feet. The SSR=23,767,290 value is about one third of the SST value of 69,045,634. This means that about 2/3 of the total variability in sale price is explained by the model that accounts for sale price. The difference (SST-SSR) represents the variability in the response variable that is explained by the model. This quantity is called the model sum of squares (SSM). \\[ \\text{SSM} = \\text{SST} - \\text{SSR} \\] It can be shown that \\(\\text{SSM}=\\displaystyle\\sum_{i=1}^n(\\hat{y}_i-\\bar{y})^2\\). The proportion of total variability in the response variable explained by the model is called the coefficient of determination, denoted \\(R^2\\). We calculate this by dividing SSM by SST. \\[ R^2=\\frac{SSM}{SST}= \\frac{SST-SSR}{SST} \\] Example: For the model with square feet as the explanatory variable, \\[ SSM = SST-SSR = 69,045,634 - 23,767,290 =45,278,344. \\] \\[ R^2 = \\frac{45,278,344}{69,045,634}=0.6557. \\] Approximately 65.6% of the total variability in sale price is explained by the model using square feet as the explanatory variable. We calculate \\(R^2\\) directly in R. summary(M_House_sqft)$r.squared ## [1] 0.6557743 2.2.5 Linear Correlation Coefficient For models with a single quantiative explanatory varible, the coefficient of determination is equal to the square of the correlation coefficient \\(r\\), discussed in Chapter 1. ggplot(data=Houses, aes(x=sqft_living, y=price)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) For linear models with a single quantitative variable, the linear correlation coefficient \\(r=\\sqrt{R^2}\\), or \\(r=-\\sqrt{R^2}\\) (with sign matching the sign on the slope of the line), provides information about the strength and direction of the linear relationship between the variables. \\(-1 \\leq r \\leq 1\\), and \\(r\\) close to \\(\\pm1\\) provides evidence of strong linear relationship, while \\(r\\) close to 0 suggests linear relationship is weak. cor(Houses$price,Houses$sqft_living) ## [1] 0.8097989 \\(r\\) is only relevant for models with a single quantitative explanatory variable and a quantitative response variable, while \\(R^2\\) is relevant for any linear model with a quantitative response variable. 2.2.6 Variability Explained by Waterfront Model We can similarly calculate the proportion of variability explained by the model using waterfront as an explanatory variable. Recall that in this model, the predicted price of a house with a waterfront is given by the average price of all waterfront houses, and the predicted price of a non-waterfront house is given by the average price of all non-waterfront houses. We can calculate residuals using these predicted values, and compare them to the residuals resulting from a model with no explanatory variables, which uses the overall average price for all predictions. The left two figures show the residuals resulting from a model that accounts for waterfront status. The figure on the right shows the residuals resulting from the model with no explanatory variables. grid.arrange(arrangeGrob(M1aResid,M1bResid, Residplot_M0 + ggtitle(&quot;Model with no Exp. Vars&quot;), ncol=3, nrow=1, widths=c(3, 2,5))) Notice that after accounting for waterfront status, the differences between observed and predicted values are bigger than they were in the model that accounted for square feet, though not as big as for the model that doesn’t use any explanatory variables. We use R to calculate SSR for the waterfront model. SSR_wf &lt;- sum(M_House_wf$residuals^2) SSR_wf ## [1] 43675043 \\[ SSM = SST-SSR = 69,045,634 - 43,675,043 =25,370,591. \\] \\[ R^2 = \\frac{25,370,591}{69,045,634}=0.3674. \\] Approximately 36.7% of the total variability in sale price is explained by the model using waterfront status as the explanatory variable. We calculate \\(R^2\\) directly in R. summary(M_House_wf)$r.squared ## [1] 0.3674467 2.2.7 Variability Explained by Multiple Regression Model We’ve seen at the model using square feet accounts for about 2/3 of the total variability in house prices, while the model using waterfront status accounts for about 1/3 of the total variability. Let’s see if we can do better by using both variables together. The left figure shows the residuals resulting from a model that accounts for both waterfront status and square feet. The figure on the right shows the residuals resulting from the model with no explanatory variables. grid.arrange(Residplot_MR + ggtitle(&quot;Multiple Regression Model&quot;) , Residplot_M0 + ggtitle(&quot;Model with no Exp. Vars&quot;), ncol=2) We use R to calculate SSR for the waterfront model. SSR_wf_sqft &lt;- sum(M_wf_sqft$residuals^2) SSR_wf_sqft ## [1] 16521296 \\[ SSM = SST-SSR = 69,045,634 - 16,521,296 =52,524,338. \\] \\[ R^2 = \\frac{52,524,338}{69,045,634}=0.761. \\] Approximately 76.1% of the total variability in sale price is explained by the model using square feet and waterfront status as the explanatory variables. We calculate \\(R^2\\) directly in R. summary(M_wf_sqft)$r.squared ## [1] 0.7607192 Including both square feet and waterfront status allows us to explain more variability in sale price than models that include one but not both of these variables. 2.2.8 Summary: SST, SSR, SSM, \\(R^2\\) the total variability in the response variable is the sum of the squared differences between the observed values and the overall average. \\[\\text{Total Variability in Response Var.}= \\text{SST} =\\displaystyle\\sum_{i=1}^n(y_i-\\bar{y})^2\\] the variability remaining unexplained even after accounting for explanatory variable(s) in a model is given by the sum of squared residuals. We abbreviate this SSR, for sum of squared residuals. \\[ \\text{SSR} = \\text{Variability Remaining}=\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2 \\] the variability explained by the model, abbreviated SSM, is given by \\[ \\text{SSM} = \\text{SST} - \\text{SSR} \\] The coefficient of determination (abbreviated \\(R^2\\)) is defined as \\[R^2=\\frac{\\text{Variability Explained by Model}}{\\text{Total Variability}}=\\frac{\\text{SSM}}{\\text{SST}} =\\frac{\\displaystyle\\sum_{i=1}^n(\\hat{y}_i-\\bar{y})^2}{\\displaystyle\\sum_{i=1}^n(y_i-\\bar{y})^2}\\] Note that some texts use different abbreviations than the ones used here. When working with resources outside this class, be sure to carefully check the notation being used. 2.2.9 \\(R^2\\) Visually Model with a single quantitative explanatory variable: Model with a single categorical explanatory variable with 3 categories: Blue Area = Total Variability (SST) Red Area = Variability Remaining Unexplained by Model (SSR) Blue Area - Red Area = Variability Explained by Model (SSM) \\(R^2 = \\frac{\\text{Area of Blue Squares} - \\text{Area of Red Squares}}{\\text{Area of Blue Squares}} = \\frac{\\text{SST}-\\text{SSR}}{\\text{SST}}= \\frac{\\text{SSM}}{\\text{SST}}\\) 2.2.10 Model Comparison Summary Model Variables Unexplained Variability Variability Explained \\(R^2\\) 0 None 69045634.1341747 0 0 1 Sq. Ft. 23767280.3817707 45278353.752404 0.6557743 2 Waterfront 43675043.0897012 25370591.0444736 0.3674467 3 Sq. Ft. and Waterfront 16521296.4889025 52524337.6452723 0.7607192 Comments on \\(R^2\\): \\(R^2\\) will never decrease when a new variable is added to a model. This does not mean that adding more variables to a model always improves its ability to make predictions on new data. \\(R^2\\) measures how well a model fits the data on which it was built. It is possible for a model with high \\(R^2\\) to “overfit” the data it was built from, and thus perform poorly on new data. We will discuss this idea extensively later in the course. On some datasets, there is a lot of “natural” variability in the response variable, and no model will achieve a high \\(R^2\\). That’s okay. Even a model with \\(R^2 = 0.10\\) or less can provide useful information. The goal is not to achieve a model that makes perfect predictions, but rather to be able to quantify the amount of uncertainty associated with the predictions we make. 2.3 Models with Interaction 2.3.1 Definition of Interaction We previously used a multiple regression model of the form \\[ \\widehat{Price} = b_0 + b_1\\times\\text{SqFt} + b_2\\times\\text{Waterfront} \\] Recall that this model assumes the slope relating price and square footage is the same (\\(b_1\\)) for houses on the waterfront as for houses not on the waterfront. An illustration of the model is shown below. PM3 This assumption of the rate of change in price with respect to living space being the same for waterfront houses, as for non-waterfront houses might be unrealistic. Let’s fit separate lines waterfront and non-waterfront houses, without requiring them to have the same slope. ggplot(data=Houses, aes(x=sqft_living, y=price, color=waterfront)) + geom_point()+stat_smooth(method=&quot;lm&quot;, se=FALSE) + ylim(c(0,5500)) + theme_bw() It appears that the prices of the houses on the waterfront are increasing more rapidly, with respect to square feet of living space, than the non-waterfront houses. The effect of additional square feet on the price of the house appears to depend on whether or not the house is on the waterfront. This is an example of an interaction between square footage and waterfront status. An interaction between two explanatory variables occurs when the effect of one explanatory variable on the response depends on the other explanatory variable. 2.3.2 Interaction Term If we want to allow for different slopes between waterfront and non-waterfront houses, we’ll need to change the mathematical equation of our model. To do that, we’ll add a coefficient \\(b_3\\), multiplied by the product of our two explanatory variables. The model equation is \\[ \\widehat{Price} = b_0 + b_1\\times\\text{Sq. Ft.} + b_2\\times\\text{waterfront} + b_3\\times\\text{Sq.Ft}\\times\\text{Waterfront} \\] The last term is called an interaction term. For a house on the waterfront (\\(\\text{waterfront}=1\\)), the equation relating price to square feet is \\[ \\begin{aligned} \\widehat{Price} &amp; = b_0 + b_1\\times\\text{Sq. Ft.} + b_2\\times\\text{1} + b_3\\times\\text{Sq.Ft}\\times\\text{1} \\\\ &amp; = (b_0+b_2) + (b_1+b_3)\\times{\\text{Sq. Ft.}} \\end{aligned} \\] For a house not on the waterfront (\\(\\text{waterfront}=0\\)), the equation relating price to square feet is \\[ \\begin{aligned} \\widehat{Price} &amp; = b_0 + b_1\\times\\text{Sq. Ft.} + b_2\\times\\text{0} + b_3\\times\\text{Sq.Ft}\\times\\text{0} \\\\ &amp; = b_0 + b_1\\times{\\text{Sq. Ft}} \\end{aligned} \\] The intercept is \\(b_0\\) for non-waterfront houses, and \\(b_0 + b_2\\) for waterfront houses. The slopse is \\(b_1\\) for non-waterfront houses, and \\(b_1 + b_3\\) for waterfront houses. Thus, the model allows both the slope and intercept to differ between waterfront and non-waterfront houses. 2.3.3 Interaction Models in R To fit an interaction model in R, use * instead of + M_House_Int &lt;- lm(data=Houses, price~sqft_living*waterfront) M_House_Int ## ## Call: ## lm(formula = price ~ sqft_living * waterfront, data = Houses) ## ## Coefficients: ## (Intercept) sqft_living ## 67.3959 0.2184 ## waterfrontYes sqft_living:waterfrontYes ## -364.5950 0.4327 The regression equation is \\[ \\widehat{Price} = 67.4 + 0.2184\\times\\text{Sq. Ft.} -364.6\\times\\text{waterfront} + 0.4327\\times\\text{Sq.Ft}\\times\\text{Waterfront} \\] For a house on the waterfront (\\(\\text{waterfront}=1\\)), the equation is \\[ \\begin{aligned} \\widehat{Price} &amp; = 67.4 + 0.2184\\times\\text{Sq. Ft.} -364.6 \\times\\text{1} + 0.4327\\times\\text{Sq.Ft}\\times\\text{1} \\\\ &amp; = (67.4 - 364.6) + (0.2184+0.4327)\\times{\\text{Sq. Ft.}} \\\\ &amp; = -297.2 + 0.6511\\times{\\text{Sq. Ft.}} \\end{aligned} \\] For a house not on the waterfront (\\(\\text{waterfront}=0\\)), the equation is \\[ \\begin{aligned} \\widehat{Price} &amp; = 67.4 + 0.2184\\times\\text{Sq. Ft.} -364.6 \\times\\text{0} + 0.4327\\times\\text{Sq.Ft}\\times\\text{0} \\\\ &amp; = 67.4 0 + 0.2184\\times{\\text{Sq. Ft.}} \\end{aligned} \\] Interpretation When interpreting \\(b_0\\) and \\(b_1\\), we need to state that the interpretations apply only to the “baseline” category (in this case non-waterfront houses). In a model with interaction, it does not make sense to talk about holding one variable constant when interpreting the effect of the other, since the effect of one variable depends on the value or category of the other. Instead, we must state the value or category of one variable when interpreting the effect of the other. Interpretations: \\(b_0\\) - On average, a house with 0 square feet that is not on the waterfront is expected to cost 67 thousand dollars. This is not a sensible interpretation since there are no houses with 0 square feet. \\(b_1\\) - For each additional square foot in size, the price of a non-waterfront house is expected to increase by 0.2184 thousand dollars. \\(b_2\\) - On average, the price of a waterfront house with 0 square feet is expected to be 364.6 thousand dollars less than the price of a non-waterfront house with 0 square feet. This is not a sensible interpretation in this case. \\(b_3\\) - For each additional square foot in size, the price of a waterfront house is expected to increase by 0.4327 thousand dollars more than a non-waterfront house. Alternatively, we could interpret \\(b_0+b_2\\) and \\(b_1+b_3\\) together. \\(b_0 + b_2\\) - On average, a house with 0 square feet that is on the waterfront is expected to cost -297.2 thousand dollars. This is not a sensible interpretation since there are no houses with 0 square feet. \\(b_1\\) - For each additional square foot in size, the price of a waterfront house is expected to increase by 0.6511 thousand dollars. Prediction We calculate predicted prices for the following houses: Houses[c(1,16), ] %&gt;% select(Id, price, sqft_living, waterfront) ## # A tibble: 2 × 4 ## Id price sqft_living waterfront ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 1 1225 5420 No ## 2 16 3075 4550 Yes \\[ \\widehat{Price}_1 = 67.4 + 0.2184\\times5420 -364.6\\times0 + 0.4327\\times5420 \\times 0 = 1191 \\text{ thousand dollars} \\] \\[ \\widehat{Price}_{16} = 67.4 + 0.2184\\times4450 -364.6\\times1 + 0.4327\\times4450 \\times 1 = 2600 \\text{ thousand dollars} \\] 2.3.4 \\(R^2\\) for Interaction Model We can calculate residuals, as well as SSR, SSM, SST, and \\(R^2\\), in the same manner we’ve seen previously. We’ll perform these calculations using R. SSR_int &lt;- sum(M_House_Int$residuals^2) SSR_int ## [1] 10139974 \\[ SSM = SST-SSR = 69,045,634 - 10,139,974 =58,905,660. \\] \\[ R^2 = \\frac{58,905,660}{69,045,634}=0.8531. \\] Approximately 85.3% of the total variability in sale price is explained by the model using square feet and waterfront status, as well as an interaction between them as the explanatory variables. We calculate \\(R^2\\) directly in R. summary(M_House_Int)$r.squared ## [1] 0.853141 We see that adding an interaction term improved the proportion of variability in house price explained by the model from 0.76 to 0.85. This is a fairly notable increase. 2.3.5 Considerations for Using Interactions It might be tempting to think we should always add an interaction term to a model when using two or more explanatory variables. Afterall, an interaction term is just another term added to the model, meaning that \\(R^2\\) will never go down. Adding an interaction term is not always a good idea, though. We saw that doing so makes interpretations more complicated. Increasing the complexity of a model also increases the risk of overfitting, potentially hurting predictive performance on new data. We should only add an interaction term if we have strong reason to believe that the rate of change in the response variable with respect to one explanatory variable really does depend on the other variable. This might come from background knowledge about the subject, or consultation with an expert in the area. It could also come from data visualization, and the increase in variability in the response variable explained when an interaction term is added to the model. In the house price dataset, we might expect that the price of waterfront houses might increase more rapidly as they get bigger than the price of non-waterfront houses. The fact that the lines shown in the scatterplot are not close to being parallel provides further evidence of a difference in rate of increase, providing justification for the use of an interaction term in the model. Furthermore, \\(R^2\\) increases notably (from 0.76 to 0.85), when an interaction term is added. All of these reasons support using an interaction term in this context. When examining a scatterplot, we should note that even if there is truly no interaction among all houses, the lines probably won’t be exactly parallel, due to random deviations among the sample of houses chosen. If the lines are reasonably close to parallel, then an interaction term is likely not needed. We’ll look more at criteria for determining whether to add an interaction term to a model in the coming sections. 2.3.6 Interaction vs Correlation It is easy to confuse the concept of interaction with that of correlation. These are, in fact, very different concepts. A correlation between two variables means that as one increases, the other is more likely to increase or decrease. We only use the word correlation to describe two quantitative variables, but we could discuss the similar notion of a relationship between categorical variables. An interaction between two explanatory variables means that the effect of one on the response depends on the other. Examples of Correlations (or relationships) Houses on the waterfront tend to be bigger than houses not on the waterfront, so there is a relationship between square feet and waterfront status. Houses with large amounts of living space in square feet are likely to have more bedrooms, so there is a correlation between living space and bedrooms. Suppose that some genres of movies (drama, comedy, action, etc.) tend to be longer than others. This is an example of a relationship between genre and length. The fact that there is a correlation between explanatory variables is NOT a reason to add an interaction term involving those variables in a model. Correlation is something entirely different than interaction! Examples of Interactions As houses on the waterfront increase in size, their price increases more rapidly than for houses not on the waterfront. This means there is an interaction between size and waterfront location. Suppose that the effect of additional bedrooms on price is different for houses with lots of living space than for houses with little living space. This would be an example of an interaction between living space and number of bedrooms. Suppose that audiences become more favorable to dramas as they get longer, but less favorable to comedies as they get longer. In this scenario, the effect of movie length on audience rating depends on the genre of the movie, indicating an interaction between length and genre. 2.4 Least Squares Estimation (LSE) 2.4.1 Line of Best Fit In this section, we’ll discuss how the “line of best fit” and regression coefficients (\\(b_j\\)) we’ve seen are obtained. Let’s start with the model for predicting price of a house, using only size in square feet as an explanatory variable. ggplot(data=Houses, aes(x=sqft_living, y=price)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) + theme_bw() M_House_sqft ## ## Call: ## lm(formula = price ~ sqft_living, data = Houses) ## ## Coefficients: ## (Intercept) sqft_living ## -484.9575 0.5328 The line \\(\\text{Price} = -485 + 0.5328 \\times \\text{Square Feet}\\) is considered the “line of best fit” in the sense that it minimizes the sum of the squared residuals. This Rossman-Chance applet provides an illustration of the line of best fit. 2.4.2 LSE in Simple Linear Regression Consider a simple linear regression(SLR) model, which is one with a singe quantitative explanatory variable \\(x\\). \\(\\hat{y}_i = b_0+b_1x_i\\) we need to choose the values of \\(b_0\\) and \\(b_1\\) that minimize: \\[ \\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2 =\\displaystyle\\sum_{i=1}^n(y_i-(b_0+b_1x_i))^2 \\] We setup the equation by substituting in the values of \\(y_i\\) and \\(x_i\\) seen in the data. Recall the first 3 houses in the dataset: kable(First3Houses) Id price waterfront sqft_living 1 1225 No 5420 2 885 No 2830 3 385 No 1620 \\[ \\begin{aligned} \\displaystyle\\sum_{i=1}^{100}(y_i-\\hat{y}_i)^2 &amp; =\\displaystyle\\sum_{i=1}^n(y_i-(b_0+b_1x_i))^2 \\\\ &amp; = (1225-(b_0+b_1(5420)))^2 + (885-(b_0+b_1(2830)))^2 + (385-(b_0+b_1(1620)))^2 + \\ldots \\end{aligned} \\] We need to find the values of \\(b_0\\) and \\(b_1\\) that minimize this expression. This is a 2-dimensional optimization problem that can be solved using multivariable calculus, or numerical or graphical methods. Using calculus, it can be shown that this quantity is minimized when \\(b_1=\\frac{\\displaystyle\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\displaystyle\\sum_{i=1}^{n}(x_i-\\bar{x})^2}=\\frac{\\displaystyle\\sum_{i=1}^{n} x_i y_i-\\frac{\\displaystyle\\sum_{i=1}^{n} x_i \\displaystyle\\sum_{i=1}^{n} y_i }{n}}{\\left(\\displaystyle\\sum_{i=1}^{n} x_i^2 -\\frac{\\left(\\displaystyle\\sum_{i=1}^{n} x_i\\right)^2}{n}\\right)}\\) \\(b_0=\\bar{y}-b_1\\bar{x}\\) (where \\(\\bar{y}=\\frac{\\displaystyle\\sum_{i=1}^{n}{y_i}}{n}\\), and \\(\\bar{x}=\\frac{\\displaystyle\\sum_{i=1}^{n}{x_i}}{n}\\)). 2.4.3 LSE for Categorical Variable Consider a model with a single categorical variable (such as waterfront), with G+1 categories, numbered \\(g=0,2, \\ldots, G\\) Then \\(\\hat{y}_i = b_0 + b_1x_{i1} + \\ldots +b_{G}x_{iG}\\). we need to minimize \\[ \\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2 =\\displaystyle\\sum_{i=1}^n(y_i-(b_0 + b_1x_{i1} + \\ldots +b_{G}x_{iG}))^2. \\] It can be shown that this is achieved when \\(b_0 = \\bar{y_0}\\) (i.e. the average response in the “baseline group”), and \\(b_j = \\bar{y_j} - \\bar{y}_0\\) 2.4.4 LSE More Generally For multiple regression models, including those involving interaction, the logic is the same. We need to choose \\(b_0, b_1, \\ldots, b_p\\) in order to minimize \\[ \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2 = \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 \\] The mathematics, however are more complicated and require inverting a matrix. This goes beyond the scope of this class, so we will let R do the estimation and use the results. More on least squares estimation in multiple regression can be found here. 2.5 ANalysis Of VAriance 2.5.1 Submodels We’ve seen 5 different models for predicting house price using some combination of square feet and waterfront status. A model A is defined to be a submodel of another model B, if every term in model A is also included in model B. Model Variables Unexplained Variability Variability Explained \\(R^2\\) 0 None 69045634.1341747 0 0 1 Sq. Ft. 23767280.3817707 45278353.752404 0.6557743 2 Waterfront 43675043.0897012 25370591.0444736 0.3674467 3 Sq. Ft. and Waterfront 16521296.4889025 52524337.6452723 0.7607192 4 Sq. Ft., Waterfront, and Interaction 10139973.548359 58905660.5858157 0.853141 Model 1 is a submodel of Model 3, since all variables used in Model 1 are also used in Model 3. Model 2 is also a submodel of Model 3. Models 1, 2, and 3 are all submodels of Model 4. Model 0 is a submodel of Models 1, 2, 3, and 4. Models 1 and 2 are not submodels of each other, since Model 1 contains a variable used in Model 2 and Model 2 contains a variable not used in Model 1. 2.5.2 F-Statistics When one model is a submodel of another, we can compare the amount of variability explained by the models, using a technique known as ANalysis Of VAriance (ANOVA). Reduced Model: \\(\\hat{y}_i = b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_qx_{iq}\\) Full Model: \\(\\hat{y}_i = b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_qx_{iq} + b_{q+1}x_{i{q+1}} \\ldots + b_px_{ip}\\) p = # terms in Full Model, not including the intercept q = # terms in Reduced Model, not including the intercept n = number of observations We calculate a statistic called F that measures the amount of variability explained by adding additional variable(s) to the model, relative to the total amount of unexplained variability. \\[ \\begin{aligned} F &amp;= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} \\end{aligned} \\] Large values of F indicate that adding the additional explanatory variables is helpful in explaining variability in the response variable Small values of F indicate that adding new explanatory variables variables does not make much of a difference in explaining variability in the response variable What counts as “large” is depends on \\(n, p,\\) and \\(q\\). We will revisit this later in the course. Example 1 Let’s Calculate an ANOVA F-Statistic to compare Models 1 and 3. Reduced Model: \\(\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{sqft_living}\\) Full Model: \\(\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{sqft_living}+ b_2\\times\\text{Waterfront}\\) \\[ \\begin{aligned} F &amp;= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} \\\\ &amp;=\\frac{\\frac{23,767,280-16,521,296}{2-1}}{\\frac{16,521,296}{100-(2+1)}} \\\\ \\end{aligned} \\] ((SSR_sqft-SSR_wf_sqft)/(2-1))/((SSR_wf_sqft)/(100-(2+1))) ## [1] 42.54269 We can calculate the statistic directly in R, using the anova command. anova(M_House_sqft, M_wf_SqFt)$F[2] ## [1] 42.54269 In the coming chapters, we’ll talk about what to conclude from an F-statistic of 42.5 Is this big enough to say that adding waterfront status to a model already including square feet helps better explain variability in sale price? (Spoiler alert: YES - an F-statistic of 42.5 is quite large and indicative that the full model is a better choice than the reduced model.) We previously saw that the model including both square feet and waterfront status had a \\(R^2\\) value considerably higher than the one including only square feet. This large F-statistic is further evidence to the benefit of considering both variables in our model. Example 2 We’ll calculate an F-statistic to compare Models 3 and 4. This can help us determine whether it is worthwhile to include an interaction term in our model. Reduced Model: \\(\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{sqft_living} + b_2\\times\\text{Waterfront}\\) Full Model: \\(\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{sqft_living}+ b_2\\times\\text{Waterfront} + b_3\\times\\text{sqft_living}\\times\\text{Waterfront}\\) \\[ \\begin{aligned} F &amp;= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} \\\\ &amp;=\\frac{\\frac{16,521,296-10,139,974}{3-2}}{\\frac{10,139,974}{100-(3+1)}} \\\\ \\end{aligned} \\] ((SSR_wf_sqft-SSR_int)/(3-2))/((SSR_int)/(100-(3+1))) ## [1] 60.41505 We can calculate the statistic directly in R, using the anova command. anova(M_wf_SqFt, M_House_Int)$F[2] ## [1] 60.41505 We observe an F-statistic of 60, which is even bigger than the one seen previously! This suggests that adding the interaction term does indeed improve the model’s ability to account for variability in prices. 2.5.3 Comparing 3 or More Categories F-statistics are commonly used when making comparisons involving categorical variables with 3 or more categories. One variable in the houses dataset, which we haven’t looked at yet, is the condition of the house at the time of sale. The table shows the number of houses in each condition listed. summary(Houses$condition) ## poor fair average good very_good ## 1 1 59 30 9 We notice that there is only one house in poor condition and one house in fair condition. These sample sizes are too small to analyze. We’ll combine these two houses with those in the “average” category, creating a new category called “average or below). Houses$condition &lt;- fct_collapse(Houses$condition, &quot;average or below&quot; = c(&quot;poor&quot;,&quot;fair&quot;, &quot;average&quot;)) The boxplot shows the distribution of houses in each category, and the table below it provides a numerical summary. ggplot(data=Houses, aes(x=condition, y=price)) + geom_boxplot() +coord_flip() Cond_Tab &lt;- Houses %&gt;% group_by(condition) %&gt;% summarize(Mean_Price = mean(price), SD_Price= sd (price), N= n()) kable(Cond_Tab) condition Mean_Price SD_Price N average or below 700.6349 768.1179 61 good 861.0000 1048.9521 30 very_good 551.8361 332.8597 9 It can be helpful to calculate a single statistic that quantifies the size of the differences between the conditions. If we were just comparing two different categories, we could simply find the difference in mean prices between them. But, with three or more categories, we need a way to represent the size of the differences with a single number. An F-statistic can serve this purpose. We’ll calculate an F-statistic for a model that includes condition, compared to a model with only an intercept term. Reduced Model: \\(\\widehat{\\text{Price}}= b_0\\) Full Model: \\(\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{good condition}+ b_2\\times\\text{very good condition}\\) Notice that the equation includes separate variables for the “good” and “very” good conditions. These variables take on value 0 if the house is not in that condition, and 1 if the house is in that condition. Here, houses in “average or below” condition are considered the “baseline” category. We’ll fit the model in R. The coefficient estimates for \\(b_0\\), \\(b_1\\) and \\(b_2\\) are shown below. M_House_Cond &lt;- lm(data=Houses, price~condition) M_House_Cond ## ## Call: ## lm(formula = price ~ condition, data = Houses) ## ## Coefficients: ## (Intercept) conditiongood conditionvery_good ## 700.6 160.4 -148.8 The model equation is \\[ \\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{good condition}+ b_2\\times\\text{very good condition} \\] Interpretations On average, houses in average or below condition cost 700.6 thousand dollars. On average, houses in good condition cost 160.4 thousand dollars more than those in average or below condition. On average, houses in very good condition cost 148.8 thousand dollars less than those in average or below condition. This last sentence is surprising and merits further investigation. We’ll leave that for future consideration. For now, we’ll calculate an F-statistic based on the models. Note that in this case, the reduced model does not include any explanatory variables, so SSR is equal to SST, which we calculate previously. SST ## [1] 69045634 We calculate SSR for the full model. SSR_cond &lt;- sum(M_House_Cond$residuals^2) SSR_cond ## [1] 68195387 \\[ \\begin{aligned} F &amp;= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} \\\\ &amp;=\\frac{\\frac{69,045,634-68,195,387}{2-0}}{\\frac{68,195,387}{100-(2+1)}} \\\\ \\end{aligned} \\] ((SST - SSR_cond)/(2-0))/(SSR_cond/(100-(2+1))) ## [1] 0.6046888 We perform the calculation directly in R. anova(M_House_Cond, M0_House)$F[2] ## [1] 0.6046888 Notice that the F-statistic of 0.6 is considerably smaller than the F-statistics we’ve seen previously. This indicates that adding condition to a model with no other explanatory variables doesn’t seem to help improve the model’s ability to account for variation in price. Put another way, there doesn’t appear to be much evidence of difference in price between houses in the different conditions. 2.5.4 F-Statistic Illustration The figure below gives an illustration of data that would produce a large F-statistic (Scenario 1), and also data that would produce a small F-statistic (Scenario 2), like the one seen in the house condition data. An F-statistic compares the amount of variability between groups to the amount of variability within groups. In scenario 1, we notice considerable differences between the groups, relative to the amount of variability within groups. In this scenario, knowing the group an observation is in will help us predict the response for that group, so we should include account for the groups in our model. We would obtain a large F-statistic when comparing a model that includes group to one that contains only an intercept term. In scenario 2, there is little difference between the overall averages in each group, and more variability between individual observations within each group. In a scenario like this, knowing the group an observation lies in does little to help us predict the response. In this scenario, predictions from a model that includes group as an explantory variable would not be much better than those from a model that does not. Hence, we would obtain a small F-statistic. Scenario 1 Scenario 2 variation between groups High Low variation within groups Low High F Statistic Large Small Result Evidence of Group Differences No evidence of differences 2.5.5 Alternative F-Statistic Formula The above illustration suggests alternative (and mathematically equivalent) way to calculate the F-statistic. We calculate the ratio of variability between different groups, relative to the amount of variability within each group For a categorical variable with \\(g\\) groups, let \\(\\bar{y}_{1\\cdot}, \\ldots, \\bar{y}_{g\\cdot}\\) represent the mean response for each group. let \\(n_1, \\ldots, n_g\\) represent the sample size for each group Then \\(\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}n_i(y_{i\\cdot}-\\bar{y}_{\\cdot\\cdot})^2}{g-1}\\) gives a measure of how much the group means differ, and \\(\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i\\cdot})^2}{n-g}\\) gives a measure of how much individual observations differ within groups An alternative formula for this F-statistic is: \\[ F= \\frac{\\text{Variability between groups}}{\\text{Variability within groups}}= \\frac{\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}n_i(y_{i\\cdot}-\\bar{y}_{\\cdot\\cdot})^2}{g-1}}{\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i\\cdot})^2}{n-g}} \\] It can be shown that this statistic is equivalent to the one we saw previously. Example Let’s recalculate the F-statistic for the conditions of the houses, using this alternate formula. The first 3 houses are shown. kable(head(Houses %&gt;% select(Id, price, condition),3)) Id price condition 1 1225 average or below 2 885 average or below 3 385 good We have seen previously that: \\(\\bar{y}_{\\cdot\\cdot}=735.3526\\) (overall average price), and \\(n=10\\) \\(\\bar{y}_{1\\cdot}=700.6349\\) (average price for average or below houses), and \\(n_1=61\\) \\(\\bar{y}_{2\\cdot}=861.0\\) (average price for good houses), and \\(n_2=30\\) \\(\\bar{y}_{3\\cdot}=551.8361\\) (average price for very good houses), and \\(n_3=9\\) Then, \\(\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{i\\cdot}-\\bar{y}_{\\cdot\\cdot})^2}{g-1} = \\frac{61(700.6349-735.3526)^2+30(861.0-735.3526)^2+9(551.8361-735.3526)^2}{3-1} = \\frac{850247.3}{2}\\), and \\(\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i\\cdot})^2}{n-g} = \\frac{(1225.0-700.6349)^2+ (885.0 - 700.6349)^2 + (385.0-861.0)^2+\\ldots}{100-3} = \\frac{68195387}{97}\\) \\[ F= \\frac{\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}n_i(y_{i\\cdot}-\\bar{y}_{\\cdot\\cdot})^2}{g-1}}{\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i\\cdot})^2}{n-g}} = \\frac{\\frac{61(700.6349-735.3526)^2+30(861.0-735.3526)^2+9(551.8361-735.3526)^2}{3-1}}{\\frac{(1225.0-700.6349)^2+ (885.0 - 700.6349)^2 + (385.0-861.0)^2+\\ldots}{100-3}} = \\frac{\\frac{850247.3}{2}}{\\frac{68195387}{97}} \\] Note that the quantity in the the quantity in the third line is equivalent to the sum of the squared residuals using M2. Thus, we can calculate F using: ((61*(700.6349-735.3526)^2+30*(861.0-735.3526)^2+9*(551.8361-735.3526)^2)/(3-1))/((SSR_cond)/(100-3)) ## [1] 0.6046889 This interpretation of the F-statistic can be seen using the AOV command in R. AOV_condition &lt;- aov(data=Houses, price~condition) summary(AOV_condition) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## condition 2 850247 425124 0.605 0.548 ## Residuals 97 68195387 703045 The condition line represents the variability in average price between different conditions The Residuals line represents the variability between individual houses within each condition The first two columns give the quantities we use in our formula. The third column, representing the ratio of the first two columns is called a mean square. The last line in the summary output includes the F-statistic for the specified model, compared to a reduced model that includes only the intercept. Reduced Model: \\(\\widehat{Y}= b_0\\) Full Model: \\(\\widehat{Y}= b_0+ b_1 X_{i1}+ \\ldots+ b_p X_{ip}\\) This statistic addresses the question “Do any of the explanatory variables help explain variability in Y?”. For models with only one categorical explanatory variable, “variability within vs variability between” interpretation of an F-statistic is very popular. This statistic is often relevant in studies in the natural and social sciences. Such studies are often referred to as One-Way ANOVA’s. In fact, these are just a special case of the full vs reduced model interpretation of the F-statistic, which can be applied to any two models, as long as one is a submodel of the other. "],["interval-estimation.html", "Chapter 3 Interval Estimation 3.1 Sampling Distributions 3.2 Bootstrapping 3.3 Bootstrap Confidence Interval Example 3.4 Bootstrapping Cautions", " Chapter 3 Interval Estimation Learning Outcomes: State null and alternative hypotheses associated with models involving categorical and quantitative explanatory variables. Explain how to use permutation tests for hypotheses involving means, medians, F-statistics, slopes, and other regression coefficients, as well as functions of these statistics. Interpret p-values in context. Explain the conclusions we should draw from from a hypothesis test, while accounting for other information available in a dataset. Explain how to simultaneously test for differences between multiple groups. Distinguish between statistical significance and practical importance. 3.1 Sampling Distributions 3.1.1 Sampling From a Population In statistics, we often do not have the time, money, or means to collect data on all individuals or units on which we want to draw conclusions. Instead, we might collect data on only a subset of the individuals, and then make inferences about all individuals we are interested in, using the information we collected. Vocabulary: A population is the entire set of individuals that we want to draw conclusions about. A sample is a subset of a population A parameter is a numerical quantity pertaining to an entire population A statistic is a numerical quantity calculated from a sample We’ll work with a dataset containing information on all 20,591 flights from New York to Chicago in 2013. Our population of interest is all 20,591 flights. In this situation, we have information on the entire population, but suppose temporarily that we didn’t. Instead, suppose we had only information on a random sample of 75 flights. The parameter of interest is the proportion of on-time arrivals out of all flights in the population of 20,591. When the parameter is proportion, we’ll denote it with the letter \\(p\\). We take a sample of 75 flights. The first 6 flights in the sample are shown below. The ontime variable tells whether or not the flight arrived on time. set.seed(08082023) S1 &lt;- sample_n(Flights_NY_CHI, 75) head(S1) ## # A tibble: 6 × 9 ## year month day carrier origin dest sched_dep_time arr_delay ontime ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2013 3 8 AA LGA ORD 1720 106 N ## 2 2013 12 15 AA JFK ORD 1715 -24 Y ## 3 2013 10 22 UA EWR ORD 1300 -12 Y ## 4 2013 8 26 UA EWR ORD 2110 15 N ## 5 2013 7 23 AA LGA ORD 1359 66 N ## 6 2013 5 13 UA EWR ORD 900 -21 Y We’ll calculate the number, and proportion of flights that arrived on time. num_ontime &lt;- sum(S1$ontime == &quot;Y&quot;) # count number of on-time arrivals Number of on-time arrivals in the sample. num_ontime ## [1] 39 Proportion of on-time arrivals in the sample. p_hat &lt;- num_ontime/75 p_hat ## [1] 0.52 In out sample 52 percent of flights arrived on-time. We’ll denote this quantity \\(\\hat{p}\\), keeping with our convention of using the \\(\\hat{}\\) symbol to represent a quantity that is calculated from data (like the predictions and estimates we saw in the previous chapter). The sample statistic \\(\\hat{p}\\) is an estimate of the population proportion \\(p\\). Of course, this was just one sample of 75 flights. We should not expect the proportion of on-time flights in our sample (\\(\\hat{p}\\)) to exactly match the proportion of on-time flights in the entire population (\\(p\\)). Nevertheless we can use the sample to estimate the proportion of all flights in the population that arrive on time. Perhaps, we could say that we would expect between 42 and 62 percent of all 2013 flights from New York to arrive on time. Or, perhaps we could be more precise and estimate that the percentage of on-time flights to be between 47 and 57. We’ll need to figure out how precise we can make our range, while still being confident that it does, in fact, contain the true population parameter. To summarize: The population is all 20,591 flights from New York to Chicago in 2013. The sample is the flights that we randomly selected. The parameter \\(p\\) is the proportion of on-time arrivals among all 2013 flights from New York to Chicago, which we do not know (though in this particular example, we could find it, since we have data on all flights in the population.) The sample statistic is the proportion of flights in our sample that arrived on time, which we know to be \\(\\hat{p}\\) = 0.52. Now, let’s take a different sample of 75 flights and see how the proportion of on-time arrivals compares. S2 &lt;- sample_n(Flights_NY_CHI, 75) num_ontime2 &lt;- sum(S2$ontime == &quot;Y&quot;) # count number of on-time arrivals p_hat2 &lt;- num_ontime2/75 p_hat2 ## [1] 0.5066667 By studying the behavior of the proportion of on-time arrivals in different samples we can gauge how close the proportion in a given sample is likely be to the unknown population parameter. If all of our samples produce very similar estimates, then it is likely that the population parameter is close to these estimates. If the sample proportion varies considerably from sample to sample, then it is possible that the proportion in any given sample might be very different than the population parameter. Let’s take 10,000 more random samples of 75 flights and record the proportion of on-time arrivals in each sample. nreps &lt;- 10000 # number of repetitions p_hat_val &lt;- rep(NA, nreps) # create vector to hold proportion of on-time arrivals Sample &lt;- 1:nreps for(i in 1:nreps){ S &lt;- sample_n(Flights_NY_CHI, 75) # take sample of 75 N_ontime &lt;- sum(S$ontime == &quot;Y&quot;) # count number of on-time arrivals p_hat_val[i] &lt;- N_ontime/75 # record proportion on-time } Samples_df &lt;- data.frame(Sample, p_hat_val) # store results in a data frame The table shows the proportion of on-time arrivals in the first 20 samples of 75 flights. kable(head(Samples_df, 20)) Sample p_hat_val 1 0.6533333 2 0.5333333 3 0.5866667 4 0.6933333 5 0.5866667 6 0.6666667 7 0.6133333 8 0.5866667 9 0.6533333 10 0.6533333 11 0.6133333 12 0.5866667 13 0.6133333 14 0.4800000 15 0.6000000 16 0.4400000 17 0.4933333 18 0.5733333 19 0.6400000 20 0.5600000 The histogram below shows the distribution of the proportion of on-time arrivals in the 10,000 different samples. Prop_Samp_Dist&lt;- ggplot(data=Samples_df, aes(x=p_hat_val)) + geom_histogram(color=&quot;white&quot;, fill=&quot;blue&quot;) + ggtitle(&quot;Sampling Distribution for Proportion On Time&quot;) + xlab(&quot;Prop. on time in sample&quot;) Prop_Samp_Dist We notice that most of our 10,000 samples yielded proportions of on-time arrivals between 0.5 and 0.7, The distribution of proportion of on-time arrivals is roughly symmetric and bell-shaped. The distribution shown in this histogram is called the sampling distribution for $. We can gauge how much the proportion of on-time arrivals varies between samples by calculating the standard deviation of this sampling distribution. The standard deviation of a sampling distribution for a statistic is also called the standard error of the statistic. In this case it represents the standard error \\(\\hat{p}\\) (the proportion of on-time arrivals), and is denoted \\(\\text{SE}(\\hat{p})\\). This standard error is shown below. SE_p_hat &lt;- sd(Samples_df$p_hat_val) SE_p_hat ## [1] 0.05659102 **Vocabulary: The sampling distribution of a statistic is the distribution of values the statistic takes on across many different samples of a given size. The standard error of a statistic is the standard deviation of that statistic’s sampling distribution. It measures how much the statistic varies between different samples of a given size. In this rare situation, we actually have data on all 20,591 flights from New York to Chicago in 2013 (our entire population), Let’s calculate the true value of the population parameter \\(p\\), the proportion of flights that arrived on-time in our actual population. p &lt;- sum(Flights_NY_CHI$ontime == &quot;Y&quot;)/20591 p ## [1] 0.6079841 In fact, just over 60% of all flights in the population arrived on time. The sampling distribution for the proportion of on-time flights is shown again below. The true proportion of on-time flights is marked by the green dotted line. The gold bar at the bottom of the histogram represents the range of sample proportions that lie within \\(\\pm 2\\) standard errors of the true population proportion of flights that arrived on time. 0.6079841 - 2(0.056591) to 0.6079841 + 2(0.056591) Prop_Samp_Dist+ geom_vline(xintercept=p, color=&quot;green&quot;, linetype=&quot;dotted&quot;, linewidth=2) + geom_segment(aes(x=p - 2*SE_p_hat,xend=p + 2*SE_p_hat, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We calculate the proportion of samples whose proportion of on-time arrivals lies within \\(\\pm 2\\) standard errors of the true proportion. Lower &lt;- p - 2*SE_p_hat Upper &lt;- p + 2*SE_p_hat sum((Samples_df$p_hat_val &gt;=Lower) &amp; (Samples_df$p_hat_val &lt;= Upper)) ## [1] 9539 r sum((Samples_df$p_hat &gt;=Lower) &amp; (Samples_df$p_hat &lt;= Upper)) out of the 10,000 simulations (approximately 95%) of the samples produced proportions within \\(\\pm 2\\) standard errors of the true population proportion of on-time flights. 3.1.2 Confidence Intervals In a real situation, we won’t have access to the entire population of flights, only the flights in a single sample. For example, recall our original sample of 75 flights, in which we observed a proportion of on-time arrivals of \\(\\hat{p}=\\) 0.52. Since we now know that 95% of all samples produce proportions that lie within two standard errors of the population proportion, we can obtain an estimate of the population proportion \\(p\\) by adding and subtracting \\(2\\times \\text{SE}(\\hat{p})\\) from our observed sample proportion \\(\\hat{p}\\). Such an interval is called an approximate 95% confidence interval for the true population proportion \\(p\\). Approximate 95% Confidence Interval for \\(\\hat{p}\\) \\[ \\hat{p} \\pm 2\\times \\text{SE}(\\hat{p}) \\] The confidence interval, based on our original sample, is calculated below. c(p_hat - 2*SE_p_hat, p_hat + 2*SE_p_hat) ## [1] 0.406818 0.633182 Based on our sample of 75 flights, we can be 95% confident that the true proportion of on-time arrivals among all 2013 flights from New York to Chicago is between 0.406818 and p_hat + 2*SE_p_hat. In fact, knowing what we do about the true value of the population parameter \\(p\\), we can see that our confidence interval does indeed contain this value. Of course, in a real situaiton, we won’t know the true value of the population parameter, so we won’t know for sure whether or not our confidence interval contains this true parameter value. A pertinent question at this stage would be ``What does 95% confidence mean?“. To answer that, let’s explore what happens when we calculate confidence intervals based on estimates many different samples. For each of our 10,000 different samples taken from our population, we’ll add and subtract two standard errors from the sample proportion \\(\\hat{p}\\) corresponding to that sample. The table below displays the value of \\(\\hat{p}\\), for the first 20 samples we took, along with the lower and upper bounds of the confidence interval, and whether or not the confidence interval contains the true parameter value \\(p\\) (either TRUE or FALSE). Samples_df &lt;- Samples_df %&gt;% mutate( Lower = p_hat_val - 2*SE_p_hat, Upper = p_hat_val + 2*SE_p_hat, Contains = p &gt;= Lower &amp; p &lt;= Upper) kable(head(Samples_df, 20)) Sample p_hat_val Lower Upper Contains 1 0.6533333 0.5401513 0.7665154 TRUE 2 0.5333333 0.4201513 0.6465154 TRUE 3 0.5866667 0.4734846 0.6998487 TRUE 4 0.6933333 0.5801513 0.8065154 TRUE 5 0.5866667 0.4734846 0.6998487 TRUE 6 0.6666667 0.5534846 0.7798487 TRUE 7 0.6133333 0.5001513 0.7265154 TRUE 8 0.5866667 0.4734846 0.6998487 TRUE 9 0.6533333 0.5401513 0.7665154 TRUE 10 0.6533333 0.5401513 0.7665154 TRUE 11 0.6133333 0.5001513 0.7265154 TRUE 12 0.5866667 0.4734846 0.6998487 TRUE 13 0.6133333 0.5001513 0.7265154 TRUE 14 0.4800000 0.3668180 0.5931820 FALSE 15 0.6000000 0.4868180 0.7131820 TRUE 16 0.4400000 0.3268180 0.5531820 FALSE 17 0.4933333 0.3801513 0.6065154 FALSE 18 0.5733333 0.4601513 0.6865154 TRUE 19 0.6400000 0.5268180 0.7531820 TRUE 20 0.5600000 0.4468180 0.6731820 TRUE The graphic below visualizes the confidence intervals produced using the estimates from the first 100 samples. The green dotted line indicates the true value of \\(p\\). The black dots indicate the value of \\(\\hat{p}\\) for each sample. Intervals that do in fact contain the true value of \\(p\\) are shown in blue, and intervals that do not contain the true value of \\(p\\) are shown in green. ggplot(data=Samples_df[1:100,], aes(y=Sample, x=p_hat_val)) + geom_point() + geom_errorbar(aes(xmin = Lower, xmax = Upper, color=Contains)) + xlab(&quot;Confidence Interval&quot;) + ylab(&quot;Sample&quot;) + geom_vline(xintercept = p, color=&quot;green&quot;, linetype=&quot;dotted&quot;, size=2) + theme_bw() Out of these 100 samples, 0 contain the true value of the population parameter \\(p\\). This is close to the 95% confidence level. The picture shows confidence intervals produced by the first 100 samples, but we actually took 10,000 different samples of 75 flights. Let’s calculate how many of these samples produced confidence intervals that contain the true value of \\(p\\). sum(Samples_df$Contains == TRUE) ## [1] 9539 Again, notice that close to 95% of the samples produced confidence intervals contain the true population parameter \\(p\\). Note that for the red intervals that do not contain \\(p\\) nothing was done incorrectly. The sample was taken at random, and the confidence interval was calculated using the correct formula. It just happened that by chance, we obtained a sample proportion \\(\\hat{p}\\) that was unusually high or low, leading to an interval that did not capture the true population parameter. This, of course, happens rarely, and approximately 95% of the samples do, in fact, result in intervals that contain the true value of \\(p\\). This brings us back to the question “what does 95% confidence mean?”. An approximate 95% confidence interval means that if we take a large number of samples and calculate confidence intervals from each of them, then approximately 95% of the samples will produce intervals containing the true population parameter. In reality, we’ll only have on sample, and won’t know whether or not our interval contains the true parameter value. Assuming we have taken the sample and calculated the interval correctly, we can rest assured in the knowledge that that 95% of all intervals taken would contain the true parameter value, and hope that ours is among that 95%. We calculated the confidence interval by taking our sample statistic \\(\\hat{p}\\) plus/minus two standard errors. Confidence intervals that are calculated by adding and subtracting a certain number of standard errors from the sample statistic are called standard error confidence intervals. This approach will work as long as the sampling distribution is symmetric and bell-shaped. Probability theory tells us that in a symmetric and bell-shaped distribution, approximately 95% of the area lies within two standard errors of the center of the distribution, given by the true parameter value. We will, however, see that this approach will not work in all cases. Not all statistics produce sampling distributions that are symmetric and bell-shaped, and we will need an alternative way to calculate confidence intervals in these situations. knitr::include_graphics(&quot;Emp_Rule.png&quot;) Figure 3.1: Image from https://openintro-ims.netlify.app/foundations-mathematical If we want to use a level of confidence that is different than 95%, we can adjust the value we multiply the standard error by. In general, a standard error confidence interval has the form \\[ \\text{Statistic } \\pm m\\times \\text{Standard Error}, \\] where the value of \\(m\\) depends on the desired level of confidence. Of course, you might ask why we needed to calculate a confidence interval for the proportion of on-time flights in the first place, since we actually have data on all 20,591 flights in the population and already know the true proportion of on-time arrivals to be 0.608. The answer is that we don’t. But, in most real situations, we will only have data from a single sample, not the entire population, and we won’t know the true population parameter. We’ll be able to build on the ideas of sampling distributions and standard error that we learned about in this section to calculate confidence intervals in those scenarios. 3.2 Bootstrapping 3.2.1 Mercury Levels in Florida Lakes A 2004 study by Lange, T., Royals, H. and Connor, L. examined Mercury accumulation in large-mouth bass, taken from a sample of 53 Florida Lakes. If Mercury accumulation exceeds 0.5 ppm, then there are environmental concerns. In fact, the legal safety limit in Canada is 0.5 ppm, although it is 1 ppm in the United States. Figure 3.2: https://www.maine.gov/ifw/fish-wildlife/fisheries/species-information/largemouth-bass.html In our sample, we have data on 53 lakes, out of more than 30,000 lakes in the the state of Florida. We’ll attempt to draw conclusions about the entire population, consisting of all lakes in Florida, using data from our sample of 53. It is not clear how the lakes in this sample of 53 were selected, or how representative they are of all lakes in the state of Florida. Let’s assume for our purposes that the lakes in the sample can be reasonably thought of as being representative of all lakes in Florida. The mercury levels of the first 10 lakes in the sample are shown in the table below. data(&quot;FloridaLakes&quot;) FloridaLakes &lt;- FloridaLakes %&gt;% rename(Mercury = AvgMercury) kable(head(FloridaLakes %&gt;% select(ID, Lake, Mercury), 10)) ID Lake Mercury 1 Alligator 1.23 2 Annie 1.33 3 Apopka 0.04 4 Blue Cypress 0.44 5 Brick 1.20 6 Bryant 0.27 7 Cherry 0.48 8 Crescent 0.19 9 Deer Point 0.83 10 Dias 0.81 The histogram shows the distribution of mercury levels in the 53 lakes in the sample. Lakes exceeding the US standard of 1 ppm are shown in red. Lakes_Hist &lt;- ggplot(data=FloridaLakes, aes(x=Mercury)) + geom_histogram(aes(fill=Mercury&lt;=1), color=&quot;white&quot;, binwidth = 0.1) + ggtitle(&quot;Mercury Levels in Sample of 53 Florida Lakes&quot;) + xlab(&quot;Mercury Level&quot;) + ylab(&quot;Frequency&quot;) + theme_bw() Lakes_Hist The proportion of lakes with mercury levels exceeding 1 ppm is calculated below. p_hat &lt;- sum(FloridaLakes$Mercury &gt; 1)/53 p_hat ## [1] 0.1132075 We see that in our sample of 53 lakes, approximately 11% have mercury levels exceeding the US standard of 1 ppm. Suppose we want to estimate the proportion of all Florida Lakes whose mercury level exceeds this standard. As we saw in the previous section, we would not expect the population proportion to exactly match the sample, due to random variability between samples. We can use the sample proportion as an estimate (\\(\\hat{p} = 0.1132\\)), and construct a confidence interval for the unknown population proportion \\(p\\). In order to construct the confidence interval, we need to know how much the sample proportion of lakes exceeding 1 ppm \\(\\hat{p}\\) could vary between different samples of size 53. That is, we need to know the standard error of \\(\\hat{p}\\). In the previous section, we calculated the standard error by taking 10,000 different samples of the same size as ours from the population, calculating the proportion for each sample, and then calculating the standard deviation of the proportions obtained from these 10,000 different samples. This procedure will not work here, however, because unlike the previous example where we really did have data on the entire population of all flights from New York to Chicago, we do not have data on all 30,000+ lakes in Florida. We cannot take a lot of different samples of size 53 from the population of all lakes, and thus, cannot obtain the sampling distribution for the the proportion of lakes exceeding 1 ppm, or estimate the standard error of \\(\\hat{p}\\). 3.2.2 Bootstrap Sampling All we have is a single sample of 53 lakes. We need to figure out how much the proportion of lakes with mercury levels exceeding 1 ppm would vary between different samples of size 53, using only the information contained in our one sample. To do this, we’ll implement a popular simulation-based strategy, known as bootstrapping. Let’s assume our sample is representative of all Florida lakes. Then, we’ll duplicate the sample many times to create a large set that will look like the population of all Florida Lakes. We can then draw samples of 53 from that large population, and record the mean mercury level for each sample of 53. An illustration of the bootstrapping procedure is shown below, using a sample of 12 colored dots, instead of the 53 lakes. In fact, duplicating the sample many times and selecting new samples of size \\(n\\) has the same effect as drawing samples of size \\(n\\) from the original sample, by putting the item drawn back in each time, a procedure called sampling with replacement. Thus, we can skip the step of copying/pasting the sample many times, and instead draw our samples with replacement. This means that in each new sample, some lakes will be drawn multiple times and others not at all. It also ensures that each sample is different, allowing us to estimate variability in the sample mean between the different samples of size 53. An illustration of the concept of bootstrapping, using sampling with replacement is shown below. The variability in sample means in our newly drawn samples is used to approximate the variability in proportion \\(\\hat{p}\\) that would occur between different samples of 53 lakes, drawn from the population of all Florida Lakes. The point of bootstrapping is to observe how much a statistic (in this case the proportion of lakes with Mercury levels exceeding 1 ppm) varies between bootstrap samples. This can act as an estimate of how much that statistic would vary between different samples of size \\(n\\), drawn from the population. The steps of bootstrap sampling can be summarized in the following algorithm. Bootstrap Algorithm For an original sample of size \\(n\\): Take a sample size \\(n\\) by randomly sampling from the original, with replacement. Thus, some observations will show up multiple times, and others not at all. This sample is called a bootstrap sample. Calculate the statistic of interest in the bootstrap sample (in this case \\(\\hat{p}\\), the proportion of lakes whose mercury levels exceed 1 ppm). Repeat steps 1 and 2 many (say 10,000) times, keeping track of the statistic of interest that is calculated in each bootstrap sample. Look at the distribution of the statistic across bootstrap samples. The variability in this bootstrap distribution can be used to approximate the variability in the sampling distribution for the statistic of interest. 3.2.3 Bootstrap Samples of Lakes The sample_n() function samples the specified number rows from a data frame, with or without replacement. The lakes in the first sample are shown below. Notice that some lakes occur multiple times, and others not at all. Bootstrap Sample 1 BootstrapSample1 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake) BootstrapSample1 %&gt;% select(ID, Lake, Mercury) ## # A tibble: 53 × 3 ## ID Lake Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 Annie 1.33 ## 2 3 Apopka 0.04 ## 3 4 Blue Cypress 0.44 ## 4 6 Bryant 0.27 ## 5 9 Deer Point 0.83 ## 6 9 Deer Point 0.83 ## 7 10 Dias 0.81 ## 8 11 Dorr 0.71 ## 9 12 Down 0.5 ## 10 12 Down 0.5 ## # ℹ 43 more rows We calculate the proportion of lakes with mercury levels exceeding 1 ppm in this bootstrap sample. Note that if a lake shows up more than once in the bootstrap sample, then it is counted however many times it shows up. sum(BootstrapSample1$Mercury &gt; 1) / 53 ## [1] 0.0754717 *Bootstrap Sample #2 We take a second bootstrap sample. Notice that the lakes chosen and omitted differ from the first sample. BootstrapSample2 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake) BootstrapSample2 %&gt;% select(ID, Lake, Mercury) ## # A tibble: 53 × 3 ## ID Lake Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Alligator 1.23 ## 2 3 Apopka 0.04 ## 3 4 Blue Cypress 0.44 ## 4 4 Blue Cypress 0.44 ## 5 5 Brick 1.2 ## 6 7 Cherry 0.48 ## 7 8 Crescent 0.19 ## 8 8 Crescent 0.19 ## 9 8 Crescent 0.19 ## 10 8 Crescent 0.19 ## # ℹ 43 more rows Proportion exceeding 1 ppm: sum(BootstrapSample2$Mercury &gt; 1) / 53 ## [1] 0.0754717 Bootstrap Sample #3 We’ll take one more bootstrap sample and calculate the proportion of lakes with mercury levels exceeding 1 ppm. BootstrapSample3 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake) BootstrapSample3 %&gt;% select(ID, Lake, Mercury) ## # A tibble: 53 × 3 ## ID Lake Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 5 Brick 1.2 ## 2 5 Brick 1.2 ## 3 6 Bryant 0.27 ## 4 6 Bryant 0.27 ## 5 7 Cherry 0.48 ## 6 8 Crescent 0.19 ## 7 8 Crescent 0.19 ## 8 9 Deer Point 0.83 ## 9 9 Deer Point 0.83 ## 10 10 Dias 0.81 ## # ℹ 43 more rows Proportion exceeding 1 ppm: sum(BootstrapSample3$Mercury &gt; 1) / 53 ## [1] 0.0754717 3.2.4 Bootstrap Distribution Now that we have seen how bootstrap sampling works, we’ll take a large number (10,000) different bootstrap samples and examine how the proportion of lakes with mercury levels exceeding 1 ppm varies between samples. We’ll use a for-loop to take many different bootstrap samples and record the observed proportion in a vector called p_hat_b p_hat &lt;- sum(FloridaLakes$Mercury &gt; 1)/53 #calculate sample statistic Bootstrap_prop &lt;- rep(NA, 10000) #setup vector to hold bootstrap statistics for (i in 1:10000){ BootstrapSample &lt;- sample_n(FloridaLakes, 53, replace=TRUE) #take bootstrap sample Bootstrap_prop[i] &lt;- sum(BootstrapSample$Mercury &gt; 1)/53 # calc. prop exceeding 1 } Lakes_Bootstrap_Prop &lt;- data.frame(Bootstrap_prop) #store values in a dataframe The distribution of proportions observed in the 10,000 different bootstrap samples is shown below. This distribution is called the bootstrap distribution. Lakes_Bootstrap_Prop_plot &lt;- ggplot(data=Lakes_Bootstrap_Prop, aes(x=Bootstrap_prop)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;Prop &gt; 1 in Bootstrap Sample &quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Bootstrap Distribution for Prop. of Lakes Exeeding 1 ppm Hg&quot;) + theme(legend.position = &quot;none&quot;) Lakes_Bootstrap_Prop_plot 3.2.5 Bootstrap SE Confidence Interval We calculate the standard deviation of this bootstrap distribution, which is an estimate of the standard error of \\(\\hat{p}\\). SE_p_hat &lt;- sd(Lakes_Bootstrap_Prop$Bootstrap_prop) Since the bootstrap distribution is roughly symmetric and bell-shaped, we can calculate a 95% confidence interval for the proportion of all Florida lakes with mercury levels exceeding 1 ppm, using bootstrap standard error confidence interval method \\[ \\hat{p} \\pm 2\\times\\text{SE}(\\hat{p}) \\] c(p_hat - 2*SE_p_hat, p_hat + 2*SE_p_hat) ## [1] 0.02655044 0.19986466 We are 95% confident that the proportion of all Florida lakes with mercury levels exceeding 1 ppm is between 0.0265504 and 0.1998647. The gold bar at the bottom of the bootstrap distribution represents this 95% confidence interval. Lakes_Bootstrap_Prop_plot + geom_segment(aes(x=p_hat - 2*SE_p_hat,xend=p_hat + 2*SE_p_hat, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) 3.2.6 Bootstrap Distribution vs Sampling Distribution We stated that the standard error of the bootstrap distribution is meant to give us and estimate of the standard error of the statistic of interest (\\(\\hat{p}\\) in this case). We do note, however, that the bootstrap distribution is not the same as the sampling distribution for a statistic illustrated in the previous section. The sampling distribution shows the distribution of values the statistic would take on accross many different samples drawn from the population. As such, it will be centered at the true population parameter (\\(p\\) in this case). The bootstrap distribution, on the other hand, shows us the distribution of values the statistic takes on across different bootstrap samples drawn from the original sample, using replacement. Since it comes entirely from the original sample, rather than the population, it will be centered at the sample statistic ( in this case \\(\\hat{p}\\)), rather than the unknown \\(p\\). Nevertheless, the amount of variability in a statistic, calculated between different bootstrap samples, is often a reasonable approximation of the amount of variability we would observe in that statistic if we could take many samples of the same size from the entire population. 3.3 Bootstrap Confidence Interval Example 3.3.1 Bootstrapping Other Statistics We’ve seen how to use bootstrapping to calculate confidence intervals for an unknown population parameter \\(p\\), using an estimate \\(\\hat{p}\\), calculated from a sample of size \\(n\\). This procedure can be applied to calculate confidence intervals for a wide range of population parameters, using statistics calculated from a sample. For example, we could calculate confidence intervals any of the following parameters, using the corresponding sample statistic. Context Parameter Statistic Proportion \\(p\\) \\(\\hat{p}\\) Mean \\(\\mu\\) \\(\\bar{x}\\) Standard Deviation \\(\\sigma\\) \\(s\\) Median no comon abbreviations Difference in Means \\(\\mu_2-\\mu_1\\) \\(\\bar{x}_2 - \\bar{x}_1\\) Regression Coefficient \\(\\beta_j\\) \\(b_j\\) Estimated Regression Response \\(\\beta_0 + \\beta_1x_{i1} + \\ldots + \\beta_px_{ip}\\) \\(b_0 + b_1x_{i1} + \\ldots + b_px_{ip}\\) We follow the same algorithm as we did when working with a proportion, and simply calculate whatever statistic we are interested in step 2, in place of \\(\\hat{p}\\), as we did previously. The bootstrap algorithm is given again, below. Bootstrap Algorithm For an original sample of size \\(n\\): Take a sample of size \\(n\\) by randomly sampling from the original sample with replacement. (Thus some observations will show up multiple times and others not at all.) Calculate the statistic of interest in the bootstrap sample. Repeat steps 1 and 2 many (say 10,000) times, keeping track of the statistic of interest that is calculated in each bootstrap sample. Look at the distribution of the statistic across bootstrap samples. The variability in this bootstrap distribution can be used to approximate the variability in the sampling distribution for the statistic of interest. We’ll now go through examples, calculating bootstrap confidence intervals for each of the parameters listed above. 3.3.2 CI for Mean The histogram shows the distribution of mercury levels of the 53 lakes in our sample. The mean and standard deviation in mercury levels for these 53 lakes is shown. Lakes_Hist &lt;- ggplot(data=FloridaLakes, aes(x=Mercury)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;, binwidth = 0.2) + ggtitle(&quot;Mercury Levels in Sample of Florida Lakes&quot;) + xlab(&quot;Mercury Level&quot;) + ylab(&quot;Frequency&quot;) Lakes_Hist We’ll calculate the mean and median mercury level for the 53 lakes in the sample. Lakes_Stats &lt;- FloridaLakes %&gt;% summarize(MeanHg = mean(Mercury), StDevHG = sd(Mercury), N=n()) kable(Lakes_Stats) MeanHg StDevHG N 0.5271698 0.3410356 53 We want to calculate a 95% confidence interval for the mean mercury level among all Florida lakes. We’ll use bootstrapping again, this time using the sample mean, rather than the proportion exceeding 1 ppm, as our statistic of interest. Bootstrap Steps Take a sample of 53 lakes by randomly sampling from the original sample of 53 lakes, with replacement. Calculate the mean mercury level in the bootstrap sample. Repeat steps 1 and 2 many (say 10,000) times, keeping track of the mean mercury level in each bootstrap sample. Look at the distribution of the mean across bootstrap samples. The variability in this bootstrap distribution can be used to approximate the variability in the sampling distribution for the mean mercury level. We’ll illustrate the procedure on 3 bootstrap samples. Bootstrap Sample 1 BootstrapSample1 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake) BootstrapSample1 %&gt;% select(ID, Lake, Mercury) ## # A tibble: 53 × 3 ## ID Lake Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 Annie 1.33 ## 2 2 Annie 1.33 ## 3 3 Apopka 0.04 ## 4 4 Blue Cypress 0.44 ## 5 5 Brick 1.2 ## 6 12 Down 0.5 ## 7 14 East Tohopekaliga 1.16 ## 8 13 Eaton 0.49 ## 9 16 George 0.15 ## 10 16 George 0.15 ## # ℹ 43 more rows We calculate the mean mercury level among the lakes in the bootstrap sample. mean(BootstrapSample1$Mercury) ## [1] 0.5292453 Bootstrap Sample #2 BootstrapSample2 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake) BootstrapSample2 %&gt;% select(ID, Lake, Mercury) ## # A tibble: 53 × 3 ## ID Lake Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Alligator 1.23 ## 2 1 Alligator 1.23 ## 3 1 Alligator 1.23 ## 4 2 Annie 1.33 ## 5 3 Apopka 0.04 ## 6 3 Apopka 0.04 ## 7 3 Apopka 0.04 ## 8 9 Deer Point 0.83 ## 9 12 Down 0.5 ## 10 12 Down 0.5 ## # ℹ 43 more rows Mean Mercury Level: mean(BootstrapSample2$Mercury) ## [1] 0.4777358 Bootstrap Sample #3 BootstrapSample3 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake) BootstrapSample3 %&gt;% select(ID, Lake, Mercury) ## # A tibble: 53 × 3 ## ID Lake Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Alligator 1.23 ## 2 3 Apopka 0.04 ## 3 6 Bryant 0.27 ## 4 7 Cherry 0.48 ## 5 8 Crescent 0.19 ## 6 10 Dias 0.81 ## 7 10 Dias 0.81 ## 8 11 Dorr 0.71 ## 9 12 Down 0.5 ## 10 12 Down 0.5 ## # ℹ 43 more rows Mean Mercury Level: mean(BootstrapSample3$Mercury) ## [1] 0.4860377 Now, we’ll take 10,000 bootstrap samples, and record the mean mercury concentration in each sample. mean &lt;- mean(FloridaLakes$Mercury) #calculate sample statistic Bootstrap_Mean &lt;- rep(NA, 10000) # setup vector to hold bootstrap statistics for (i in 1:10000){ BootstrapSample &lt;- sample_n(FloridaLakes, 53, replace=TRUE) # take bootstrap sample Bootstrap_Mean[i] &lt;- mean(BootstrapSample$Mercury) # calculate mean in bootstrap sample } Lakes_Bootstrap_Results_Mean &lt;- data.frame(Bootstrap_Mean) #store results in data frame The bootstrap distribution for the mean mercury level is shown below, along with its standard error. Lakes_Bootstrap_Mean_Plot &lt;- ggplot(data=Lakes_Bootstrap_Results_Mean, aes(x=Bootstrap_Mean)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;Mean Mercury in Bootstrap Sample &quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Bootstrap Distribution for Sample Mean in Florida Lakes&quot;) + theme(legend.position = &quot;none&quot;) Lakes_Bootstrap_Mean_Plot We’ll calculate the standard error of the mean. This is a measure of how much the mean varies between samples. SE_mean &lt;- sd(Lakes_Bootstrap_Results_Mean$Bootstrap_Mean) SE_mean ## [1] 0.045862 Notice that the standard error of the mean is much less than the sample standard deviation of 0.527. Interpretations of sample standard deviation and standard error of the mean The sample standard deviation measures the amount of variability in mercury levels between the 53 individual lakes in our sample. The standard error of the mean measures the amount of variability in sample mean mercury levels between different samples of size 53. There is more variability between mercury levels in individual lakes than there is between average mercury levels in different samples of size 53. Since the bootstrap distribution is roughly symmetric and bell-shaped, we can use the bootstrap standard error method to calculate an approximate 95% confidence interval for the mean mercury level among all Florida lakes. \\[ \\text{Statistic} \\pm 2\\times\\text{Standard Error} \\] In this case, the statistic of interest is the sample mean \\(\\bar{x}=0.527\\). The confidence interval is \\[ \\begin{aligned} &amp; \\bar{x} \\pm 2\\times\\text{SE}(\\bar{x}) \\\\ &amp; = 0.527 \\pm 2\\times{0.458} \\end{aligned} \\] 95% Confidence Interval: c(mean - 2*SE_mean, mean + 2*SE_mean) ## [1] 0.4354458 0.6188938 The 95% confidence interval is shown by the gold bar on the graph of the bootstrap distribution below. Lakes_Bootstrap_Mean_Plot + geom_segment(aes(x=mean - 2*SE_mean,xend=mean + 2*SE_mean, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We are 95% confident that the average mercury level among all Florida lakes is between 0.4354458 and 0.6188938 parts per million. 3.3.3 CI for Standard Deviation Now, we’ll calculate a confidence interval for the standard deviation in mercury levels among all Florida lakes. Recall that the sample standard deviation (\\(s\\)) was: Sample_SD &lt;- sd(FloridaLakes$Mercury) Sample_SD ## [1] 0.3410356 We’ll use this estimate to calculate a confidence interval for the population standard deviation \\(\\sigma\\). This time, our statistic of interest is the sample standard deviation \\(s\\). Bootstrap Steps Take a sample of 53 lakes by randomly sampling from the original sample of 53 lakes, with replacement. Calculate the standard deviation in mercury level in the bootstrap sample. Repeat steps 1 and 2 many (say 10,000) times, keeping track of the standard deviation mercury level in each bootstrap sample. Look at the distribution of the standard deviations across bootstrap samples. The variability in this bootstrap distribution can be used to approximate the variability in the sampling distribution for the standard deviation in mercury level. We’ll illustrate the procedure on 3 bootstrap samples. Bootstrap Sample 1 BootstrapSample1 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake) BootstrapSample1 %&gt;% select(ID, Lake, Mercury) ## # A tibble: 53 × 3 ## ID Lake Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 3 Apopka 0.04 ## 2 3 Apopka 0.04 ## 3 4 Blue Cypress 0.44 ## 4 9 Deer Point 0.83 ## 5 10 Dias 0.81 ## 6 10 Dias 0.81 ## 7 10 Dias 0.81 ## 8 11 Dorr 0.71 ## 9 12 Down 0.5 ## 10 12 Down 0.5 ## # ℹ 43 more rows We calculate the standard deviation in mercury levels among the lakes in the bootstrap sample. sd(BootstrapSample1$Mercury) ## [1] 0.2790135 Bootstrap Sample #2 BootstrapSample2 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake) BootstrapSample2 %&gt;% select(ID, Lake, Mercury) ## # A tibble: 53 × 3 ## ID Lake Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Alligator 1.23 ## 2 1 Alligator 1.23 ## 3 1 Alligator 1.23 ## 4 1 Alligator 1.23 ## 5 2 Annie 1.33 ## 6 2 Annie 1.33 ## 7 2 Annie 1.33 ## 8 6 Bryant 0.27 ## 9 7 Cherry 0.48 ## 10 9 Deer Point 0.83 ## # ℹ 43 more rows Standard Deviation in Mercury Level: sd(BootstrapSample2$Mercury) ## [1] 0.3788695 Bootstrap Sample #3 BootstrapSample3 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake) BootstrapSample3 %&gt;% select(ID, Lake, Mercury) ## # A tibble: 53 × 3 ## ID Lake Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Alligator 1.23 ## 2 2 Annie 1.33 ## 3 2 Annie 1.33 ## 4 2 Annie 1.33 ## 5 3 Apopka 0.04 ## 6 5 Brick 1.2 ## 7 5 Brick 1.2 ## 8 7 Cherry 0.48 ## 9 7 Cherry 0.48 ## 10 8 Crescent 0.19 ## # ℹ 43 more rows Standard Deviation Mercury Level: sd(BootstrapSample3$Mercury) ## [1] 0.3881135 Now, we’ll take 10,000 bootstrap samples, and record the standard deviation in mercury concentration in each sample. Sample_SD &lt;- sd(FloridaLakes$Mercury) #calculate sample statistic Bootstrap_SD &lt;- rep(NA, 10000) # setup vector to hold bootstrap statistics for (i in 1:10000){ BootstrapSample &lt;- sample_n(FloridaLakes, 53, replace=TRUE) # take bootstrap sample Bootstrap_SD[i] &lt;- sd(BootstrapSample$Mercury) # calculate standard deviation in bootstrap sample } Lakes_Bootstrap_Results_SD &lt;- data.frame(Bootstrap_SD) #store results in data frame The bootstrap distribution for the mean mercury level is shown below, along with its standard error. Lakes_Bootstrap_SD_Plot &lt;- ggplot(data=Lakes_Bootstrap_Results_SD, aes(x=Bootstrap_SD)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;SD in Mercury in Bootstrap Sample &quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Bootstrap Distribution for Sample SD in Florida Lakes&quot;) + theme(legend.position = &quot;none&quot;) Lakes_Bootstrap_SD_Plot We’ll calculate the standard error of the standard deviation. This is a measure of how much the standard deviation varies between samples. SE_SD &lt;- sd(Lakes_Bootstrap_Results_SD$Bootstrap_SD) SE_SD ## [1] 0.0286248 Since the bootstrap distribution is roughly symmetric and bell-shaped, we can use the bootstrap standard error method to calculate an approximate 95% confidence interval for the standard deviation in mercury levels among all Florida lakes. \\[ \\text{Statistic} \\pm 2\\times\\text{Standard Error} \\] In this case, the statistic of interest is the sample standard deviation \\(s=0.341\\). The confidence interval is \\[ \\begin{aligned} &amp; s \\pm 2\\times\\text{SE}(s) \\\\ &amp; = 0.341 \\pm 2\\times{0.029} \\end{aligned} \\] 95% Confidence Interval: c(Sample_SD - 2*SE_SD, Sample_SD + 2*SE_SD ) ## [1] 0.2837860 0.3982852 The 95% confidence interval is shown by the gold bar on the graph of the bootstrap distribution below. Lakes_Bootstrap_SD_Plot + geom_segment(aes(x=Sample_SD - 2*SE_SD,xend=Sample_SD + 2*SE_SD, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We are 95% confident that the standard deviation in mercury levels among all Florida lakes is between 0.4699202 and 0.5844194 parts per million. 3.3.4 CI for Median We already calculated a confidence interval for the mean mercury level among all Florida lakes. We could calculate a bootstrap confidence interval for the median mercury level as well, but since the distribution of mercury levels in the lakes is roughly symmetric, the mean is a reasonable measure of center, and there is not a clear reason for using the median instead. When a distribution is skewed or contains large outliers, however, the median is a more robust measure of center than the mean. Recall the distribution of 100 Seattle house prices seen in Chapters 1 and 2. ggplot(data=Houses, aes(x=price)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + ggtitle(&quot;Distribution of House Prices&quot;) + xlab(&quot;Price&quot;) + ylab(&quot;Frequency&quot;) These 100 houses are a sample of all houses sold in Seattle in 2014 and 2015, so we can use statistics from our sample to draw conclusions about all houses sold in Seattle in this time period. In this subsection, we’ll use bootstrapping to calculate a 95% confidence interval for the median price among all houses sold in Seattle in this time period. We calculate the sample median price. Sample_Median &lt;- median(Houses$price) Sample_Median ## [1] 507.5 Bootstrap Steps Take a sample of 100 houses by randomly sampling from the original sample of 100 houses, with replacement. Calculate the median price in the bootstrap sample. Repeat steps 1 and 2 many (say 10,000) times, keeping track of the median price in each bootstrap sample. Look at the distribution of the median price across bootstrap samples. The variability in this bootstrap distribution can be used to approximate the variability in the sampling distribution for the median price. We’ll illustrate the procedure on 3 bootstrap samples. Bootstrap Sample 1 BootstrapSample1 &lt;- sample_n(Houses, 100, replace=TRUE) %&gt;% arrange(Id) BootstrapSample1 %&gt;% select(Id, price) ## # A tibble: 100 × 2 ## Id price ## &lt;int&gt; &lt;dbl&gt; ## 1 2 885. ## 2 4 253. ## 3 4 253. ## 4 7 550. ## 5 8 485. ## 6 11 410 ## 7 12 571 ## 8 12 571 ## 9 13 1325 ## 10 14 696 ## # ℹ 90 more rows We calculate the median price among the houses in the bootstrap sample. median(BootstrapSample1$price) ## [1] 543.5 Bootstrap Sample #2 BootstrapSample2 &lt;- sample_n(Houses, 100, replace=TRUE) %&gt;% arrange(Id) BootstrapSample2 %&gt;% select(Id, price) ## # A tibble: 100 × 2 ## Id price ## &lt;int&gt; &lt;dbl&gt; ## 1 6 310. ## 2 7 550. ## 3 7 550. ## 4 9 315. ## 5 10 425 ## 6 12 571 ## 7 13 1325 ## 8 15 605. ## 9 16 3075 ## 10 17 438 ## # ℹ 90 more rows Median Price: median(BootstrapSample2$price) ## [1] 484 Bootstrap Sample #3 BootstrapSample3 &lt;- sample_n(Houses, 100, replace=TRUE) %&gt;% arrange(Id) BootstrapSample3 %&gt;% select(Id, price) ## # A tibble: 100 × 2 ## Id price ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1225 ## 2 2 885. ## 3 3 385. ## 4 4 253. ## 5 6 310. ## 6 6 310. ## 7 7 550. ## 8 7 550. ## 9 8 485. ## 10 9 315. ## # ℹ 90 more rows Median Price: median(BootstrapSample3$price) ## [1] 520 Now, we’ll take 10,000 bootstrap samples, and record the median price in each sample. Sample_Med &lt;- median(Houses$price) #calculate sample median Bootstrap_Med &lt;- rep(NA, 10000) # setup vector to hold bootstrap statistics for (i in 1:10000){ BootstrapSample &lt;- sample_n(Houses, 100, replace=TRUE) # take bootstrap sample Bootstrap_Med[i] &lt;- median(BootstrapSample$price) # calculate standard deviation in bootstrap sample } Houses_Bootstrap_Results_Med &lt;- data.frame(Bootstrap_Med) #store results in data frame The bootstrap distribution for the median price is shown below, along with its standard error. Houses_Bootstrap_Med_Plot &lt;- ggplot(data=Houses_Bootstrap_Results_Med, aes(x=Bootstrap_Med)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;Median Price in Bootstrap Sample &quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Bootstrap Distribution for Median Price in Seattle Houses&quot;) + theme(legend.position = &quot;none&quot;) Houses_Bootstrap_Med_Plot We’ll calculate the standard error of the median. This is a measure of how much the median varies between samples. SE_Med &lt;- sd(Houses_Bootstrap_Results_Med$Bootstrap_Med) SE_Med ## [1] 46.936 The standard error measures the amount of variability in median house price between different samples of size 100. Note that this is different than the sample standard deviation, which represents the standard deviation in prices between the 100 different houses in the sample. Notice that the bootstrap distribution for the median is not symmetric and bell-shaped. Thus, we cannot be assured that 95% of samples will produce a statistic within two standard errors of the mean, so the standard error confidence interval method is not appropriate here. Instead, we’ll calculate a confidence interval by taking the middle 95% of the values in the bootstrap distribution. We’ll calculate the 0.025 quantile and the 0.975 quantile of this bootstrap distribution. These are the points below which lie 2.5% and 97.5% of the medians in the bootstrap distribution. Thus, the middle 95% of the medians lie between these values. q.025 &lt;- quantile(Houses_Bootstrap_Results_Med$Bootstrap_Med, 0.025) q.025 ## 2.5% ## 410 q.975 &lt;- quantile(Houses_Bootstrap_Results_Med$Bootstrap_Med, 0.975) q.975 ## 97.5% ## 600 The 95% confidence interval is shown by the gold bar on the graph of the bootstrap distribution below. Houses_Bootstrap_Med_Plot + geom_segment(aes(x=q.025,xend=q.975, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We are 95% confident that the median price among all houses that sold in Seattle between 2014 and 2015 is between 410 and 600 thousand dollars. 3.3.5 CI for Difference in Means We previously calculated a confidence interval for the average mercury level among all lakes in Florida. Now, we’ll calculate an interval for the difference in average mercury levels between lakes in Northern Florida, compared to Southern Florida. The boxplot shows and table below describe the distribution of mercury levels for lakes in Northern Florida, compared to Southern Florida. LakesBP &lt;- ggplot(data=FloridaLakes, aes(x=Location, y=Mercury, fill=Location)) + geom_boxplot() + geom_jitter() + ggtitle(&quot;Mercury Levels in Florida Lakes&quot;) + xlab(&quot;Location&quot;) + ylab(&quot;Mercury Level&quot;) + theme(axis.text.x = element_text(angle = 90)) + coord_flip() LakesBP LakesTable &lt;- FloridaLakes %&gt;% group_by(Location) %&gt;% summarize(MeanHg=mean(Mercury), StDevHg=sd(Mercury), N=n()) LakesTable ## # A tibble: 2 × 4 ## Location MeanHg StDevHg N ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 N 0.425 0.270 33 ## 2 S 0.696 0.384 20 In our sample of 33 Northern Lakes and 20 Southern Lakes, we saw a difference of 0.27 ppm. We’ll calculate a confidence interval to estimate how big or small this difference could be among all Florida lakes. We’ll use a statistical model to calculate the average mercury levels in Northern and Southern Florida. \\(\\widehat{\\text{Mercury}} = b_0 +b_1\\text{I}_{\\text{South}}\\) \\(b_0\\) represents the mean mercury level for lakes in North Florida, and \\(b_1\\) represents the mean difference in mercury level for lakes in South Florida, compared to North Florida The estimates for corresponding to the original sample are shown below. M &lt;- lm(data=FloridaLakes, Mercury~Location) M ## ## Call: ## lm(formula = Mercury ~ Location, data = FloridaLakes) ## ## Coefficients: ## (Intercept) LocationS ## 0.4245 0.2720 Thus, we can obtain a confidence interval for the difference in average mercury levels by fitting a regression model to each of our bootstrap samples and recording the value of the sample statistic \\(b_1\\), which represents this difference. Alternatively, we could calculate the mean from each group separately and subtract. When comparing groups, we make one modification in Step #1 of the bootstrap process. Rather than drawing a sample of size \\(n\\) at random, with replacement, we’ll draw the same number of observations from each group as were in the original sample. In this case, we had 33 northern lakes, and 20 southern lakes. Bootstrap Steps Take a sample of 33 northern lakes and 20 southern lakes by randomly sampling from the original sample, with replacement. Fit a regression model with location as the explanatory variable and record the value of \\(b_1\\), representing the difference between the means for each group (South-North). Repeat steps 1 and 2 many (say 10,000) times, keeping track of the difference in means in each bootstrap sample. Look at the distribution of the differences in means across bootstrap samples. The variability in this bootstrap distribution can be used to approximate the variability in the sampling distribution for the difference in means between mercury levels in Northern and Southern Florida. We’ll illustrate the procedure on 3 bootstrap samples. Bootstrap Sample 1 NLakes &lt;- sample_n(FloridaLakes %&gt;% filter(Location==&quot;N&quot;), 33, replace=TRUE) ## sample 33 northern lakes SLakes &lt;- sample_n(FloridaLakes %&gt;% filter(Location==&quot;S&quot;), 20, replace=TRUE) ## sample 20 southern lakes BootstrapSample1 &lt;- rbind(NLakes, SLakes) %&gt;% arrange(ID) %&gt;% select(ID, Lake, Location, Mercury) ## combine Northern and Southern Lakes BootstrapSample1 ## # A tibble: 53 × 4 ## ID Lake Location Mercury ## &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 2 Annie S 1.33 ## 2 4 Blue Cypress S 0.44 ## 3 4 Blue Cypress S 0.44 ## 4 4 Blue Cypress S 0.44 ## 5 5 Brick S 1.2 ## 6 5 Brick S 1.2 ## 7 6 Bryant N 0.27 ## 8 6 Bryant N 0.27 ## 9 7 Cherry N 0.48 ## 10 10 Dias N 0.81 ## # ℹ 43 more rows We fit a regression model to the bootstrap sample and calculate the regression coefficients. We’re interested in the second coefficient, \\(b_1\\), which represents the mean difference between lakes in Southern and Northern Florida Mb1 &lt;- lm(data=BootstrapSample1, Mercury ~ Location) ## fit linear model Mb1 ## ## Call: ## lm(formula = Mercury ~ Location, data = BootstrapSample1) ## ## Coefficients: ## (Intercept) LocationS ## 0.4376 0.2889 NLakes &lt;- sample_n(FloridaLakes %&gt;% filter(Location==&quot;N&quot;), 33, replace=TRUE) ## sample 33 northern lakes SLakes &lt;- sample_n(FloridaLakes %&gt;% filter(Location==&quot;S&quot;), 20, replace=TRUE) ## sample 20 southern lakes BootstrapSample2 &lt;- rbind(NLakes, SLakes) %&gt;% arrange(ID) %&gt;% select(ID, Lake, Location, Mercury) ## combine Northern and Southern Lakes BootstrapSample2 ## # A tibble: 53 × 4 ## ID Lake Location Mercury ## &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1 Alligator S 1.23 ## 2 4 Blue Cypress S 0.44 ## 3 6 Bryant N 0.27 ## 4 7 Cherry N 0.48 ## 5 8 Crescent N 0.19 ## 6 8 Crescent N 0.19 ## 7 8 Crescent N 0.19 ## 8 9 Deer Point N 0.83 ## 9 9 Deer Point N 0.83 ## 10 10 Dias N 0.81 ## # ℹ 43 more rows Bootstrap Sample 2 Mb2 &lt;- lm(data=BootstrapSample2, Mercury ~ Location) ## fit linear model Mb2 ## ## Call: ## lm(formula = Mercury ~ Location, data = BootstrapSample2) ## ## Coefficients: ## (Intercept) LocationS ## 0.4521 0.2814 Bootstrap Sample 3 NLakes &lt;- sample_n(FloridaLakes %&gt;% filter(Location==&quot;N&quot;), 33, replace=TRUE) ## sample 33 northern lakes SLakes &lt;- sample_n(FloridaLakes %&gt;% filter(Location==&quot;S&quot;), 20, replace=TRUE) ## sample 20 southern lakes BootstrapSample3 &lt;- rbind(NLakes, SLakes) %&gt;% arrange(ID) %&gt;% select(ID, Lake, Location, Mercury) ## combine Northern and Southern Lakes BootstrapSample3 ## # A tibble: 53 × 4 ## ID Lake Location Mercury ## &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1 Alligator S 1.23 ## 2 2 Annie S 1.33 ## 3 4 Blue Cypress S 0.44 ## 4 5 Brick S 1.2 ## 5 6 Bryant N 0.27 ## 6 7 Cherry N 0.48 ## 7 9 Deer Point N 0.83 ## 8 9 Deer Point N 0.83 ## 9 11 Dorr N 0.71 ## 10 11 Dorr N 0.71 ## # ℹ 43 more rows Mb3 &lt;- lm(data=BootstrapSample3, Mercury ~ Location) ## fit linear model Mb3 ## ## Call: ## lm(formula = Mercury ~ Location, data = BootstrapSample3) ## ## Coefficients: ## (Intercept) LocationS ## 0.4991 0.1619 We’ll now take 10,000 different bootstrap samples and look at the bootstrap distribution for \\(b_1\\), the difference in mean mercury levels between lakes in Southern and Northern Florida M &lt;- lm(data=FloridaLakes, Mercury~Location) #fit model to original sample Sample_b1 &lt;- M$coefficients[2] # record b1 value (second coefficient) Bootstrap_b1 &lt;- rep(NA, 10000) #vector to store b1 values for (i in 1:10000){ NLakes &lt;- sample_n(FloridaLakes %&gt;% filter(Location==&quot;N&quot;), 33, replace=TRUE) ## sample 33 northern lakes SLakes &lt;- sample_n(FloridaLakes %&gt;% filter(Location==&quot;S&quot;), 20, replace=TRUE) ## sample 20 southern lakes BootstrapSample &lt;- rbind(NLakes, SLakes) ## combine Northern and Southern Lakes M &lt;- lm(data=BootstrapSample, Mercury ~ Location) ## fit linear model Bootstrap_b1[i] &lt;- M$coefficients[2] ## record b1 } NS_Lakes_Bootstrap_Results &lt;- data.frame(Bootstrap_b1) #save results as dataframe The bootstrap distribution for the difference in means, \\(b_1\\), is shown below, along with the standard error for the difference. NS_Lakes_Bootstrap_Plot_b1 &lt;- ggplot(data=NS_Lakes_Bootstrap_Results, aes(x=Bootstrap_b1)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;Mean Difference (b1) in Bootstrap Sample&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Northern vs Southern Lakes: Bootstrap Distribution for b1&quot;) NS_Lakes_Bootstrap_Plot_b1 Standard Error of the Difference in Means \\(b_1\\) SE_b1 &lt;- sd(NS_Lakes_Bootstrap_Results$Bootstrap_b1) SE_b1 ## [1] 0.09563947 The bootstrap distribution is symmetric and bell-shaped, so we can use the standard error method to calculate a 95% confidence interval. \\[ \\begin{aligned} &amp; b_1 \\pm 2\\times\\text{SE}(b_1) \\\\ &amp; = 0.271 \\pm 2\\times{0.095} \\end{aligned} \\] 95% Confidence Interval: c(Sample_b1 - 2*SE_b1, Sample_b1 + 2*SE_b1) ## LocationS LocationS ## 0.0806756 0.4632335 The 95% confidence interval is shown by the gold bar on the graph of the bootstrap distribution below. NS_Lakes_Bootstrap_Plot_b1 + geom_segment(aes(x=Sample_b1 - 2*SE_b1,xend=Sample_b1 + 2*SE_b1, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We are 95% confident that the mean mercury level among all lakes in Southern Florida is between 0.0806756 and 0.4632335 higher than the mean mercury level among all lakes in Northern Florida. Question: We previously performed a hypothesis test and concluded that there was evidence that mean mercury level was higher for lakes in South Florida than Northern Florida. Is this confidence interval consistent with the result of the hypothesis test? Why or why not? 3.3.6 CI for Regression Slope In addition to the mercury levels of the Florida lakes, we have data on the pH level of each lake. pH level measures the acidity of a lake, ranging from 0 to 14, with 7 being neutral, and lower levels indicating more acidity. We plot the pH level against the mercury level in our sample of 53 lakes. ggplot(data=FloridaLakes, aes(y=Mercury, x=pH)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) The regression equation is \\[ \\widehat{\\text{Mercury}} = b_0 + b_1\\times\\text{pH} \\] Regression estimates \\(b_0\\) and \\(b_1\\) are shown below. M &lt;- lm(data=FloridaLakes, Mercury~pH) M ## ## Call: ## lm(formula = Mercury ~ pH, data = FloridaLakes) ## ## Coefficients: ## (Intercept) pH ## 1.5309 -0.1523 On average, lakes with pH level 0 are expected to have a mercury level of 1.53 ppm. For each one-unit increase in pH, mercury level is expected to decrease by 0.15 ppm. These estimates are sample statistics, calculated from our sample of 53 lakes. We can think of our regression equation estimates \\(b_0\\) and \\(b_1\\) as estimates of parameters \\(\\beta_0\\) and \\(\\beta_1\\), which pertain to the slope and intercept of the regression line pertaining to the entire population of all lakes in Florida. We’ll use \\(b_0\\) and \\(b_1\\) to estimate \\(\\beta_0\\) and \\(\\beta_1\\) in the same way that we used sample proportion \\(\\hat{p}\\) to estimate population proportion \\(p\\) and sample mean \\(\\bar{x}\\) to estimate population mean \\(\\mu\\). The intercept, \\(\\beta_0\\) has little meaning here, but the slope \\(\\beta_1\\) represents the average change in mercury level for each one-unit increase in pH, among all Florida lakes. We’ll use bootstrapping to find a confidence interval for this quantity. Bootstrap Steps Take a sample of 53 lakes by randomly sampling from the original sample, with replacement. Fit a regression model with pH as the explanatory variable and record the value of slope \\(b_1\\). Repeat steps 1 and 2 many (say 10,000) times, keeping track of slope of the regression line for each bootstrap sample. Look at the distribution of the slopes across bootstrap samples. The variability in this bootstrap distribution can be used to approximate the variability in the sampling distribution for the slope relating mercury and pH levels. We’ll illustrate the procedure on 3 bootstrap samples. Bootstrap Sample 1 BootstrapSample1 &lt;- sample_n(FloridaLakes , 53, replace=TRUE) %&gt;% arrange(ID) %&gt;% select(ID, Lake, pH, Mercury) ## combine Northern and Southern Lakes BootstrapSample1 ## # A tibble: 53 × 4 ## ID Lake pH Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 Annie 5.1 1.33 ## 2 2 Annie 5.1 1.33 ## 3 3 Apopka 9.1 0.04 ## 4 4 Blue Cypress 6.9 0.44 ## 5 4 Blue Cypress 6.9 0.44 ## 6 6 Bryant 7.3 0.27 ## 7 6 Bryant 7.3 0.27 ## 8 7 Cherry 5.4 0.48 ## 9 8 Crescent 8.1 0.19 ## 10 10 Dias 6.4 0.81 ## # ℹ 43 more rows We fit a regression model to the bootstrap sample and calculate the regression coefficients. We’re again interested in the second coefficient, \\(b_1\\), which now represents the slope of the regression line. Mb1 &lt;- lm(data=BootstrapSample1, Mercury ~ pH) # fit linear model Mb1 ## ## Call: ## lm(formula = Mercury ~ pH, data = BootstrapSample1) ## ## Coefficients: ## (Intercept) pH ## 1.2681 -0.1098 Bootstrap Sample 2 BootstrapSample2 &lt;- sample_n(FloridaLakes , 53, replace=TRUE) %&gt;% arrange(ID) %&gt;% select(ID, Lake, pH, Mercury) BootstrapSample2 ## # A tibble: 53 × 4 ## ID Lake pH Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Alligator 6.1 1.23 ## 2 1 Alligator 6.1 1.23 ## 3 2 Annie 5.1 1.33 ## 4 2 Annie 5.1 1.33 ## 5 3 Apopka 9.1 0.04 ## 6 6 Bryant 7.3 0.27 ## 7 7 Cherry 5.4 0.48 ## 8 9 Deer Point 5.8 0.83 ## 9 12 Down 7.2 0.5 ## 10 13 Eaton 7.2 0.49 ## # ℹ 43 more rows Mb2 &lt;- lm(data=BootstrapSample2, Mercury ~ pH) # fit linear model Mb2 ## ## Call: ## lm(formula = Mercury ~ pH, data = BootstrapSample2) ## ## Coefficients: ## (Intercept) pH ## 1.4872 -0.1394 Bootstrap Sample 3 BootstrapSample3 &lt;- sample_n(FloridaLakes , 53, replace=TRUE) %&gt;% arrange(ID) %&gt;% select(ID, Lake, pH, Mercury) BootstrapSample3 ## # A tibble: 53 × 4 ## ID Lake pH Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 Annie 5.1 1.33 ## 2 2 Annie 5.1 1.33 ## 3 2 Annie 5.1 1.33 ## 4 3 Apopka 9.1 0.04 ## 5 4 Blue Cypress 6.9 0.44 ## 6 4 Blue Cypress 6.9 0.44 ## 7 6 Bryant 7.3 0.27 ## 8 8 Crescent 8.1 0.19 ## 9 8 Crescent 8.1 0.19 ## 10 10 Dias 6.4 0.81 ## # ℹ 43 more rows Mb3 &lt;- lm(data=BootstrapSample3, Mercury ~ pH) # fit linear model Mb3 ## ## Call: ## lm(formula = Mercury ~ pH, data = BootstrapSample3) ## ## Coefficients: ## (Intercept) pH ## 1.6431 -0.1761 We’ll now take 10,000 different bootstrap samples and look at the bootstrap distribution for \\(b_1\\), the slope of the regression line relating mercury level and pH. M &lt;- lm(data=FloridaLakes, Mercury~pH) #fit model to original sample Sample_b1 &lt;- M$coefficients[2] # record b1 value (second coefficient) Bootstrap_b1 &lt;- rep(NA, 10000) #vector to store b1 values for (i in 1:10000){ BootstrapSample &lt;- sample_n(FloridaLakes , 53, replace=TRUE) #take bootstrap sample M &lt;- lm(data=BootstrapSample, Mercury ~ pH) # fit linear model Bootstrap_b1[i] &lt;- M$coefficients[2] # record b1 } Lakes_Bootstrap_Slope_Results &lt;- data.frame(Bootstrap_b1) #save results as dataframe The bootstrap distribution for the slopes, \\(b_1\\), is shown below, along with the standard error for the difference. Lakes_Bootstrap_Plot_Slope &lt;- ggplot(data=Lakes_Bootstrap_Slope_Results, aes(x=Bootstrap_b1)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;Slope in Bootstrap Sample&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Bootstrap Distribution for Slope&quot;) Lakes_Bootstrap_Plot_Slope Standard Error of the slope \\(b_1\\) SE_b1 &lt;- sd(Lakes_Bootstrap_Slope_Results$Bootstrap_b1) SE_b1 ## [1] 0.02712872 The bootstrap distribution is symmetric and bell-shaped, so we can use the standard error method to calculate a 95% confidence interval. \\[ \\begin{aligned} &amp; b_1 \\pm 2\\times\\text{SE}(b_1) \\\\ &amp; = -0.1523 \\pm 2\\times{0.027} \\end{aligned} \\] 95% Confidence Interval: c(Sample_b1 - 2*SE_b1, Sample_b1 + 2*SE_b1) ## pH pH ## -0.20655831 -0.09804342 The 95% confidence interval is shown by the gold bar on the graph of the bootstrap distribution below. Lakes_Bootstrap_Plot_Slope + geom_segment(aes(x=Sample_b1 - 2*SE_b1,xend=Sample_b1 + 2*SE_b1, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We are 95% confident that among all Florida lakes, for each 1 unit increase in pH, mercury level decreases between 0.2065583 and 0.0980434, on average. 3.3.7 CI for Regression Response In addition to calculating a confidence interval for the slope of the regression line relating mercury and pH levels in a lake, we can also calculate a confidence interval for the average mercury level among all lakes with a given pH. We’ll calculate a confidence interval for the average mercury level among all lakes with a neutral pH level of 7. The regression equation is \\[ \\begin{aligned} \\widehat{\\text{Mercury}} &amp; = b_0 + b_1\\times\\text{pH} \\\\ &amp; = 1.5309 - 0.1523\\times\\text{pH} \\end{aligned} \\] so the expected mercury level among all lakes with \\(\\text{pH} = 7\\) is \\(b_0+7b_1 = 1.5309-0.1523(7)=0.4648\\) ppm. This quantity is a statistic calculated from a sample of 53 lakes, so we would not expect the average mercury level among all lakes in the population to be exactly equal to 0.4648. Again, we’ll use this sample statistic as an estimate of the population parameter, and use bootstrapping to estimate the variability associated with this statistic, in order to make a confidence interval. Bootstrap Steps Take a sample of 53 southern lakes by randomly sampling from the original sample, with replacement. Fit a regression model with location as the explanatory variable and record the values of \\(b_0\\) and \\(b_1\\). Use these to calculate \\(b_0+7b_1\\). Repeat steps 1 and 2 many (say 10,000) times, keeping track of \\(b_0\\) and \\(b_1\\), and calculating \\(b_0+7b_1\\) in each bootstrap sample. Look at the distribution of the expected response, \\(b_0 + 7b_1\\), across bootstrap samples. The variability in this bootstrap distribution can be used to approximate the variability in the sampling distribution for the expected mercury level among all lakes with pH level of 7. We’ll illustrate the procedure on 3 bootstrap samples. Bootstrap Sample 1 BootstrapSample1 &lt;- sample_n(FloridaLakes , 53, replace=TRUE) %&gt;% arrange(ID) %&gt;% select(ID, Lake, pH, Mercury) ## combine Northern and Southern Lakes BootstrapSample1 ## # A tibble: 53 × 4 ## ID Lake pH Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 Annie 5.1 1.33 ## 2 4 Blue Cypress 6.9 0.44 ## 3 6 Bryant 7.3 0.27 ## 4 7 Cherry 5.4 0.48 ## 5 9 Deer Point 5.8 0.83 ## 6 10 Dias 6.4 0.81 ## 7 10 Dias 6.4 0.81 ## 8 10 Dias 6.4 0.81 ## 9 10 Dias 6.4 0.81 ## 10 11 Dorr 5.4 0.71 ## # ℹ 43 more rows We fit a regression model to the bootstrap sample and calculate the regression coefficients. We’re interested in the second coefficient, \\(b_1\\), which represents the mean difference between lakes in Southern and Northern Florida Mb1 &lt;- lm(data=BootstrapSample1, Mercury ~ pH) ## fit linear model b0 &lt;- Mb1$coefficients[1] # record value of b0 (first coefficient) b1 &lt;- Mb1$coefficients[2] # record value of b1 (second coefficient) b0+7*b1 #calculate b0+7*b1 ## (Intercept) ## 0.4755848 Bootstrap Sample 2 BootstrapSample2 &lt;- sample_n(FloridaLakes , 53, replace=TRUE) %&gt;% arrange(ID) %&gt;% select(ID, Lake, pH, Mercury) BootstrapSample2 ## # A tibble: 53 × 4 ## ID Lake pH Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Alligator 6.1 1.23 ## 2 1 Alligator 6.1 1.23 ## 3 3 Apopka 9.1 0.04 ## 4 4 Blue Cypress 6.9 0.44 ## 5 12 Down 7.2 0.5 ## 6 14 East Tohopekaliga 5.8 1.16 ## 7 16 George 8.2 0.15 ## 8 17 Griffin 8.7 0.19 ## 9 17 Griffin 8.7 0.19 ## 10 17 Griffin 8.7 0.19 ## # ℹ 43 more rows Mb2 &lt;- lm(data=BootstrapSample2, Mercury ~ pH) ## fit linear model b0 &lt;- Mb2$coefficients[1] # record value of b0 (first coefficient) b1 &lt;- Mb2$coefficients[2] # record value of b1 (second coefficient) b0+7*b1 #calculate b0+7*b1 ## (Intercept) ## 0.4327569 Bootstrap Sample 3 BootstrapSample3 &lt;- sample_n(FloridaLakes , 53, replace=TRUE) %&gt;% arrange(ID) %&gt;% select(ID, Lake, pH, Mercury) BootstrapSample3 ## # A tibble: 53 × 4 ## ID Lake pH Mercury ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Alligator 6.1 1.23 ## 2 2 Annie 5.1 1.33 ## 3 2 Annie 5.1 1.33 ## 4 4 Blue Cypress 6.9 0.44 ## 5 6 Bryant 7.3 0.27 ## 6 7 Cherry 5.4 0.48 ## 7 8 Crescent 8.1 0.19 ## 8 9 Deer Point 5.8 0.83 ## 9 11 Dorr 5.4 0.71 ## 10 11 Dorr 5.4 0.71 ## # ℹ 43 more rows Mb3 &lt;- lm(data=BootstrapSample3, Mercury ~ pH) # fit linear model b0 &lt;- Mb3$coefficients[1] # record value of b0 (first coefficient) b1 &lt;- Mb3$coefficients[2] # record value of b1 (second coefficient) b0+7*b1 #calculate b0+7*b1 ## (Intercept) ## 0.4717755 We’ll now take 10,000 different bootstrap samples and record the values of \\(b_0\\), \\(b_1\\), which we’ll then use to calculate \\(b_0+7b_1\\). M &lt;- lm(data=FloridaLakes, Mercury~pH) #fit model to original sample Sample_b0 &lt;- M$coefficients[1] # record b0 value (second coefficient) Sample_b1 &lt;- M$coefficients[2] # record b1 value (second coefficient) Sample_Exp7 &lt;- Sample_b0 + 7*Sample_b1 # calculate sample expected mercury when pH=7 Bootstrap_b0 &lt;- rep(NA, 10000) #vector to store b1 values Bootstrap_b1 &lt;- rep(NA, 10000) #vector to store b1 values for (i in 1:10000){ BootstrapSample &lt;- sample_n(FloridaLakes , 53, replace=TRUE) #take bootstrap sample M &lt;- lm(data=BootstrapSample, Mercury ~ pH) # fit linear model Bootstrap_b0[i] &lt;- M$coefficients[1] # record b0 Bootstrap_b1[i] &lt;- M$coefficients[2] # record b1 } Bootstrap_Exp7 &lt;- Bootstrap_b0 + 7*Bootstrap_b1 # calcualte expected response for each bootstrap sample Lakes_Bootstrap_Exp7_Results &lt;- data.frame(Bootstrap_b0, Bootstrap_b1, Bootstrap_Exp7) #save results as dataframe The bootstrap distribution for the expected mercury level among all lakes with pH level 7, \\(b_0+7b_1\\), is shown below, along with the standard error for this quantity. Lakes_Bootstrap_Plot_Exp7 &lt;- ggplot(data=Lakes_Bootstrap_Exp7_Results, aes(x=Bootstrap_Exp7)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;Expected Mercury Level in Bootstrap Sample&quot;) + ylab(&quot;Frequency&quot;) + ggtitle( &quot;Bootstrap Distribution for Exp. Mercury when pH=7&quot;) Lakes_Bootstrap_Plot_Exp7 Standard Error of the expected response \\(b_0 + 7b_1\\) SE_Exp7 &lt;- sd(Lakes_Bootstrap_Exp7_Results$Bootstrap_Exp7) SE_Exp7 ## [1] 0.03704046 Again, the bootstrap distribution is symmetric and bell-shaped, so we can use the standard error method to calculate a 95% confidence interval. \\[ \\begin{aligned} &amp; b_1 \\pm 2\\times\\text{SE}(b_1) \\\\ &amp; = 0.4648 \\pm 2\\times{0.037} \\end{aligned} \\] 95% Confidence Interval: c(Sample_Exp7 - 2*SE_Exp7, Sample_Exp7 + 2*SE_Exp7) ## (Intercept) (Intercept) ## 0.3907318 0.5388936 The 95% confidence interval is shown by the gold bar on the graph of the bootstrap distribution below. Lakes_Bootstrap_Plot_Exp7 + geom_segment(aes(x=Sample_Exp7 - 2*SE_Exp7,xend=Sample_Exp7 + 2*SE_Exp7, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We are 95% confident that average mercury level among all Florida lakes with pH level 7 is between 0.3907318 and 0.5388936 ppm. 3.3.8 More CI’s in Regression 3.4 Bootstrapping Cautions "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
