[["index.html", "Stat 255: Statistics for Data Science Notes Preface", " Stat 255: Statistics for Data Science Notes Andrew Sage - Lawrence University 2022-09-11 Preface These notes serve as the primary textual resource for Stat 255: Statistics for Data Science at Lawrence University. What is this course about? Stat 255 provides an introduction to essential statistical tasks including modeling, inference, prediction, and computation. The course employs a modern approach, intended to equip students with skills needed for working with today’s complex data. Traditional concepts, like interval estimation, and hypothesis testing, are introduced through the lens of multivariate models and simulation. Data computation in R plays a central role throughout the course. The course’s overarching learning outcomes are: Visualize and wrangle data using statistical software R. Build and assess multivariate models to predict future outcomes. Quantify uncertainty associated with estimates and predictions. Explain the assumptions associated with statistical models, and evalate whether these assumptions are reasonably satisfied in context. Write reproducible analyses, using statistical software. Work with data in an ethical and responsible manner. More specific learning tasks, related to these outcomes are provided in each chapter. Who is this course intended for? This course is intended for students who are interested in learning statistical modeling and data computation skills that might prove useful in further courses, research, or career. Stat 255 can serve as either: a first course in statistics for students with a strong quantitative background, typically including calculus. a second course in statistics, building on introductory topics taught in courses like Lawrence’s Stat 107: Principles of Statistics, or AP Statistics. At Lawrence, this course is required for the Statistics Track of the Mathematics Major, the Economics and Mathematics-Economics Majors, and the Statistics and Data Science Minor. The prerequisite for the course is either 1) a prior college-level course in statistics (i.e. STAT 107, BIOL 170 or 280, ANTH 207, AP Stats) OR 2) Calculus. (Math 140, AP Calculus, or equivalent). The course does not assume any prior knowledge of statistics, but does move more rapidly than a typical introductory statistics course. Students engage rigorously in statistical thinking and computation, intended to equip them with essential skills for further study in statistics and data science. "],["exploratory-data-analysis.html", "Chapter 1 Exploratory Data Analysis 1.1 Exploring Data Visualizations 1.2 Exploratory Analysis in R", " Chapter 1 Exploratory Data Analysis Learning Outcomes: Interpret graphical summaries of data, including boxplots, histograms, violin plots, density plots, scatterplots, and correlation plots. Read data from a .csv file into R. Preview data in R. Create graphical summaries of data using R. Calculate summary statistics for entire datasets and grouped summaries. Create reproducible documents using R Markdown. 1.1 Exploring Data Visualizations 1.1.1 COVID-19 Data The following data were contained in a UK technical report on the Delta Variant on August 2, 2021 Vaccination Status Deaths Total Cases Fatality Rate Fully Vax 402 47,008 0.86% Unvax 253 151,054 0.17 % ggplot(data=Covid_Data, aes(x=Vac_Status, fill=Outcome)) + geom_bar(position=&quot;fill&quot;) One explanation would be that vaccines don’t work, and even cause harm. Can you think of another explanation? 1.1.2 Breakdown by Age &lt; 50 We break down the cases based on whether the patient was older or younger than 50. Under 50 Vaccination Status Deaths Total Cases Fatality Rate Fully Vax 13 25,536 0.05% Unvax 48 147, 612 0.03 % 50 or Older Vaccination Status Deaths Total Cases Fatality Rate Fully Vax 389 21,472 1.81% Unvax 205 3,440 5.96 % ggplot(data=Covid_Data, aes(x=Vac_Status, fill=Outcome)) + geom_bar(position=&quot;fill&quot;) + facet_wrap(~Age) Before accounting for age, vaccinated people appear to be 5 times MORE likely to die from covid than unvaccinated people. After accounting for age, vaccinated young people appear to be about 1.5 times more likely to die from covid (though death rates are very low) overall, and vaccinated older people appear to be more than 3 times LESS likely. 1.1.3 Vaccination by Age Breakdown ggplot(data=Covid_Data, aes(x=Vac_Status, fill=Age)) + geom_bar(position=&quot;fill&quot;) Since almost all of the unvaccinated people were from the lower risk category, the death rate among unvaccinated people appears lower. We should account for age when comparing survival rates. 1.1.4 Simpson’s Paradox Simpson’s Paradox refers to a situation where an apparent trend either disappears or reverses when one or more additional variables are accounted for. These additional variables are called confounding variables. In this situation, age is a counfouding variable. Older people are more likely to be vaccinated, and also more likely to die of covid (regardless of vaccination status), thus it appears that vaccinated people are more likely to die from covid, unless we account for age. When we build statistical models, it will be important to account for potential confounding variables in our data, otherwise a model will give misleading results. Final note: Subsequent data has shown that among people under 50, vaccinated people also had lower death rates (though death rates remained very low for both vaccinated and unvaccinated people). The slightly higher death rate for vaccinated people under 50, seen in these early data, is likely due to young people with other risk conditions being vaccinated first. Data Source 1.1.5 Exploring Diamond Prices The following data come from a dataset with information on over 53,940 diamonds. We’ll explore the relationship between price (in $ US) and quality of the cut of the diamond (ideal, premium, very good, good, fair.\" Consider the following plots: ggplot(data=diamonds, aes(x=price, y=cut, fill=cut)) + geom_boxplot(outlier.size=0.01, outlier.alpha = 0.1) + stat_summary(fun=mean, geom=&quot;point&quot;, shape=4, color=&quot;red&quot;, size=3) + ggtitle(&quot;Price by Quality of Cut&quot;) Diamonds come in different carat sizes, so we should consider information about that as well. We examine a histogram, displaying the number of diamonds with each cut, and carat size. ggplot(data=diamonds, aes(x=carat, fill=cut)) + geom_histogram() + ggtitle(&quot;Diamonds by Carat Size and Cut Quality&quot;) The table shows the number of diamonds of each cut, as well as the average carat size and price of each diamond. diamonds %&gt;% group_by(cut) %&gt;% summarize(N=n(), Avg_carat=mean(carat), Avg_price=mean(price) ) ## # A tibble: 5 × 4 ## cut N Avg_carat Avg_price ## &lt;ord&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Fair 1610 1.05 4359. ## 2 Good 4906 0.849 3929. ## 3 Very Good 12082 0.806 3982. ## 4 Premium 13791 0.892 4584. ## 5 Ideal 21551 0.703 3458. Finally, we use a scatterplot to visualize cut, price, and carat size. ggplot(data=diamonds, aes(x=carat, y=price, color=cut)) + geom_point() + ggtitle(&quot;Price by Carat Size and Cut&quot;) Question: How is Simpson’s paradox present in the diamonds data? What is the confounding variable? What conclusion should we draw about the relationship between the price of a diamond and the quality of the cut? 1.2 Exploratory Analysis in R This section provides examples of how to read data into R, create graphics, like those in the previous section, and calculate summary statistics. We’ll work with data on movies released in Hollywood between 2012 and 2018. 1.2.1 Loading the Data We’ll begin by loading the tidyverse package, which can be used to create professional graphics, and wrangle (or manipulate) data into forms that are informative and easy to work with. library(tidyverse) Next, we read in the data itself, from the website where it is stored. HollywoodMovies &lt;- read_csv(&quot;https://www.lock5stat.com/datasets3e/HollywoodMovies.csv&quot;) 1.2.2 Previewing the Data head() The head() function displays the first 5 rows of the dataset. head(HollywoodMovies) ## # A tibble: 6 × 15 ## Movie LeadStudio RottenTomatoes AudienceScore Genre TheatersOpenWeek ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2016: Obama&#39;s … Rocky Mou… 26 73 Docu… 1 ## 2 21 Jump Street Sony Pict… 85 82 Come… 3121 ## 3 A Late Quartet Entertain… 76 71 Drama 9 ## 4 A Royal Affair Magnolia … 90 82 Drama 7 ## 5 Abraham Lincol… Twentieth… 35 51 Horr… 3108 ## 6 Act of Valor Relativit… 27 72 Acti… 3039 ## # … with 9 more variables: OpeningWeekend &lt;dbl&gt;, BOAvgOpenWeekend &lt;dbl&gt;, ## # Budget &lt;dbl&gt;, DomesticGross &lt;dbl&gt;, WorldGross &lt;dbl&gt;, ForeignGross &lt;dbl&gt;, ## # Profitability &lt;dbl&gt;, OpenProfit &lt;dbl&gt;, Year &lt;dbl&gt; The rows of the dataset are called observations. In this case, the observations are the movies. The columns of the dataset, which contain information about the movies, are called variables. glimpse The glimpse() command shows the number of observations (rows), and the number of variables, (columns). We also see the name of each variable and its type. Variable types include Categorical variables, which take on groups or categories, rather than numeric values. In R, these might be coded as logical &lt;logi&gt;, character &lt;chr&gt;, factor &lt;fct&gt; and ordered factor &lt;ord&gt;. Quantitative variables, which take on meaningful numeric values. These include numeric &lt;num&gt;, integer &lt;int&gt;, and double &lt;dbl&gt;. glimpse(HollywoodMovies) ## Rows: 1,295 ## Columns: 15 ## $ Movie &lt;chr&gt; &quot;2016: Obama&#39;s America&quot;, &quot;21 Jump Street&quot;, &quot;A Late Qu… ## $ LeadStudio &lt;chr&gt; &quot;Rocky Mountain Pictures&quot;, &quot;Sony Pictures Releasing&quot;,… ## $ RottenTomatoes &lt;dbl&gt; 26, 85, 76, 90, 35, 27, 91, 56, 11, 44, 93, 63, 87, 9… ## $ AudienceScore &lt;dbl&gt; 73, 82, 71, 82, 51, 72, 62, 47, 47, 63, 82, 51, 63, 9… ## $ Genre &lt;chr&gt; &quot;Documentary&quot;, &quot;Comedy&quot;, &quot;Drama&quot;, &quot;Drama&quot;, &quot;Horror&quot;, … ## $ TheatersOpenWeek &lt;dbl&gt; 1, 3121, 9, 7, 3108, 3039, 132, 245, 2539, 3192, 3, 1… ## $ OpeningWeekend &lt;dbl&gt; 0.03, 36.30, 0.08, 0.04, 16.31, 24.48, 1.14, 0.70, 11… ## $ BOAvgOpenWeekend &lt;dbl&gt; 30000, 11631, 8889, 5714, 5248, 8055, 8636, 2857, 449… ## $ Budget &lt;dbl&gt; 3.0, 42.0, NA, NA, 68.0, 12.0, NA, 7.5, 35.0, 50.0, 1… ## $ DomesticGross &lt;dbl&gt; 33.35, 138.45, 1.56, 1.55, 37.52, 70.01, 1.99, 3.01, … ## $ WorldGross &lt;dbl&gt; 33.35, 202.81, 6.30, 7.60, 137.49, 82.50, 3.59, 8.54,… ## $ ForeignGross &lt;dbl&gt; 0.00, 64.36, 4.74, 6.05, 99.97, 12.49, 1.60, 5.53, 9.… ## $ Profitability &lt;dbl&gt; 1334.00, 482.88, NA, NA, 202.19, 687.50, NA, 113.87, … ## $ OpenProfit &lt;dbl&gt; 1.20, 86.43, NA, NA, 23.99, 204.00, NA, 9.33, 32.57, … ## $ Year &lt;dbl&gt; 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012,… There are 1,295 movies in the dataset, and 15 variables for each film. summary summary displays the mean, minimum, first quartile, median, third quartile, and maximum for each numeric variable. summary(HollywoodMovies) ## Movie LeadStudio RottenTomatoes AudienceScore ## Length:1295 Length:1295 Min. : 0.00 Min. :10.00 ## Class :character Class :character 1st Qu.:33.00 1st Qu.:49.00 ## Mode :character Mode :character Median :61.00 Median :64.00 ## Mean :57.58 Mean :62.18 ## 3rd Qu.:84.00 3rd Qu.:77.00 ## Max. :99.00 Max. :99.00 ## NA&#39;s :6 ## Genre TheatersOpenWeek OpeningWeekend BOAvgOpenWeekend ## Length:1295 Min. : 1.0 Min. : 0.020 Min. : 204 ## Class :character 1st Qu.: 152.5 1st Qu.: 0.845 1st Qu.: 3482 ## Mode :character Median :2459.0 Median : 7.600 Median : 6586 ## Mean :2008.0 Mean : 17.541 Mean : 13400 ## 3rd Qu.:3213.5 3rd Qu.: 20.810 3rd Qu.: 14534 ## Max. :4529.0 Max. :257.700 Max. :240000 ## ## Budget DomesticGross WorldGross ForeignGross ## Min. : 0.90 Min. : 1.02 Min. : 0.74 Min. : -0.76 ## 1st Qu.: 12.00 1st Qu.: 6.40 1st Qu.: 13.09 1st Qu.: 3.91 ## Median : 30.00 Median : 26.46 Median : 50.37 Median : 21.58 ## Mean : 51.38 Mean : 58.16 Mean : 147.01 Mean : 88.84 ## 3rd Qu.: 65.00 3rd Qu.: 66.44 3rd Qu.: 160.38 3rd Qu.: 89.75 ## Max. :365.00 Max. :936.66 Max. :2068.22 Max. :1369.54 ## NA&#39;s :239 ## Profitability OpenProfit Year ## Min. : 2.3 Min. : 0.05 Min. :2012 ## 1st Qu.: 139.1 1st Qu.: 12.87 1st Qu.:2013 ## Median : 268.9 Median : 31.77 Median :2015 ## Mean : 435.7 Mean : 64.50 Mean :2015 ## 3rd Qu.: 483.0 3rd Qu.: 62.59 3rd Qu.:2017 ## Max. :10176.0 Max. :3373.00 Max. :2018 ## NA&#39;s :239 NA&#39;s :239 Notice that 239 films have missing information on some of the variables, recorded as NA’s. 1.2.3 Adding a New Variable We can use the mutate() function to create a new variable based on variables already in the dataset. In the data description, the variable Profitability is defined as WorldGross as a percentage of Budget. Thus, films for which Profitability exceeds 100 were profitable. We create a variable to tell whether or not a film was profitable. Note that in R, a variable defined as a condition, such as Profitability&gt;100 will return values of either TRUE or FALSE. HollywoodMovies &lt;- HollywoodMovies %&gt;% mutate(Profitable = Profitability &gt; 100) summary(HollywoodMovies$Profitable) ## Mode FALSE TRUE NA&#39;s ## logical 170 886 239 1.2.4 Selecting Columns If the dataset contains a large number of variables, narrow down to the ones you are interested in working with. This can be done with the select() command. If there are not very many variables to begin with, or you are interested in all of them, then you may skip this step. Let’s narrow the dataset down to the variables Movie, RottenTomatoes, AudienceScore, Genre, WorldGross, Budget, “Profitable”, and Year. MoviesSubset &lt;- HollywoodMovies %&gt;% select(Movie, RottenTomatoes, AudienceScore, Genre, WorldGross, Budget, Profitable, Year) 1.2.5 Filtering by Row The filter() command narrows a dataset down to rows that meet specified conditions. Filtering by a Categorical Variable Let’s filter the data to only include action movies, comedies, dramas, and horror movies. We’ll also keep only those films whose budget was listed, excluding the 239 NA’s. The command !is.na() returns only values that are not NA’s. MoviesSubset1 &lt;- MoviesSubset %&gt;% filter(Genre %in% c(&quot;Action&quot;, &quot;Comedy&quot;, &quot;Drama&quot;, &quot;Horror&quot;)) %&gt;% filter(!is.na(Budget)) In R, the ! operator means “not”. glimpse(MoviesSubset1) ## Rows: 679 ## Columns: 8 ## $ Movie &lt;chr&gt; &quot;21 Jump Street&quot;, &quot;Abraham Lincoln: Vampire Hunter&quot;, &quot;A… ## $ RottenTomatoes &lt;dbl&gt; 85, 35, 27, 56, 44, 93, 63, 86, 34, 86, 74, 41, 71, 32,… ## $ AudienceScore &lt;dbl&gt; 82, 51, 72, 47, 63, 82, 51, 86, 55, 76, 64, 35, 62, 70,… ## $ Genre &lt;chr&gt; &quot;Comedy&quot;, &quot;Horror&quot;, &quot;Action&quot;, &quot;Drama&quot;, &quot;Comedy&quot;, &quot;Drama… ## $ WorldGross &lt;dbl&gt; 202.81, 137.49, 82.50, 8.54, 236.80, 36.79, 71.00, 36.7… ## $ Budget &lt;dbl&gt; 42.0, 68.0, 12.0, 7.5, 50.0, 10.0, 49.0, 4.6, 220.0, 1.… ## $ Profitable &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T… ## $ Year &lt;dbl&gt; 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2… Filtering by a Quantitative Variable Let’s filter the data to only include films whose world gross exceeds 100 million dollars. MoviesSubset2 &lt;- MoviesSubset %&gt;% filter(WorldGross &gt;100) Now, let’s preview the data again. glimpse(MoviesSubset2) ## Rows: 444 ## Columns: 8 ## $ Movie &lt;chr&gt; &quot;21 Jump Street&quot;, &quot;Abraham Lincoln: Vampire Hunter&quot;, &quot;A… ## $ RottenTomatoes &lt;dbl&gt; 85, 35, 44, 96, 34, 78, 85, 66, 38, 88, 78, 17, 74, 45,… ## $ AudienceScore &lt;dbl&gt; 82, 51, 63, 90, 55, 76, 71, 67, 46, 92, 75, 32, 56, 72,… ## $ Genre &lt;chr&gt; &quot;Comedy&quot;, &quot;Horror&quot;, &quot;Comedy&quot;, &quot;Thriller&quot;, &quot;Action&quot;, &quot;Ad… ## $ WorldGross &lt;dbl&gt; 202.81, 137.49, 236.80, 227.14, 313.48, 554.61, 123.68,… ## $ Budget &lt;dbl&gt; 42.0, 68.0, 50.0, 45.0, 220.0, 185.0, 12.0, 102.0, 150.… ## $ Profitable &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T… ## $ Year &lt;dbl&gt; 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2… We’ll use MoviesSubset1 from this point forward. 1.2.6 Histogram Next, we’ll create graphics to help us visualize the distributions and relationships between variables. We’ll use the ggplot() function, which is part of the tidyverse package. Histograms are useful for displaying the distribution of a single quantitative variable General Template for Histogram ggplot(data=DatasetName, aes(x=VariableName)) + geom_histogram(fill=&quot;colorchoice&quot;, color=&quot;colorchoice&quot;) + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;x-axis label&quot;) + ylab(&quot;y-axis label&quot;) Histogram of Audience Scores ggplot(data=MoviesSubset1, aes(x=AudienceScore)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + ggtitle(&quot;Distribution of Audience Scores&quot;) + xlab(&quot;Audience Score&quot;) + ylab(&quot;Frequency&quot;) 1.2.7 Density Plots Density plots show the distribution for a quantitative variable like audience score. Scores can be compared across categories, like genre. General Template for Density Plot ggplot(data=DatasetName, aes(x=QuantitativeVariable, color=CategoricalVariable, fill=CategoricalVariable)) + geom_density(alpha=0.2) + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;Axis Label&quot;) + ylab(&quot;Frequency&quot;) alpha, ranging from 0 to 1 dictates transparency. Density Plot of Audience Scores ggplot(data=MoviesSubset1, aes(x=AudienceScore, color=Genre, fill=Genre)) + geom_density(alpha=0.2) + ggtitle(&quot;Distribution of Audience Scores&quot;) + xlab(&quot;Audience Score&quot;) + ylab(&quot;Frequency&quot;) 1.2.8 Boxplot Boxplots can be used to compare a quantitative variable with a categorical variable General Template for Boxplot ggplot(data=DatasetName, aes(x=CategoricalVariable, y=QuantitativeVariable)) + geom_boxplot() + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;Variable Name&quot;) + ylab(&quot;Variable Name&quot;) You can make the plot horizontal by adding + coordflip(). You can turn the axis text vertical by adding theme(axis.text.x = element_text(angle = 90)). Boxplot Comparing Scores for Genres ggplot(data=MoviesSubset1, aes(x=Genre, y=AudienceScore)) + geom_boxplot() + ggtitle(&quot;Audience Score by Genre&quot;) + xlab(&quot;Genre&quot;) + ylab(&quot;Audience Score&quot;) + theme(axis.text.x = element_text(angle = 90)) 1.2.9 Violin Plot Violin plots are an alternative to boxplots. The width of the violin tells us the density of observations in a given range. General Template for Violin Plot ggplot(data=DatasetName, aes(x=CategoricalVariable, y=QuantitativeVariable, fill=CategoricalVariable)) + geom_violin() + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;Variable Name&quot;) + ylab(&quot;Variable Name&quot;) Violin Plot Comparing Scores for Genres ggplot(data=MoviesSubset1, aes(x=Genre, y=AudienceScore, fill=Genre)) + geom_violin() + ggtitle(&quot;Audience Score by Genre&quot;) + xlab(&quot;Genre&quot;) + ylab(&quot;Audience Score&quot;) + theme(axis.text.x = element_text(angle = 90)) We can view the boxplot and violin plot together. 1.2.10 Scatterplots Scatterplots are used to visualize the relationship between two quantitative variables. Scatterplot Template ggplot(data=DatasetName, aes(x=CategoricalVariable, y=QuantitativeVariable)) + geom_point() + ggtitle(&quot;Plot Title&quot;) + ylab(&quot;Axis Label&quot;) + xlab(&quot;Axis Label&quot;) Scatterplot Comparing Audience Score and Rotten Tomatoes Score ggplot(data=MoviesSubset1, aes(x=RottenTomatoes, y=AudienceScore)) + geom_point() + ggtitle(&quot;Audience and Critics Ratings&quot;) + ylab(&quot;Audience Rating&quot;) + xlab(&quot;Critics&#39; Rating&quot;) We see that there is an upward trend, indicating a positive association between critics scores (RottenTomatoes), and audience scores. However, there is a lot of variability, and the relationship is moderately strong at best. We can also add color, size, and shape to the scatterplot to display information about other variables. ggplot(data=MoviesSubset1, aes(x=RottenTomatoes, y=AudienceScore, color=Genre, size=WorldGross)) + geom_point() + ggtitle(&quot;Audience and Critics Ratings&quot;) + ylab(&quot;Audience Rating&quot;) + xlab(&quot;Critics&#39; Rating&quot;) We can add labels for points meeting certain conditions, using geom_text(). This should be done carefully, to avoid overlap. ggplot(data=MoviesSubset1, aes(x=RottenTomatoes, y=AudienceScore, color=Genre, size=WorldGross)) + geom_point() + ggtitle(&quot;Audience and Critics Ratings&quot;) + ylab(&quot;Audience Rating&quot;) + xlab(&quot;Critics&#39; Rating&quot;) + geom_text(data = MoviesSubset1 %&gt;% filter(WorldGross &gt;800), aes(label = Movie), color=&quot;black&quot;, check_overlap = TRUE) 1.2.11 Bar Graphs Bar graphs can be used to visualize one or more categorical variables Bar Graph Template ggplot(data=DatasetName, aes(x=CategoricalVariable)) + geom_bar(fill=&quot;colorchoice&quot;,color=&quot;colorchoice&quot;) + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;Variable Name&quot;) + ylab(&quot;Frequency&quot;) Bar Graph by Genre ggplot(data=MoviesSubset1, aes(x=Genre)) + geom_bar(fill=&quot;lightblue&quot;,color=&quot;white&quot;) + ggtitle(&quot;Number of Films by Genre&quot;) + xlab(&quot;Genre&quot;) + ylab(&quot;Number of Films&quot;) + theme(axis.text.x = element_text(angle = 90)) 1.2.12 Stacked and Side-by-Side Bar Graphs Stacked Bar Graph Template ggplot(data = DatasetName, mapping = aes(x = CategoricalVariable1, fill = CategoricalVariable2)) + stat_count(position=&quot;fill&quot;) + theme_bw() + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;Variable 1&quot;) + ylab(&quot;Proportion of Variable 2&quot;) + theme(axis.text.x = element_text(angle = 90)) Stacked Bar Graph Example The stat_count(position=\"fill\") command creates a stacked bar graph, comparing two categorical variables. Let’s explore whether certain genres are more profitable than others, using the profitability variable. ggplot(data = MoviesSubset1, mapping = aes(x = Genre, fill = Profitable)) + stat_count(position=&quot;fill&quot;) + theme_bw() + ggtitle(&quot;Profitability by Genre&quot;) + xlab(&quot;Genre&quot;) + ylab(&quot;Proportion Profitable&quot;) + theme(axis.text.x = element_text(angle = 90)) Side-by-side Bar Graph Template We can create a side-by-side bar graph, using position=dodge. ggplot(data = DatasetName, mapping = aes(x = CategoricalVariable1, fill = CategoricalVariable2)) + geom_bar(position = &quot;dodge&quot;) + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;Genre&quot;) + ylab(&quot;Frequency&quot;) Side-by-side Bar Graph Example ggplot(data = MoviesSubset1, mapping = aes(x = Genre, fill = Profitable)) + geom_bar(position = &quot;dodge&quot;) + ggtitle(&quot;Number of Films by Genre&quot;) + xlab(&quot;Genre&quot;) + ylab(&quot;Number of Films&quot;) + theme(axis.text.x = element_text(angle = 90)) 1.2.13 Correlation Plot Correlation plots can be used to visualize relationships between quantitative variables. These can be helpful when we proceed to modeling. Explanatory variables that are highly correlated with the response are often strong predictors that should be included in a model. However, including two explanatory variables that are highly correlated with one another can create interpretation problems. The cor() function calculates correlations between quantitative variables. We’ll use select_if to select only numeric variables. The `use=“complete.obs” command tells R to ignore observations with missing data. cor(select_if(HollywoodMovies, is.numeric), use=&quot;complete.obs&quot;) %&gt;% round(2) ## RottenTomatoes AudienceScore TheatersOpenWeek OpeningWeekend ## RottenTomatoes 1.00 0.71 -0.27 0.14 ## AudienceScore 0.71 1.00 -0.21 0.20 ## TheatersOpenWeek -0.27 -0.21 1.00 0.59 ## OpeningWeekend 0.14 0.20 0.59 1.00 ## BOAvgOpenWeekend 0.40 0.35 -0.37 0.13 ## Budget 0.06 0.13 0.59 0.72 ## DomesticGross 0.24 0.31 0.51 0.93 ## WorldGross 0.20 0.28 0.52 0.90 ## ForeignGross 0.17 0.25 0.49 0.84 ## Profitability 0.08 0.08 0.04 0.15 ## OpenProfit -0.10 -0.11 0.13 0.12 ## Year 0.04 -0.05 0.08 0.04 ## BOAvgOpenWeekend Budget DomesticGross WorldGross ForeignGross ## RottenTomatoes 0.40 0.06 0.24 0.20 0.17 ## AudienceScore 0.35 0.13 0.31 0.28 0.25 ## TheatersOpenWeek -0.37 0.59 0.51 0.52 0.49 ## OpeningWeekend 0.13 0.72 0.93 0.90 0.84 ## BOAvgOpenWeekend 1.00 0.08 0.27 0.25 0.22 ## Budget 0.08 1.00 0.70 0.78 0.79 ## DomesticGross 0.27 0.70 1.00 0.94 0.86 ## WorldGross 0.25 0.78 0.94 1.00 0.98 ## ForeignGross 0.22 0.79 0.86 0.98 1.00 ## Profitability 0.14 -0.12 0.17 0.15 0.13 ## OpenProfit -0.07 -0.14 0.07 0.02 -0.01 ## Year -0.04 0.01 0.03 0.04 0.04 ## Profitability OpenProfit Year ## RottenTomatoes 0.08 -0.10 0.04 ## AudienceScore 0.08 -0.11 -0.05 ## TheatersOpenWeek 0.04 0.13 0.08 ## OpeningWeekend 0.15 0.12 0.04 ## BOAvgOpenWeekend 0.14 -0.07 -0.04 ## Budget -0.12 -0.14 0.01 ## DomesticGross 0.17 0.07 0.03 ## WorldGross 0.15 0.02 0.04 ## ForeignGross 0.13 -0.01 0.04 ## Profitability 1.00 0.84 -0.01 ## OpenProfit 0.84 1.00 -0.03 ## Year -0.01 -0.03 1.00 The corrplot() function in the corrplot() package provides a visualization of the correlations. Larger, thicker circles indicate stronger correlations. library(corrplot) Corr &lt;- cor(select_if(HollywoodMovies, is.numeric), use=&quot;complete.obs&quot;) corrplot(Corr) We can also display a numeric version of the correlations by setting method=\"number\". Corr &lt;- cor(select_if(HollywoodMovies, is.numeric), use=&quot;complete.obs&quot;) corrplot(Corr, method=&quot;number&quot;) 1.2.14 Scatterplot Matrix A scatterplot matrix is a grid of plots. It can be created using the ggpairs() function in the GGally package. The scatterplot matrix shows us: Along the diagonal are density plots for quantitative variables, or bar graphs for categorical variables, showing the distribution of each variable. Under the diagonal are plots showing the relationships between the variables in the corresponding row and column. Scatterplots are used when both variables are quantitative, bar graphs are used when both variables are categorical, and boxplots are used when one variable is categorical, and the other is quantitative. Above the diagonal are correlations between quantitative variables. We need to remove the column with the movie names. This is done using select. library(GGally) ggpairs(MoviesSubset1 %&gt;% select(-Movie)) The scatterplot matrix is useful for helping us notice key trends in our data. However, the plot can hard to read as it is quite dense, especially when there are a large number of variables. These can help us look for trends from a distance, but we should then focus in on more specific plots. 1.2.15 Summary Tables group_by() and summarize() The group_by() and summarize() commands are useful for breaking categorical variables down by category. For example, let’s calculate number of films in each genre, and the mean, median, and standard deviation in film WorldGross by genre. MoviesSubset1 %&gt;% group_by(Genre) %&gt;% summarize(N = n(), Mean_Gross = mean(WorldGross, na.rm=TRUE), Median_Gross = median(WorldGross, na.rm=TRUE), StDev_Gross = sd(WorldGross, na.rm = TRUE)) %&gt;% arrange(desc(Mean_Gross)) ## # A tibble: 4 × 5 ## Genre N Mean_Gross Median_Gross StDev_Gross ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Action 162 371. 222. 393. ## 2 Horror 78 103. 79.6 113. ## 3 Comedy 160 85.1 65.0 77.0 ## 4 Drama 279 76.4 34.7 119. Notes: 1. The n() command calculates the number of observations in a category. 2. The na.rm=TRUE command removes missing values, so that summary statistics can be calculated. 3. arrange(desc(Mean_Gross)) arranges the table in descending order of Mean_Gross. To arrange in ascending order, use arrange(Mean_Gross). The kable() function in the knitr() package creates tables with professional appearance. library(knitr) MoviesTable &lt;- MoviesSubset1 %&gt;% group_by(Genre) %&gt;% summarize(N = n(), Mean_Gross = mean(WorldGross, na.rm=TRUE), Median_Gross = median(WorldGross, na.rm=TRUE), StDev_Gross = sd(WorldGross, na.rm = TRUE)) %&gt;% arrange(desc(Mean_Gross)) kable(MoviesTable) Genre N Mean_Gross Median_Gross StDev_Gross Action 162 370.85648 221.59 393.01213 Horror 78 102.55423 79.63 113.14402 Comedy 160 85.14469 64.99 77.00407 Drama 279 76.42573 34.72 119.08083 "],["introduction-to-statistical-models.html", "Chapter 2 Introduction to Statistical Models 2.1 Predicting House Prices 2.2 Variability Explained by a Model 2.3 Multiple Regression Model 2.4 Least-Squares Estimation 2.5 ANalysis Of VAriance 2.6 Models Involving Interaction 2.7 More on Interaction", " Chapter 2 Introduction to Statistical Models Learning Outcomes: Calculate sums of squares related to variability explained, including SST, SSR, and SSM., when given small datasets and/or summary statistics. Explain the meaning of SST, SSR, and SSM in a given context. Calculate \\(R^2\\) and ANOVA F-Statistics, when given small datasets and/or summary statistics. Intrepret \\(R^2\\) and F-statistics in context. Explain the process for estimating least-squares regression coefficients. Calculate predictions from linear regression models. Interpret regression coefficients for models involving quantitative and/or categorical variables in context, or explain why it is inappropriate to do so. Explain the meaning of interaction between quantitative and categorical explanatory variables. Apply graphical methods, statistical summaries, and background knowledge to argue for whether or not interaction term(s) should be used in a statistical model. Determine slopes, intercepts, and other regression coefficients for specific categories or values of an explanatory variable in models that involve interaction. 2.1 Predicting House Prices 2.1.1 House Prices in Ames IA Shown below are the prices of 10 houses sold in Ames, IA between 2006 and 2010. Houses$SalePrice ## [1] 187.00 163.99 235.00 113.00 110.00 84.90 123.00 176.50 150.00 137.00 ggplot(data=Houses, aes(x=SalePrice)) + geom_histogram(binwidth=60, fill=&quot;blue&quot;, color=&quot;white&quot;) + xlab(&quot;Price in thousands&quot;) + ylab(&quot;Frequency&quot;)+ theme_bw() + ggtitle(&quot;House Prices in Ames, IA&quot;) summary(Houses$SalePrice) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 84.9 115.5 143.5 148.0 173.4 235.0 Question: Suppose we know that a particular house sold in Ames during this time period, but know nothing else about the house. Based on this estimation, how might we predict the price of the house? How confident are you in this prediction? 2.1.2 Prices by Neighborhood Now, suppose that the houses are in one of three neighborhoods (College Creek, Edwards, North Ames). Prices of the houses in each neighborhood are shown below. p_House_Nbhd &lt;- ggplot(data=Houses, aes(x=Neighborhood, y=SalePrice)) +geom_point() + theme_bw() + ggtitle(&quot;Houses by Neighborhood&quot;) p_House_Nbhd kable(Houses %&gt;% select(Neighborhood, SalePrice)) Neighborhood SalePrice CollgCr 187.00 CollgCr 163.99 CollgCr 235.00 Edwards 113.00 Edwards 110.00 Edwards 84.90 NAmes 123.00 NAmes 176.50 NAmes 150.00 NAmes 137.00 Question: Suppose we know that a house sold in the College Creek neighborhood. How might we predict the price of the house? How confident are you in this prediction? 2.1.3 House Prices by Size The scatterplot shows the relationship between the size of a house in square feet, and its saleprice. ggplot(data=Houses, aes(x=SquareFeet, y=SalePrice)) + geom_point() Question: Suppose we know that another house, not in the original data, had 1200 square feet. How might we predict the saleprice of the house? How confident are you in this prediction? 2.1.4 Explanatory and Response Variables Now, suppose we want to use information about the house’s neighborhood in our predictions. We’ll predict a price of a house to be the average price among houses in the neightborhood. The variable we are trying to predict (price) is called the response variable (denoted \\(Y\\)). The variable(s) we use to help us make the prediction (neighborhood) is(are) called explanatory variables (denoted \\(X\\)). These are also referred to as predictor variables or covariates. 2.1.5 Prediction with No Explanatory Variables If we have the prices of \\(n\\) houses, \\(y_i, y_2, \\ldots, y_n\\), and want to predict the price of a new house, without information about any explanatory variables the ``statistically optimal\" prediction is the overall average price of all houses in the dataset. \\[ \\widehat{\\text{Price}} = \\bar{y}, \\text{where } \\bar{y}=\\frac{\\displaystyle\\sum_{i=1}^ny_i}{n}\\]. The symbol \\(\\widehat{\\text{Price}}\\), represents the predicted, or expected, price. ggplot(data=Houses, aes(x=1, y=SalePrice)) + geom_point() + ylab(&quot;Price in thousands&quot;) + xlab(&quot;&quot;)+ theme(axis.text.x = element_blank()) + stat_summary(fun = mean, geom = &quot;errorbar&quot;, aes(ymax = ..y.., ymin = ..y..), color=&quot;red&quot;) + annotate(&quot;text&quot;, y=145, x=1.25, label=&quot;sample mean&quot;, color=&quot;red&quot;) + theme_bw() + ggtitle(&quot;All 10 Houses&quot;) mean(Houses$SalePrice) ## [1] 148.039 Without information about any explanatory variables, we would predict the price of a house sold in Ames, IA during this time to be about 148 thousand dollars. 2.1.6 Simple Model in R M0 &lt;- lm(data=Houses, SalePrice~1) summary(M0) ## ## Call: ## lm(formula = SalePrice ~ 1, data = Houses) ## ## Residuals: ## Min 1Q Median 3Q Max ## -63.139 -32.539 -4.539 25.334 86.961 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 148.04 13.97 10.6 0.0000022 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 44.17 on 9 degrees of freedom Without information about any explanatory variables, we would predict the price of a house sold in Ames, IA during this time to be about 148 thousand dollars. 2.1.7 Prediction for a Categorical Explanatory Variable When we have a categorical explanatory variable (e.g. neighborhood), the “statistically optimal” prediction is the average response among all observations in the given category. p_House_Nbhd NbhdTbl &lt;- Houses %&gt;% group_by(Neighborhood) %&gt;% summarize(AveragePrice=mean(SalePrice)) kable(NbhdTbl) Neighborhood AveragePrice CollgCr 195.3300 Edwards 102.6333 NAmes 146.6250 We predict the price of a house in College Creek to be 195.33 thousand dollars, compared with 102.63 thousand dollars in Edwards, and 146.62 thousand dollars in Edwards. 2.1.8 Model by Neighborhood in R M_Nbhd &lt;- lm(data=Houses, SalePrice ~ Neighborhood) summary(M_Nbhd) ## ## Call: ## lm(formula = SalePrice ~ Neighborhood, data = Houses) ## ## Residuals: ## Min 1Q Median 3Q Max ## -31.340 -15.706 -2.477 9.617 39.670 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 195.33 14.89 13.118 0.00000349 *** ## NeighborhoodEdwards -92.70 21.06 -4.402 0.00315 ** ## NeighborhoodNAmes -48.70 19.70 -2.473 0.04267 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 25.79 on 7 degrees of freedom ## Multiple R-squared: 0.7348, Adjusted R-squared: 0.6591 ## F-statistic: 9.699 on 2 and 7 DF, p-value: 0.009603 For categorical explanatory variables, R treats the category that comes first alphabetically (in this case CCreek), as a baseline. The intercept gives the prediction for this category. We would expect a house in College Creek to cost 195.33 thousand dollars. Each of the other rows in the coefficients table represent the difference between the expected response (price) for that category (neighborhood), compared to the baseline. We would expect a house in Edwards to cost 92.70 thousand less than a house in College Creek. (hence costing 102.63 thousand) We would expect a house in North Ames to cost 48.71 thousand less than a house in College Creek. (hence costing 146.62 thousand) 2.1.9 Model Notation for Houses by Neighborhood The model can be expressed in the form: \\(\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{I}_{\\text{Edwards}} +b_2 \\times\\text{I}_{\\text{NAmes}}\\) \\(\\widehat{\\text{Price}}= 195.33+ -92.7 \\times\\text{I}_{\\text{Edwards}} +-48.7 \\times\\text{I}_{\\text{NAmes}}\\), where represents an indicator variables, taking on values 0 or 1. - Example: \\[ \\text{I}_{\\text{Edwards}} =\\begin{cases} 1 &amp; \\text{if house is in Edwards Neighborhood} \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] Predicted Prices: College Creek: \\(\\widehat{\\text{Price}}= 195.33+ -92.7 \\times0 +-48.7 \\times0 = 195.33\\) thousand. Edwards: \\(\\widehat{\\text{Price}}= 195.33+ -92.7 \\times1 +-48.7 \\times0 = 102.63\\) thousand. North Ames: \\(\\widehat{\\text{Price}}= 195.33+ -92.7 \\times0 +-48.7 \\times1 = 146.62\\) thousand. 2.1.10 Prediction for Quantitative Explanatory Variable For a quantitative explanatory variable like square feet, the “statistically optimal” prediction is found by fitting a “line of best fit” to the data (more details to come.) ggplot(data=Houses, aes(x=SquareFeet, y=SalePrice)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) For a house with 1200 square feet, predicted price is about $150 thousand. 2.1.11 Model using Square Feet M_SqFt &lt;- lm(data=Houses, SalePrice~SquareFeet) summary(M_SqFt) ## ## Call: ## lm(formula = SalePrice ~ SquareFeet, data = Houses) ## ## Residuals: ## Min 1Q Median 3Q Max ## -39.235 -14.309 2.052 10.966 43.971 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.82000 36.35455 0.188 0.85586 ## SquareFeet 0.12079 0.03022 3.997 0.00397 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 27.06 on 8 degrees of freedom ## Multiple R-squared: 0.6663, Adjusted R-squared: 0.6246 ## F-statistic: 15.97 on 1 and 8 DF, p-value: 0.003967 2.1.12 Model for SquareFeet and Interpretations In the model using both square feet and neighborhood, the regression equation is \\(\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{SquareFeet}\\) \\(\\widehat{\\text{Price}}= 6.82+ 0.121 \\times\\text{SquareFeet}\\) \\(\\widehat{\\text{Price}}\\) represents the expected, or predicted, price. The slope, \\(b_1\\) represents the expected change in price (in thousands) per one-unit increase in square feet. The price of a house is expected to increase by 121 dollars for each additional square foot. The intercept, \\(b_0\\) represents the expected price of a house with 0 square feet. - In this situation, this is not a meaningful interpretation. 2.1.13 Calculating Predicted Prices \\(\\widehat{\\text{Price}}= 6.82+ 0.121 \\times\\text{SquareFeet}\\) Predicted price for a house with 1200 square feet: \\(\\widehat{\\text{Price}}= 6.82+ 0.121 \\times 1200 = 151.8\\) thousand dollars. 2.2 Variability Explained by a Model 2.2.1 Quantifying Variability We’ve seen the “statistically optimal” way to make predictions for data with categorical and quantitative explanatory variables, but we shouldn’t expect these predictions to be exact. We see that prices of individual houses vary from one another, even if they are in the same neighborhood or have the same size. We can get a sense of how much variability we should expect in our prediction by looking at how much the values in our dataset differ from the predicted (mean) price. The difference between the true and predicted values (\\(y_i - \\hat{y}_i\\)) is called the \\(ith\\) residual. 2.2.2 Residuals for Three Models Model with No Explanatory Variables M0Resid &lt;-ggplot(data=Houses, aes(x = 1:10, y = SalePrice)) +geom_point() + geom_segment(aes(xend = 1:10, yend = M0$fitted.values), color=&quot;red&quot;) + geom_abline(slope=0, intercept=mean(M0$fitted.values)) + xlab(&quot;&quot;) + theme_bw() + theme(axis.text.x = element_blank()) M0Resid Model with Neighborhood as Explanatory Variable Model with Square Feet as Explanatory Variable ggplot(data=Houses, aes(x = SquareFeet, y = SalePrice)) + geom_segment(aes(xend = SquareFeet, yend = M_SqFt$fitted.values), color=&quot;red&quot;) + geom_point() + geom_point(aes(y = M_SqFt$fitted.values), shape = 1) + stat_smooth(method=&quot;lm&quot;, se=FALSE)+ theme_bw() 2.2.3 Quantifying Unexplained Variablility The residuals tell us how much variability in the response variable (sale price) is left unexplained by a model. For a model with given explanatory variable(s), we can calculate the proportion of variability in the response variable explained by the model by comparing the size of the residuals to those of a model with no explanatory variables. * \\(\\displaystyle\\sum_{i=1}^n (y_i - \\hat{y})=0\\), so this is not a helpful measure, but we can instead use: \\[ \\displaystyle\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\] 2.2.4 Total Sum of Squares (SST) We call the sum of squared residuals for the model with no explanatory variables the total sum of squares, abbreviated SST. In this model, \\(\\hat{y}_i = \\bar{y}\\). \\[SST = \\displaystyle\\sum_{i=1}^n (y_i - \\bar{y})^2 \\] SalePrice &lt;- Houses$SalePrice Predicted &lt;- M0$fitted.values SquareFeet &lt;- Houses$SquareFeet Residual &lt;- M0$residuals ResidSq &lt;- Residual^2 Residdf &lt;- data.frame(SquareFeet, SalePrice, Predicted, Residual, ResidSq) Residdf ## SquareFeet SalePrice Predicted Residual ResidSq ## 1 1499 187.00 148.039 38.961 1517.959521 ## 2 1456 163.99 148.039 15.951 254.434401 ## 3 1525 235.00 148.039 86.961 7562.215521 ## 4 672 113.00 148.039 -35.039 1227.731521 ## 5 1179 110.00 148.039 -38.039 1446.965521 ## 6 930 84.90 148.039 -63.139 3986.533321 ## 7 864 123.00 148.039 -25.039 626.951521 ## 8 1414 176.50 148.039 28.461 810.028521 ## 9 1144 150.00 148.039 1.961 3.845521 ## 10 1008 137.00 148.039 -11.039 121.859521 sum(Houses$SalePrice - mean(Houses$SalePrice)) ## [1] 0.0000000000001421085 sum((Houses$SalePrice - mean(Houses$SalePrice))^2) ## [1] 17558.52 knitr::include_graphics(&quot;SST.png&quot;) SST represents the total amount of variability in sale price (without accounting for any explanatory variables). 2.2.5 SSR and SSM When we fit a model with an explanatory variable (such as neighborhood), we can see how much the sum of squared residuals decreases, based on information introduced by that explanatory variable. SalePrice &lt;- Houses$SalePrice Predicted &lt;- M_Nbhd$fitted.values Residual &lt;- M_Nbhd$residuals ResidSq &lt;- Residual^2 Neighborhood &lt;- Houses$Neighborhood Residdf &lt;- data.frame(Neighborhood, SalePrice, Predicted, Residual, ResidSq) Residdf ## Neighborhood SalePrice Predicted Residual ResidSq ## 1 CollgCr 187.00 195.3300 -8.330000 69.38890 ## 2 CollgCr 163.99 195.3300 -31.340000 982.19560 ## 3 CollgCr 235.00 195.3300 39.670000 1573.70890 ## 4 Edwards 113.00 102.6333 10.366667 107.46778 ## 5 Edwards 110.00 102.6333 7.366667 54.26778 ## 6 Edwards 84.90 102.6333 -17.733333 314.47111 ## 7 NAmes 123.00 146.6250 -23.625000 558.14062 ## 8 NAmes 176.50 146.6250 29.875000 892.51562 ## 9 NAmes 150.00 146.6250 3.375000 11.39063 ## 10 NAmes 137.00 146.6250 -9.625000 92.64062 sum(M_Nbhd$residuals^2) ## [1] 4656.188 the variability in sale price remaining unexplained even after accounting for neighborhood is given by the sum of squared residuals. We abbreviate this SSR, for sum of squared residuals. \\[ \\text{SSR} = \\text{Variability Remaining}=\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2 \\] the difference, \\(\\text{SST} - \\text{SSR} = 17558.52 - 4656.188 = 12902.33\\) tells us how much variability in sale price is explained by the model using neighborhood as an explanatory variable. We call this quantity the sum of squares explained by the model, abbreviated SSM. The proportion of variability in sale price explained by the model using neighborhoods as an explanatory variable is \\[\\frac{SSM}{SST}=\\frac{12902.33}{17558.52} = 0.7348 \\] 73.5% of the variation in house price is explained by the model using neighborhood as an explanatory variable. The proportion of variability in the response variable explained by a model with given explanatory variables is called the coefficient of determination, and is given the symbol \\(R^2\\). Our value matches the value of “Multiple R-squared” in the 2nd last line of the R model summary. summary(M_Nbhd) ## ## Call: ## lm(formula = SalePrice ~ Neighborhood, data = Houses) ## ## Residuals: ## Min 1Q Median 3Q Max ## -31.340 -15.706 -2.477 9.617 39.670 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 195.33 14.89 13.118 0.00000349 *** ## NeighborhoodEdwards -92.70 21.06 -4.402 0.00315 ** ## NeighborhoodNAmes -48.70 19.70 -2.473 0.04267 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 25.79 on 7 degrees of freedom ## Multiple R-squared: 0.7348, Adjusted R-squared: 0.6591 ## F-statistic: 9.699 on 2 and 7 DF, p-value: 0.009603 2.2.6 Summary: SST, SSR, SSM, \\(R^2\\) the total variability in house prices is the sum of the squared differences between price and average price. \\[\\text{Total Variability in Price}= \\text{SST} =\\displaystyle\\sum_{i=1}^n(y_i-\\bar{y})^2\\] the variability remaining unexplained even after accounting for neighborhood is given by the sum of squared residuals. We abbreviate this SSR, for sum of squared residuals. \\[ \\text{SSR} = \\text{Variability Remaining}=\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2 \\] the variability explained by the model, abbreviated SSM, is given by \\[ \\text{SSM} = \\text{SST} - \\text{SSR} \\] It can be shown that \\(\\text{SSM}=\\displaystyle\\sum_{i=1}^n(\\hat{y}_i-\\bar{y})^2\\). These abbreviations here vary across texts. Be careful! The coefficient of determination (abbreviated \\(R^2\\)) is defined as \\[R^2=\\frac{\\text{Variability Explained by Model}}{\\text{Total Variability}}=\\frac{\\text{SSM}}{\\text{SST}} =\\frac{\\displaystyle\\sum_{i=1}^n(\\hat{y}_i-\\bar{y})^2}{\\displaystyle\\sum_{i=1}^n(y_i-\\bar{y})^2}\\] 2.2.7 \\(R^2\\) Visually Blue Area = Total Variability (SST) Red Area = Variability Remaining Unexplained by Model (SSR) Blue Area - Red Area = Variability Explained by Model (SSM) \\(R^2 = \\frac{\\text{Area of Blue Squares} - \\text{Area of Red Squares}}{\\text{Area of Blue Squares}} = \\frac{\\text{SST}-\\text{SSR}}{\\text{SST}}= \\frac{\\text{SSM}}{\\text{SST}}\\) 2.2.8 Residuals for Square Feet Model ggplot(data=Houses, aes(x = SquareFeet, y = SalePrice)) + geom_segment(aes(xend = SquareFeet, yend = M_SqFt$fitted.values), color=&quot;red&quot;) + geom_point() + geom_point(aes(y = M_SqFt$fitted.values), shape = 1) + stat_smooth(method=&quot;lm&quot;, se=FALSE)+ theme_bw() SalePrice &lt;- Houses$SalePrice Predicted &lt;- M_SqFt$fitted.values Residual &lt;- M_SqFt$residuals ResidSq &lt;- Residual^2 SquareFeet &lt;- Houses$SquareFeet Residdf &lt;- data.frame(SquareFeet, SalePrice, Predicted, Residual, ResidSq) Residdf ## SquareFeet SalePrice Predicted Residual ResidSq ## 1 1499 187.00 187.88858 -0.8885824 0.7895786 ## 2 1456 163.99 182.69449 -18.7044870 349.8578356 ## 3 1525 235.00 191.02920 43.9708019 1933.4314183 ## 4 672 113.00 87.99284 25.0071576 625.3579304 ## 5 1179 110.00 149.23485 -39.2348498 1539.3734427 ## 6 930 84.90 119.15741 -34.2574142 1173.5704309 ## 7 864 123.00 111.18508 11.8149181 139.5922893 ## 8 1414 176.50 177.62118 -1.1211847 1.2570550 ## 9 1144 150.00 145.00710 4.9929021 24.9290718 ## 10 1008 137.00 128.57926 8.4207385 70.9088361 sum(M_SqFt$residuals^2) ## [1] 5859.068 2.2.9 Variation Explained by SquareFeet Model Created at http://www.rossmanchance.com/applets/RegShuffle.htm. Blue Area = Total Variability (SST) Red Area = Variability Remaining Unexplained by Model (SSR) Blue Area - Red Area = Variability Explained by Model (SSM) \\(R^2 = \\frac{\\text{Area of Blue Squares} - \\text{Area of Red Squares}}{\\text{Area of Blue Squares}} = \\frac{\\text{SST}-\\text{SSR}}{\\text{SST}}= \\frac{\\text{SSM}}{\\text{SST}}\\) 2.2.10 Variation Explained by Square Feet Model Total variability in house prices SST = 17,558.52 Variability remaining unexplained after accounting for square feet is SSR = 5,859.07 Variation explained by model accounting for square feet is \\[ \\text{SSM} = 17,558.52 - 5,859.07 = 11,699.45 \\] Proportion of variation explained by model accounting for square feet is \\[ R^2=\\frac{11,699.45}{17,558.52}\\approx0.6663\\] 66.6% of the variation in house price is explained by the model using square feet as an explanatory variable. summary(M_SqFt) ## ## Call: ## lm(formula = SalePrice ~ SquareFeet, data = Houses) ## ## Residuals: ## Min 1Q Median 3Q Max ## -39.235 -14.309 2.052 10.966 43.971 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.82000 36.35455 0.188 0.85586 ## SquareFeet 0.12079 0.03022 3.997 0.00397 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 27.06 on 8 degrees of freedom ## Multiple R-squared: 0.6663, Adjusted R-squared: 0.6246 ## F-statistic: 15.97 on 1 and 8 DF, p-value: 0.003967 2.2.11 Linear Correlation Coefficient ggplot(data=Houses, aes(x=SquareFeet, y=SalePrice)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) For linear models with a single quantitative variable, the linear correlation coefficient \\(r=\\sqrt{R^2}\\), or \\(r=-\\sqrt{R^2}\\) (with sign matching the sign on the slope of the line), provides information about the strength and direction of the linear relationship between the variables. \\(-1 \\leq r \\leq 1\\), and \\(r\\) close to \\(\\pm1\\) provides evidence of strong linear relationship, while \\(r\\) close to 0 suggests linear relationship is weak. \\(r\\) is only relevant for models with a single quantitative explanatory variable and a quantitative response variable, while \\(R^2\\) is relevant for any linear model with a quantitative response variable. cor(Houses$SalePrice,Houses$SquareFeet) ## [1] 0.8162794 2.3 Multiple Regression Model 2.3.1 Multiple Regression Model Suppose we have information on both the neighborhood and square feet in the houses. We can account for both of these together using a multiple regression model, i.e. a model with more than one explanatory variable. kable(Houses) Neighborhood SquareFeet SalePrice CollgCr 1499 187.00 CollgCr 1456 163.99 CollgCr 1525 235.00 Edwards 672 113.00 Edwards 1179 110.00 Edwards 930 84.90 NAmes 864 123.00 NAmes 1414 176.50 NAmes 1144 150.00 NAmes 1008 137.00 How can we predict the price of a: 848 square foot house in College Creek? 1200 square foot house in North Ames? 2314 square foot house in Edwards? 2.3.2 2-Variable Model Different Slopes We could try to lines to the houses in each neighborhood, independent of the other neighborhoods. ggplot(data=Houses, aes(x=SquareFeet, y=SalePrice, color=Neighborhood)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) Since there are only 3-4 houses per neighbhorhood, though, this isn’t a lot of information to go on. 2.3.3 2-Variable Model with Constant Slope Instead, we’ll assume the rate of increase wrt. square feet (i.e. slope) is the same in each neighborhood, but that some neighborhoods are more expensive than others. This allows us to use all 10 houses to estimate slope, while allowing intercepts to differ between neighborhoods. 2.3.4 House Price 2-Variable Model Summary M_Nbhd_SqFt &lt;- lm(data=Houses, SalePrice~SquareFeet+Neighborhood) summary(M_Nbhd_SqFt) ## ## Call: ## lm(formula = SalePrice ~ SquareFeet + Neighborhood, data = Houses) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.125 -9.050 -5.653 9.069 37.791 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 106.72593 68.92188 1.549 0.172 ## SquareFeet 0.05933 0.04517 1.314 0.237 ## NeighborhoodEdwards -59.09436 32.49761 -1.818 0.119 ## NeighborhoodNAmes -25.81232 25.59807 -1.008 0.352 ## ## Residual standard error: 24.55 on 6 degrees of freedom ## Multiple R-squared: 0.7941, Adjusted R-squared: 0.6911 ## F-statistic: 7.711 on 3 and 6 DF, p-value: 0.01757 2.3.5 MR Model for SquareFeet and Neighborhood In the model using both square feet and neighborhood, the regression equation is \\(\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{SquareFeet}+ b_2\\times\\text{I}_{Edwards} + b_3 \\times\\text{I}_{NAmes}\\) \\(\\widehat{\\text{Price}}= 106.73+ 0.06 \\times\\text{SquareFeet}+ -59.09 \\times\\text{I}_{Edwards} +-25.81 \\times\\text{I}_{NAmes}\\) The intercept \\(b_0\\) represents the expected price of a house in College Creek with 0 square feet. the intercept has no meaningful interpretation in this context \\(b_1\\) represents the expected change in price (in thousands) per one-unit increase in square feet, assuming neighborhood is the same. on average, we expect the price of a house to increase by $0.05933 thousand (i.e. $59.33) for each additional square foot, assuming the houses are in the same neighborhood. \\(b_2\\) and \\(b_3\\) represent the expected difference in price between a house in the Edwards (or North Ames) neighborhood, compared to the College Creek neighborhood, assuming square footage is the same. We expect a house in the Edwards neighborhood to cost $59.094 less than a house in the College Creek Neighborhood, assuming the houses are the same size. We expect a house in the North Ames Neighborhood to cost $25.812 less than a house in the College Creek Neighborhood, assuming the houses are the same size. 2.3.6 Predicting Price in MR Model \\(\\widehat{\\text{Price}}= 106.73+ 0.06 \\times\\text{SquareFeet}+ -59.09 \\times\\text{I}_{Edwards} +-25.81 \\times\\text{I}_{NAmes}\\) 848 square foot house in College Creek \\(\\widehat{\\text{Price}}= 106.73+ 0.06 \\times848+ -59.09 \\times0 +-25.81 \\times 0 =157.0378\\) thousand 1200 square foot house in North Ames \\(\\widehat{\\text{Price}}= 106.73+ 0.06 \\times1200+ -59.09 \\times0 +-25.81 \\times1 = 152.1096\\) thousand 2314 square foot house in Edwards \\(\\widehat{\\text{Price}}= 106.73+ 0.06 \\times\\text{SquareFeet}+ -59.09 \\times1 +-25.81 \\times 0 =184.9212\\) thousand 2.3.7 Risk of Extrapolation Note that 2314 square feet is well outside the range of our observed data. We should treat this prediction with caution, since we don’t know whether the trend we see in our data will continue. 2.3.8 Residuals for 2-Variable Model 2.3.9 Residuals for 2-Variable Model (cont.) SalePrice &lt;- Houses$SalePrice Predicted &lt;- M_Nbhd_SqFt$fitted.values Residual &lt;- M_Nbhd_SqFt$residuals ResidSq &lt;- M_Nbhd_SqFt$residuals^2 Residdf &lt;- data.frame(SalePrice, Predicted, Residual, ResidSq) Residdf ## SalePrice Predicted Residual ResidSq ## 1 187.00 195.6662 -8.666221 75.103383 ## 2 163.99 193.1149 -29.124898 848.259699 ## 3 235.00 197.2089 37.791119 1428.168680 ## 4 113.00 87.5034 25.496603 650.076744 ## 5 110.00 117.5853 -7.585270 57.536321 ## 6 84.90 102.8113 -17.911333 320.815835 ## 7 123.00 132.1774 -9.177395 84.224570 ## 8 176.50 164.8106 11.689410 136.642314 ## 9 150.00 148.7907 1.209343 1.462509 ## 10 137.00 140.7214 -3.721358 13.848508 sum(M_Nbhd_SqFt$residuals^2) ## [1] 3616.139 2.3.10 Variation Explained by 2-Variable Model Total Variation in house prices: SST=17,558.52 Variation remaining unexplained after accounting for square feet is SSR=3,616.139 Variation explained by model accounting for square feet is \\[SSM=SST-SSR=17,558.52 - 3,616.139 = 13,942.38\\] Proportion of variation in house prices explained by model is: \\[ R^2 = \\frac{13,942.38}{17,558.52}\\approx0.794 \\] 79.4% of the variation in house price is explained by the model using square feet and neighborhood as an explanatory variables. 2.3.11 Model Comparison Summary Model Variables Unexplained Variability Variability Explained \\(R^2\\) 0 None 17558.52489 0 0 1 Nbhd 4656.1875667 12902.3373233 0.734819 2 Sq. Ft. 5859.0678887 11699.4570013 0.6663121 3 Nbhd, Sq. Ft. 3616.1385638 13942.3863262 0.7940523 Comments on \\(R^2\\): \\(R^2\\) will never decrease when a new variable is added to a model. This does not mean that adding more variables to a model always improves its ability to make predictions on new data. \\(R^2\\) measures how well a model fits the data on which it was built. It is possible for a model with high \\(R^2\\) to “overfit” the data it was built from, and thus perform poorly on new data. We will discuss this idea extensively later in the course. On some datasets, there is a lot of “natural” variability in the response variable, and no model will achieve a high \\(R^2\\). That’s okay. Even a model with \\(R^2 = 0.10\\) or less can provide useful information. The task of a statistician is not to achieve a model that makes perfect predictions, but rather to be able to quantify the amount of uncertainty associated with the predictions we make. 2.4 Least-Squares Estimation 2.4.1 Line of Best Fit ggplot(data=Houses, aes(x=SquareFeet, y=SalePrice)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) The line \\(\\text{Price} = 6.82 + 0.12 \\times \\text{Square Feet}\\) is considered the “line of best fit” in the sense that it minimizes the sum of the squared residuals. This Rossman-Chance applet provides an illustration of the line of best fit. 2.4.2 Least-Squares Estimation in Simple Linear Regression Consider a simple linear regression(SLR) model, which is one with a singe quantitative explanatory variable. \\(\\hat{y}_i = b_0+b_1x_i\\) we need to choose the values of \\(b_0\\) and \\(b_1\\) that minimize: \\[ \\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2 =\\displaystyle\\sum_{i=1}^n(y_i-(b_0+b_1x_i))^2 \\] 2.4.3 Least-Squares Estimation in Simple Linear Regression (cont.) Using calculus, it can be shown that this quantity is minimized when \\(b_1=\\frac{\\displaystyle\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\displaystyle\\sum_{i=1}^{n}(x_i-\\bar{x})^2}=\\frac{\\displaystyle\\sum_{i=1}^{n} x_i y_i-\\frac{\\displaystyle\\sum_{i=1}^{n} x_i \\displaystyle\\sum_{i=1}^{n} y_i }{n}}{\\left(\\displaystyle\\sum_{i=1}^{n} x_i^2 -\\frac{\\left(\\displaystyle\\sum_{i=1}^{n} x_i\\right)^2}{n}\\right)}\\) \\(b_0=\\bar{y}-b_1\\bar{x}\\) (where \\(\\bar{y}=\\frac{\\displaystyle\\sum_{i=1}^{n}{y_i}}{n}\\), and \\(\\bar{x}=\\frac{\\displaystyle\\sum_{i=1}^{n}{x_i}}{n}\\)). 2.4.4 LS Estimation for One Categorical Variable Consider a model with a single categorical variable (such as neighborhood), with G+1 categories, numbered \\(g=0,2, \\ldots, G\\) Then \\(\\hat{y}_i = b_0 + b_1x_{i1} + \\ldots +b_{G}x_{iG}\\). we need to minimize \\[ \\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2 =\\displaystyle\\sum_{i=1}^n(y_i-(b_0 + b_1x_{i1} + \\ldots +b_{G}x_{iG}))^2. \\] It can be shown that this is achieved when \\(b_0 = \\bar{y_0}\\) (i.e. the average response in the “baseline group”), and \\(b_j = \\bar{y_j} - \\bar{y}_0\\) 2.4.5 LS Estimation More Generally For multiple regression models, the logic is the same. We need to choose \\(b_0, b_1, \\ldots, b_p\\) in order to minimize \\[ \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2 = \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 \\] The mathematics, however are more complicated and require inverting a matrix. This goes beyond the scope of this class, so we will let R do the estimation and use the results. More on least squares estimation in multiple regression can be found here. 2.5 ANalysis Of VAriance 2.5.1 Submodels Model Variables Unexplained Variability Variability Explained \\(R^2\\) 0 None 17558.52489 0 0 1 Nbhd. 4656.1875667 12902.3373233 0.734819 2 Sq. Ft 5859.0678887 11699.4570013 0.6663121 3 Nbhd, Sq. Ft. 3616.1385638 13942.3863262 0.7940523 Notice that Model 1 is a submodel of Model 3, since all variables used in Model 1 are also used in Model 3. Model 2 is also a submodel of Model 3. Model 0 is a submodel of Models 1, 2, and 3. Models 1 and 2 are not submodels of each other, since Model 1 contains a variable used in Model 2 and Model 2 contains a variable not used in Model 1. 2.5.2 Comparing Submodels When one model is a submodel of another, we can compare the amount of variability explained by the models, using a technique known as ANalysis Of VAriance (ANOVA). Reduced Model: \\(\\hat{y}_i = b_0 + b_1x_{i1} + b_2{x_i2} + \\ldots + b_qx_{iq}\\) Full Model: \\(\\hat{y}_i = b_0 + b_1x_{i1} + b_2{x_i2} + \\ldots + b_qx_{iq} + b_{q+1}x_{i{q+1}} \\ldots + b_px_{ip}\\) p = # variables in Full Model q = # variables in Reduced Model n = number of observations We calculate a statistic called F: \\[ \\begin{aligned} F &amp;= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} \\end{aligned} \\] Questions: 1. If the full model does a much better job explaining variability in the response variable than the reduced model, will the F-statistic be large or small? Can an F-statistic ever be negative? Why or why not? 2.5.3 Comments on F-Statistic The F-statistic measures the amount of variability explained by adding additional variable(s) to the model, relative to the total amount of unexplained variability. Large values of F indicate that adding the additional explanatory variables is helpful in explaining variability in the response variable Small values of F indicate that adding new explanatory variables variables does not make much of a difference in explaining variability in the response variable What counts as “large” is depends on \\(n, p,\\) and \\(q\\). We will revisit this later in the course. 2.5.4 ANOVA F-Statistic Let’s Calculate an ANOVA F-Statistic to compare Models 2 and 3. Reduced Model: \\(\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{SquareFeet}\\) Full Model: \\(\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{SquareFeet}+ b_2\\times\\text{I}_{Edwards} + b_3 \\times\\text{I}_{NAmes}\\) \\[ \\begin{aligned} F &amp;= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} \\\\ &amp;=\\frac{\\frac{5,859.07-3,616.14}{3-1}}{\\frac{3,616.13}{10-(3+1)}} \\\\ \\end{aligned} \\] SSR2 &lt;- sum(M_SqFt$residuals^2); SSR3 &lt;- sum(M_Nbhd_SqFt$residuals^2); ((SSR2-SSR3)/(3-1))/((SSR3)/(10-(3+1))) ## [1] 1.860766 2.5.5 ANOVA F-Statistic for M2 vs M3 in R anova(M_SqFt, M_Nbhd_SqFt) ## Analysis of Variance Table ## ## Model 1: SalePrice ~ SquareFeet ## Model 2: SalePrice ~ SquareFeet + Neighborhood ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 8 5859.1 ## 2 6 3616.1 2 2242.9 1.8608 0.2351 Notice the F-statistic has the same value. Later, we will examine what this tells us about adding Neighborhood to a model already containing square feet as an explanatory variable. 2.5.6 ANOVA F-Statistic for M1 vs M0 Now, let’s compare Models 0 and 1. Reduced Model: \\(\\widehat{\\text{Price}}_i = b_0\\) Full Model: \\(\\widehat{\\text{Price}}_i = b_0 + b_1\\text{I}_{\\text{Edwards}} + b_2\\text{I}_{\\text{NAmes}}\\) \\[ \\begin{aligned} F &amp;= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} \\\\ &amp;=\\frac{\\frac{17558.52-4656.19}{2-0}}{\\frac{4656.19}{10-(2+1)}} \\end{aligned} \\] SSR0 &lt;- sum(M0$residuals^2); SSR1 &lt;- sum(M_Nbhd$residuals^2); ((SSR0-SSR1)/(2-0))/((SSR1)/(10-(2+1))) ## [1] 9.698531 2.5.7 ANOVA F-Statistic for M0 vs M1 in R anova(M0, M_Nbhd) ## Analysis of Variance Table ## ## Model 1: SalePrice ~ 1 ## Model 2: SalePrice ~ Neighborhood ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 9 17558.5 ## 2 7 4656.2 2 12902 9.6985 0.009603 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 2.5.8 ANOVA F-Statistic for Categorical Variables The difference between M1 and M0 is that M1 considers the house’s neighborhood, while M0 does not. If neighborhood is helpful in modeling house price, then we would expect to see a high F-statistic. Another way to think about this is that if the amount of variability in house prices between different neighborhoods is large, relative to the amount of variability within neighborhoods, then the F-statistic should be large. In fact, an alternative (an mathematically equivalent) way to calculate the F-statistic is to calculate the ratio of variability between different neighborhoods, relative to the amount of variability within neighborhoods. 2.5.9 F-Statistic for Categorical Variables Illustration An F-statistic compares the amount of variability between groups to the amount of variability within groups. Scenario 1 Scenario 2 variation between groups High Low variation within groups Low High F Statistic Large Small Result Evidence of Group Differences No evidence of differences Question: Suppose, in these scenarios, we perform an F-test comparing a model the includes group as an explanatory variable, to one the includes no explanatory variables? Which scenario (1 or 2) would you expect to result in a larger F-statistic? 2.5.10 Alternative F-Statistic Formula For a categorical variable with \\(g\\) groups, let \\(\\bar{y}_{1\\cdot}, \\ldots, \\bar{y}_{g\\cdot}\\) represent the mean response for each group. let \\(n_1, \\ldots, n_g\\) represent the sample size for each group Then \\(\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}n_i(y_{i\\cdot}-\\bar{y}_{\\cdot\\cdot})^2}{g-1}\\) gives a measure of how much the group means differ, and \\(\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i\\cdot})^2}{n-g}\\) gives a measure of how much individual observations differ within groups An alternative formula for this F-statistic is: \\[ F= \\frac{\\text{Variability between Neighborhoods}}{\\text{Variability within Neighborhoods}}= \\frac{\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}n_i(y_{i\\cdot}-\\bar{y}_{\\cdot\\cdot})^2}{g-1}}{\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i\\cdot})^2}{n-g}} \\] It can be shown that this statistic is equivalent to the one we saw previously. 2.5.11 Calculating F-Statistic for Categorical Variables We have seen previously that: \\(\\bar{y}_{\\cdot\\cdot}=148.039\\) (overall average price), and \\(n=10\\) \\(\\bar{y}_{1\\cdot}=195.330\\) (average price in College Creek), and \\(n_1=3\\) \\(\\bar{y}_{2\\cdot}=102.633\\) (average price in Edwards), and \\(n_2=4\\) \\(\\bar{y}_{3\\cdot}=146.625\\) (average price in North Ames), and \\(n_3=3\\) Then, \\(\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{i\\cdot}-\\bar{y}_{\\cdot\\cdot})^2}{g-1} = \\frac{3(195.330-148.039)^2+3(102.633-148.039)^2+4(146.625-148.039)^2}{3-1} = \\frac{12902}{2}\\), and \\(\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i\\cdot})^2}{n-g} = \\frac{(123.00-146.625)^2+ (187.00 - 195.33)^2 + \\ldots + (137.00-146.625)^2}{10-3} = \\frac{4656}{7}\\) 2.5.12 Calculating F-Statistic for Categorical Variables \\[ F= \\frac{\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}n_i(y_{i\\cdot}-\\bar{y}_{\\cdot\\cdot})^2}{g-1}}{\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i\\cdot})^2}{n-g}} = \\frac{\\frac{(195.330-148.039)^2+(102.633-148.039)^2+(146.625-148.039)^2}{3-1}}{\\frac{(123.00-146.625)^2+ (187.00 - 195.33)^2 + \\ldots + (137.00-146.625)^2}{10-3}} = \\frac{\\frac{12902}{2}}{\\frac{4656}{7}} \\] Note that the quantity in the the quantity in the third line is equivalent to the sum of the squared residuals using M2. Thus, we can calculate F using: ((3*(195.330-148.039)^2+3*(102.633-148.039)^2+4*(146.625-148.039)^2)/(3-1))/(sum(M_Nbhd$residuals^2)/(10-3)) ## [1] 9.6986 2.5.13 Alternative Calculation in R This interpretation of the F-statistic can be seen using the AOV command in R. AOV_Nbhd &lt;- aov(data=Houses, SalePrice~Neighborhood) summary(AOV_Nbhd) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Neighborhood 2 12902 6451 9.699 0.0096 ** ## Residuals 7 4656 665 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The Neighborhood line represents the variability between neighborhoods The Residuals line represents the variability within neighborhoods The first two columns give the quantities we use in our formula. The third column, representing the ratio of the first two columns is called a mean square. 2.5.14 F-Statistic in R Output The last line in the summary output includes the F-statistic for the specified model, compared to a reduced model that includes only the intercept. Reduced Model: \\(\\widehat{Y}= b_0\\) Full Model: \\(\\widehat{Y}= b_0+ b_1 X_{i1}+ \\ldots+ b_p X_{ip}\\) This statistic addresses the question “Do any of the explanatory variables help explain variability in Y?”. When there is only one explanatory variable in the model, this statistic can be used to test whether there is evidence that this statistic is associated with \\(Y\\). 2.5.15 F-Statistic in R Output M1 The F-statistic compares a full model that includes neighborhood to a reduced model that predicts each price using the overall average. summary(M_Nbhd) ## ## Call: ## lm(formula = SalePrice ~ Neighborhood, data = Houses) ## ## Residuals: ## Min 1Q Median 3Q Max ## -31.340 -15.706 -2.477 9.617 39.670 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 195.33 14.89 13.118 0.00000349 *** ## NeighborhoodEdwards -92.70 21.06 -4.402 0.00315 ** ## NeighborhoodNAmes -48.70 19.70 -2.473 0.04267 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 25.79 on 7 degrees of freedom ## Multiple R-squared: 0.7348, Adjusted R-squared: 0.6591 ## F-statistic: 9.699 on 2 and 7 DF, p-value: 0.009603 2.5.16 F-Statistic in R Output M2 The F-statistic compares a full model that includes square feet to a reduced model that predicts each price using the overall average. summary(M_SqFt) ## ## Call: ## lm(formula = SalePrice ~ SquareFeet, data = Houses) ## ## Residuals: ## Min 1Q Median 3Q Max ## -39.235 -14.309 2.052 10.966 43.971 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.82000 36.35455 0.188 0.85586 ## SquareFeet 0.12079 0.03022 3.997 0.00397 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 27.06 on 8 degrees of freedom ## Multiple R-squared: 0.6663, Adjusted R-squared: 0.6246 ## F-statistic: 15.97 on 1 and 8 DF, p-value: 0.003967 2.5.17 F-Statistic in R Output M3 The F-statistic compares a full model that includes square feet and neighborhood to a reduced model that predicts each price using only the overall average. summary(M_Nbhd_SqFt) ## ## Call: ## lm(formula = SalePrice ~ SquareFeet + Neighborhood, data = Houses) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.125 -9.050 -5.653 9.069 37.791 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 106.72593 68.92188 1.549 0.172 ## SquareFeet 0.05933 0.04517 1.314 0.237 ## NeighborhoodEdwards -59.09436 32.49761 -1.818 0.119 ## NeighborhoodNAmes -25.81232 25.59807 -1.008 0.352 ## ## Residual standard error: 24.55 on 6 degrees of freedom ## Multiple R-squared: 0.7941, Adjusted R-squared: 0.6911 ## F-statistic: 7.711 on 3 and 6 DF, p-value: 0.01757 2.5.18 When to Use F-Statistics for Model Comparison We have used F-statistics to compare models 1 and 3, and models 0 and 2. We could also calculate F-statistics comparing models 2 and 3, models 0 and 1, and models 0 and 3. We cannot use an F-statistic to compare models 1 and 2, since neither is a submodel of the other. When comparing a model to the “intercept-only” model, we can use the model summary output. When comparing other to other submodels, use the aov() or anova() commands. 2.6 Models Involving Interaction 2.6.1 Bear Weights Dataset The Bolstad R package provides data on body measurements for a sample of 143 wild bears, who were anesthetized, measured and weighed, then released. Variables include: ID.- Indentification number Age - Bear’s age, in months. Note, wild bears are always born in January, so an expert can estimate the bear’s age without directly asking it how old it is. Month- Month when the measurement was made. 1 = Jan., 12 = Dec. Since bears hibernate in the winter, their body shape probably depends on the season. Sex - 1 = male 2 = female Head.L - Length of the head, in inches Head.W. - Width of the head, in inches Neck.G. - Girth (distance around) the neck, in inches Length. - Body length, in inches Chest.G. - Girth (distance around) the chest, in inches Weight - Weight of the bear, in pounds Obs.No - Observation number for this bear. For example, the bear with ID = 41 (Bertha) was measured on four occasions, in the months coded 7, 8, 11, and 5. The value of Obs.No goes from 1 to 4 for these observations. Name - The names of the bears given to them by the researchers Question of Interest: How quickly do bears gain weight as they grow? Do male and female bears gain weight at the same rate? 2.6.2 Exploring Bears Data library(Bolstad) data(bears) glimpse(bears) ## Rows: 143 ## Columns: 12 ## $ ID &lt;int&gt; 39, 41, 41, 41, 41, 43, 43, 45, 45, 48, 69, 83, 83, 83, 83, 91… ## $ Age &lt;int&gt; 19, 19, 20, 23, 29, 19, 20, 55, 67, 81, NA, 115, 117, 124, 140… ## $ Month &lt;int&gt; 7, 7, 8, 11, 5, 7, 8, 7, 7, 9, 10, 7, 9, 4, 8, 8, 4, 9, 7, 4, … ## $ Sex &lt;int&gt; 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1,… ## $ Head.L &lt;dbl&gt; 10.0, 11.0, 12.0, 12.5, 12.0, 11.0, 12.0, 16.5, 16.5, 15.5, 16… ## $ Head.W &lt;dbl&gt; 5.0, 6.5, 6.0, 5.0, 6.0, 5.5, 5.5, 9.0, 9.0, 8.0, 8.0, 10.0, 7… ## $ Neck.G &lt;dbl&gt; 15.0, 20.0, 17.0, 20.5, 18.0, 16.0, 17.0, 28.0, 27.0, 31.0, 32… ## $ Length &lt;dbl&gt; 45.0, 47.5, 57.0, 59.5, 62.0, 53.0, 56.0, 67.5, 78.0, 72.0, 77… ## $ Chest.G &lt;dbl&gt; 23.0, 24.0, 27.0, 38.0, 31.0, 26.0, 30.5, 45.0, 49.0, 54.0, 52… ## $ Weight &lt;int&gt; 65, 70, 74, 142, 121, 80, 108, 344, 371, 416, 432, 348, 476, 4… ## $ Obs.No &lt;int&gt; 1, 1, 2, 3, 4, 1, 2, 1, 2, 1, 1, 1, 2, 3, 4, 1, 1, 2, 1, 1, 2,… ## $ Name &lt;fct&gt; Allen, Berta, Berta, Berta, Berta, Clyde, Clyde, Doc, Doc, Qui… head(bears) ## ID Age Month Sex Head.L Head.W Neck.G Length Chest.G Weight Obs.No Name ## 1 39 19 7 1 10.0 5.0 15.0 45.0 23 65 1 Allen ## 2 41 19 7 2 11.0 6.5 20.0 47.5 24 70 1 Berta ## 3 41 20 8 2 12.0 6.0 17.0 57.0 27 74 2 Berta ## 4 41 23 11 2 12.5 5.0 20.5 59.5 38 142 3 Berta ## 5 41 29 5 2 12.0 6.0 18.0 62.0 31 121 4 Berta ## 6 43 19 7 1 11.0 5.5 16.0 53.0 26 80 1 Clyde 2.6.3 Bears Data Cleaning Notice that we have multiple observations on the same bears. The procedures we have learned so far require observations to be independent of each other. Thus, we’ll keep only the first observation on each bear. Bears_Subset &lt;- bears %&gt;% filter(Obs.No == 1) The variables Month and Sex are coded as integers, but it really makes more sense to think of these as categorical variables. Thus, we will convert them to factors. Bears_Subset$Month &lt;- as.factor(Bears_Subset$Month) Bears_Subset$Sex &lt;- as.factor(Bears_Subset$Sex) summary(Bears_Subset) ## ID Age Month Sex Head.L ## Min. : 39.0 Min. : 8.00 8 :23 1:62 Min. : 9.00 ## 1st Qu.:525.0 1st Qu.: 17.00 9 :20 2:35 1st Qu.:12.00 ## Median :579.0 Median : 34.00 10 :14 Median :13.00 ## Mean :537.6 Mean : 42.64 7 :11 Mean :13.29 ## 3rd Qu.:640.0 3rd Qu.: 57.25 11 : 9 3rd Qu.:14.50 ## Max. :911.0 Max. :177.00 4 : 8 Max. :18.50 ## NA&#39;s :41 (Other):12 ## Head.W Neck.G Length Chest.G ## Min. : 4.000 Min. :10.00 Min. :36.00 Min. :19.00 ## 1st Qu.: 5.000 1st Qu.:17.50 1st Qu.:54.50 1st Qu.:30.00 ## Median : 6.000 Median :20.00 Median :61.00 Median :34.00 ## Mean : 6.364 Mean :21.03 Mean :60.41 Mean :35.93 ## 3rd Qu.: 7.000 3rd Qu.:24.00 3rd Qu.:67.00 3rd Qu.:42.00 ## Max. :10.000 Max. :32.00 Max. :83.00 Max. :55.00 ## ## Weight Obs.No Name ## Min. : 26.0 Min. :1 Ian : 2 ## 1st Qu.:114.0 1st Qu.:1 Abe : 1 ## Median :154.0 Median :1 Addy : 1 ## Mean :187.2 Mean :1 Albert : 1 ## 3rd Qu.:236.0 3rd Qu.:1 Allen : 1 ## Max. :514.0 Max. :1 (Other):89 ## NA&#39;s : 2 2.6.4 Bear Weights and Ages Histogram of Bear Weights ggplot(data=Bears_Subset, aes(x=Weight)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;Weight&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Weights of Bears&quot;) We see that bears most commonly weigh between 100 and 200 lbs, and the distribution of weights is right-skewed. Histogram of Bear Ages ggplot(data=Bears_Subset, aes(x=Age)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;Age (in months)&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Ages of Bears (in months)&quot;) Most of the bears are less than 72 months (6 years) old, although there are a few older bears. Bears with Missing Ages Recall that 41 bears had missing ages. They will be ignored if we use age in our model. To see how this might impact predicted weights, let’s look at how weights compare for bears with and without missing ages. ggplot(data=Bears_Subset, aes(x=is.na(Age), y=Weight)) + geom_boxplot() + coord_flip() Bears with missing ages do not seem to be systematically different than those whose ages are recorded, with respect to weight, so the missing ages should not cause too much concern with out model results. Boxplot of Weight by Sex ggplot(data=Bears_Subset, aes(y=Weight, x=Sex)) + geom_boxplot() + xlab(&quot;Sex(1=M, 2=F)&quot;) + ylab(&quot;Weight in lbs&quot;) + ggtitle(&quot;Weight by Sex&quot;) + coord_flip() Boxplot of Age by Sex ggplot(data=Bears_Subset, aes(y=Age, x=Sex)) + geom_boxplot() + xlab(&quot;Sex(1=M, 2=F)&quot;) + ylab(&quot;Age in Months&quot;) + ggtitle(&quot;Age by Sex&quot;) + coord_flip() The median age for female bears is older than for male bears. There are 2 male bears that are much older than any others. Scatterplot of Age and Weight ggplot(data=Bears_Subset, aes(x=Age, y=Weight, color=Sex)) + geom_point() + xlab(&quot;Age in Months&quot;) We see that there is a positive, roughly linear, relationship between age and weight. We should note that this linear trend is not likely to continue outside the range of our observed ages. 2.6.5 Two Possible Models We’ll consider two possible models, based on different sets of assumptions. Model 1 Assumptions: * Assume weight increases linearly with age * Allow for differences in expected weight for male and female bears of same age * Assume male and female bears gain weight at the same rate as they age Model 2 Assumptions * Assumes weight increases linearly with age * Allows for differences in expected weight for male and female bears of same age * Allows male and female bears to gain weight at different rates as they age 2.6.6 Models with Interaction Notice that in Model 1, the effect of age on weight is the same for both sexes, while in Model 2, the effect of age on weight depends on sex. An interaction between two explanatory variables occurs when the effect of one explanatory variable on the response depends on the other explanatory variable. In Model 2, there is an interaction between age and sex. **Note that neigher Model 1 nor Model 2 is inherently “correct”. They are just different ways to represent and model growth of bears. We should rely on our intuition, and background knowledge, as well as the data to inform us which model is more appropriate in a given context. 2.6.7 Model Equations Model 1: \\(\\widehat{\\text{Weight}}= b_0+ b_1 \\times\\text{Age}+ b_2\\times\\text{I}_{Female}\\) Model 2: \\(\\widehat{\\text{Weight}}= b_0+ b_1 \\times\\text{Age}+ b_2\\times\\text{I}_{Female} + b_3\\times\\text{Age}\\times\\text{I}_{Female}\\) The term \\(b_3\\times\\text{Age}\\times\\text{I}_{Female}\\), involving a product of the explanatory variables is called an interaction term. 2.6.8 Expected Weight Equations Model 1: \\(\\widehat{\\text{Weight}}= b_0+ b_1 \\times\\text{Age}+ b_2\\times\\text{I}_{Female}\\) Sex Pred. Weight M \\(b_0 + b_1 \\times\\text{Age}\\) F \\((b_0 + b_2) + b_1 \\times\\text{Age}\\) Model 2: \\(\\widehat{\\text{Weight}}= b_0+ b_1 \\times\\text{Age}+ b_2\\times\\text{I}_{Female} + b_3\\times\\text{Age}\\times\\text{I}_{Female}\\) Sex Pred. Weight M \\(b_0 + b_1 \\times\\text{Age}\\) F \\((b_0 + b_2) + (b_1 + b_3) \\times\\text{Age}\\) Question: How should we interpret the coefficient \\(b_1\\) in Model 1? Is the interpretation the same in Model 2? Why or why not? 2.6.9 Interpretations of Interaction Model Coefficients Model 2 \\(\\widehat{\\text{Weight}}= b_0+ b_1 \\times\\text{Age}+ b_2\\times\\text{I}_{Female} + b_3\\times\\text{Age}\\times\\text{I}_{Female}\\) Sex Pred. Weight M \\(b_0 + b_1 \\times\\text{Age}\\) F \\((b_0 + b_2) + (b_1 + b_3) \\times\\text{Age}\\) Interpretations: \\(b_0\\): expected weight of a male bear at birth (caution:extrapolation) \\(b_1\\): expected weight gain per month for male bears \\(b_2\\): expected difference in weight between female and male bears at birth (caution:extrapolation) \\(b_3\\): expected difference in monthly weight gain for female bears, compared to male bears \\(b_0+b_2\\): expected weight of a female bear at birth (caution:extrapolation) \\(b1 + b3\\): expected weight gain per month for female bears 2.6.10 Model 1 R Output Bears_M_Age_Sex &lt;- lm(data=Bears_Subset, Weight ~ Age + Sex) summary(Bears_M_Age_Sex) ## ## Call: ## lm(formula = Weight ~ Age + Sex, data = Bears_Subset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -164.194 -48.483 -3.723 27.766 188.684 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 82.6049 16.4019 5.036 0.0000058437067215 *** ## Age 2.9242 0.2914 10.035 0.0000000000000744 *** ## Sex2 -79.8967 20.1416 -3.967 0.00022 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 71.33 on 53 degrees of freedom ## (41 observations deleted due to missingness) ## Multiple R-squared: 0.6679, Adjusted R-squared: 0.6554 ## F-statistic: 53.29 on 2 and 53 DF, p-value: 0.0000000000002061 2.6.11 Model 2 R Output To fit an interaction model in R, use * instead of + Bears_M_Age_Sex_Int &lt;- lm(data=Bears_Subset, Weight~ Age*Sex) summary(Bears_M_Age_Sex_Int) ## ## Call: ## lm(formula = Weight ~ Age * Sex, data = Bears_Subset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -207.583 -38.854 -9.574 23.905 174.802 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 70.4322 17.7260 3.973 0.000219 *** ## Age 3.2381 0.3435 9.428 0.000000000000765 *** ## Sex2 -31.9574 35.0314 -0.912 0.365848 ## Age:Sex2 -1.0350 0.6237 -1.659 0.103037 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 70.18 on 52 degrees of freedom ## (41 observations deleted due to missingness) ## Multiple R-squared: 0.6846, Adjusted R-squared: 0.6664 ## F-statistic: 37.62 on 3 and 52 DF, p-value: 0.0000000000004552 2.6.12 Model 1 Interpretations \\(\\widehat{\\text{Weight}}= 82.60 + 2.92 \\times\\text{Age} - 79.90\\times\\text{I}_{Female}\\) Male Bears: \\[ \\widehat{\\text{Weight}}= 70.43 + 2.92 \\times\\text{Age} \\] Female Bears: \\[ \\widehat{\\text{Weight}}= (82.60 -79.90) + (2.92)\\times\\text{Age} \\\\ = 2.7+2.92\\times Age \\] For bears of the same sex, weight is expected to increase by 2.92 lbs. each month. On average, a female bear is expected to weigh 79.90 lbs less than a male bear of the same age. Approximately 67% of the variation in bear weights is explained by the model using age and sex as explanatory variables. 2.6.13 Model 2 Interpretations \\(\\widehat{\\text{Weight}}= 70.43 + 3.24 \\times\\text{Age}- 31.96\\times\\text{I}_{Female} -1.04\\times\\text{Age}\\times\\text{I}_{Female}\\) For Model 2, we can write different equations for bears of each sex: Male Bears: \\[ \\widehat{\\text{Weight}}= 70.43 + 3.24 \\times\\text{Age} \\] Female Bears: \\[ \\widehat{\\text{Weight}}= (70.43 -31.96) + (3.24-1.04)\\times\\text{Age} \\\\ = 38.47+ 2.20\\times Age \\] On average, a male bear is expected to weigh \\(b_0=70.43\\) lbs at birth, and a female bear is expected to weigh 38.47 lbs at birth. We should treat these interpretation with caution, since all bears in the dataset were at least 8 months old. A male bear is expected to gain 3.24 lbs per month, on average. A female bear is expected to gain 2.2 lbs per month on average. Approximately 68.5% of the variation in bear weights is explained by the model using age and sex as explanatory variables. 2.6.14 Predicting Bear Weights Suppose Sally and Yogi are 25 month old bears, Sally is a female, Yogi a male. Model 1 Sally’s Predicted Weight: \\(\\widehat{\\text{Weight}}= 82.60 + 2.920 \\times 25 -78.90\\times 1 \\approx 75.8 \\text{ lbs.}\\) Yogi’s Predicted Weight: \\(\\widehat{\\text{Weight}}= 82.60 + 2.920 \\times 25 -78.90\\times 0 \\approx 155.7 \\text{ lbs.}\\) Model 2 Sally’s Predicted Weight: \\(\\widehat{\\text{Weight}}= 70.43+ 3.24 \\times 25- 31.96\\times1 -1.04\\times25\\times1 \\approx 93.55 \\text{ lbs.}\\) Yogi’s Predicted Weight: \\(\\widehat{\\text{Weight}}= 70.43+ 3.24 \\times 25- 31.96\\times0 -1.04\\times25\\times0 \\approx 151.38 \\text{ lbs.}\\) 2.6.15 predict Function in R We can use the predict function in R to calculate predictions. We first create a dataframe with the values and categories of explanatory variables for the cases we’re trying to predict. Sex &lt;- factor(c(1, 2)) Age &lt;- c(25, 25) NewBears &lt;- data.frame(Age, Sex) Model 1 predict(Bears_M_Age_Sex, newdata=NewBears) ## 1 2 ## 155.71061 75.81394 Model 2 predict(Bears_M_Age_Sex_Int, newdata=NewBears) ## 1 2 ## 151.38566 93.55306 2.6.16 Bears Weight Model Considerations \\(R^2\\) increased from 0.67 to 0.68 when the interaction term is added. This is a relatively small increase, so we might question whether the interaction term is needed. The constant slope model allows us to combine information across sexes to estimate the expected slope. The interaction model treats the two sexes completely separately, thus has less information to use for each estimate. Which model is preferable is not clear. In addition to the data, we should consider other relevent information. Do experts who study bears believe it is reasonable to assume that male and female bears grow at the same rate per month? While the models yield drastically different predictions for very young bears, the differences are not as big for bear 8 months or older. Regardless of model we use, we should be careful about making predictions for bears that are younger or older than those that we have data on. Both of these models contain assumptions that are probably unrealistic Both models assume that bears of the same sex gain weight linearly with age. A more realistic model might assume that bears gain weight more quickly when they are younger, and that the rate of growth slows once they reach adulthood. Are there variables not included in the model that might be predictive of a bear’s weight? Of course there is no statistical model that perfectly describes expected weight gain of bears. The question is whether we can find a model that provides an approximation that is reasonable enough to draw conclusions from. As statistician George Box famously said, “All models are wrong, but some are useful.” 2.7 More on Interaction 2.7.1 Two Categorical Variables In the previous section, we saw an example of an interaction involving a quantitative variable (age), and a categorical variable (sex). Interactions between two categorical variables (or two quantitative variables) also occur in practice. Recall the definition of an interaction is that the effect of one explanatory variable on the response variable depends on the other categorical variable. 2.7.2 Bears Weight By Season and Sex We’ll investigate whether bears tend to weigh more in certain seasons than others, and whether there effect of season is the same for male and female bears. We start by creating plots to explore the relationship between season and sex. Observations per Season Recall that the dataset contains the month the bear was observed. Let’s combine the months of April and May into a category called “Spring”, June, July, and August into “Summer”, and “September”, “October”, and “November”, into “Fall”. Bears_Subset &lt;- Bears_Subset %&gt;% mutate(Season = ifelse(Month %in% 4:5, &quot;Spring&quot;, ifelse(Month %in% 6:8, &quot;Summer&quot;, &quot;Fall&quot;))) Bears_Subset$Season &lt;- as.factor(Bears_Subset$Season) ggplot(data=Bears_Subset, aes(x=Season)) + geom_bar(color=&quot;white&quot;, fill=&quot;lightblue&quot;) Boxplot of Weight by Season ggplot(data=Bears_Subset, aes(x=Season, y=Weight)) + geom_boxplot() + geom_jitter() Boxplot of Weight by Sex ggplot(data=Bears_Subset, aes(y=Weight, x=Sex)) + geom_boxplot() + geom_jitter()+ xlab(&quot;Sex(1=M, 2=F)&quot;) + ylab(&quot;Weight&quot;) + ggtitle(&quot;Weight by Sex&quot;) + coord_flip() We see that male bears (Category 1) weigh more than female bears on average, and that there is more variability in the weights of male bears than female bears. 2.7.3 Two Models We’ll consider two models: Model 1 Assumptions (No interaction): Allows weights to differ by sex and season. Assumes difference between sexes is the same in each season and difference between seasons is the same for each sex. Model 2 Assumptions: (Interaction between sex and season) Allows for differences between seasons and sexes. Allows for differences between sexes to vary between seasons and difference between seasons to vary between sexes. 2.7.4 Model Equations Model 1 Equation (No Interaction) \\(\\widehat{\\text{Weight}} = b_0+b_1\\times\\text{I}_{\\text{Spring}}+b_2\\times\\text{I}_{\\text{Summer}} + b_3\\times\\text{I}_{\\text{Female}}\\) Season Male Female Fall \\(b_0\\) \\(b_0 + b_3\\) Spring \\(b_0 + b_1\\) \\(b_0 + b_1+ b_3\\) Summer \\(b_0 + b_2\\) \\(b_0 + b_2+ b_3\\) Model 2 Equation (Interaction) \\[\\widehat{\\text{Weight}} = b_0 + b_1 \\times\\text{I}_{\\text{Spring}} + b_2\\times\\text{I}_{\\text{Summer}} +b_3\\times\\text{I}_{\\text{Female}}\\\\ +b_4\\times\\text{I}_{\\text{Spring}}\\text{I}_{\\text{Female}} +b_5\\times\\text{I}_{\\text{Summer}}\\text{I}_{\\text{Female}}\\] Male Female Fall \\(b_0\\) \\(b_0+b_3\\) Spring \\(b_0+b_1\\) \\(b_0+b_1 +b_3+b_4\\) Summer \\(b_0+b_2\\) \\(b_0+b_2+b_3+b_5\\) \\(b_4\\) and \\(b_5\\) are called interaction effects. Notice that in Model 1, \\(b3\\) always represents the expected difference in weights between male and female bears, regardless of season, while in Model 2, the expected difference between male and female bears’ weights depends on the season. 2.7.5 Bears Season and Sex Interaction Model To fit an interaction model in R, use * instead of + Bears_M_Season_Sex_Int &lt;- lm(data=Bears_Subset, Weight~Season * Sex) summary(Bears_M_Season_Sex_Int) ## ## Call: ## lm(formula = Weight ~ Season * Sex, data = Bears_Subset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -181.14 -73.14 -13.07 58.81 292.86 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 221.14 20.28 10.905 &lt;0.0000000000000002 *** ## SeasonSpring -14.00 45.99 -0.304 0.762 ## SeasonSummer -17.95 29.49 -0.608 0.544 ## Sex2 -50.07 35.54 -1.409 0.162 ## SeasonSpring:Sex2 -29.08 68.34 -0.425 0.672 ## SeasonSummer:Sex2 -30.41 50.73 -0.599 0.550 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 109.2 on 91 degrees of freedom ## Multiple R-squared: 0.1064, Adjusted R-squared: 0.0573 ## F-statistic: 2.167 on 5 and 91 DF, p-value: 0.06458 2.7.6 Predictions for Season and Sex Interaction Model \\[\\widehat{\\text{Weight}} = b_0 + b_1 \\times\\text{I}_{\\text{Spring}} + b_2\\times\\text{I}_{\\text{Summer}} +b_3\\times\\text{I}_{\\text{Female}}\\\\ +b_4\\times\\text{I}_{\\text{Spring}}\\text{I}_{\\text{Female}} +b_5\\times\\text{I}_{\\text{Summer}}\\text{I}_{\\text{Female}}\\] \\[ \\begin{aligned} \\widehat{\\text{Weight}} &amp;= 221.14 -14.00 \\times\\text{I}_{\\text{Spring}} -17.95\\times\\text{I}_{\\text{Summer}} -50.07\\times\\text{I}_{\\text{Female}} \\\\ &amp;-29.08\\times\\text{I}_{\\text{Spring}}\\text{I}_{\\text{Female}} -30.41\\times\\text{I}_{\\text{Summer}}\\text{I}_{\\text{Female}} \\end{aligned} \\] Male Female Fall 221.14 -14.00(0) -17.95(0) -50.07(0) -29.08(0)(0) -30.41(0)(0)=221.14 221.14 -14.00(0) -17.95(0) -50.07(1) -29.08(0)(1) -30.41(0)(1) =171.07 Spring 221.14 -14.00(1) -17.95(0) -50.07(0) -29.08(1)(0) -30.41(0)(0) =207.14 211.375 + 8.01221.14 -14.00(1) -17.95(0) -50.07(1) -29.08(1)(1) -30.41(0)(1)=128.00 Summer 221.14 -14.00(0) -17.95(1) -50.07(0) -29.08(0)(0) -30.41(1)(0) =203.19 221.14 -14.00(0) -17.95(1) -50.07(1) -29.08(0)(1) -30.41(1)(1) =122.71 2.7.7 Season and Sex Interaction Model Interpretations \\[\\widehat{\\text{Weight}} = b_0 + b_1 \\times\\text{I}_{\\text{Spring}} + b_2\\times\\text{I}_{\\text{Summer}} +b_3\\times\\text{I}_{\\text{Female}}\\\\ +b_4\\times\\text{I}_{\\text{Spring}}\\text{I}_{\\text{Female}} +b_5\\times\\text{I}_{\\text{Summer}}\\text{I}_{\\text{Female}}\\] Male Female Fall \\(b_0\\) \\(b_0+b_3\\) Spring \\(b_0+b_1\\) \\(b_0+b_1 +b_3+b_4\\) Summer \\(b_0+b_2\\) \\(b_0+b_2+b_3+b_5\\) \\(b_0\\) represents expected weight of male bear in fall \\(b_1\\) represents difference between expected male bear weight in spring, compared to fall \\(b_2\\) represents difference between expected male bear weight in summer, compared to fall \\(b_3\\) represents difference between expected female bear weight, compared to male bear weight in fall \\(b_4\\) represents difference in expected weights between the sexes in the spring, compared to the difference in the fall \\(b_5\\) represents difference in expected weights between the sexes in the summer, compared to the difference in the fall 2.7.8 Interpretations for Season and Sex Interaction Model \\[ \\begin{aligned} \\widehat{\\text{Weight}} &amp;= 221.14 -14.00 \\times\\text{I}_{\\text{Spring}} -17.95\\times\\text{I}_{\\text{Summer}} -50.07\\times\\text{I}_{\\text{Female}} \\\\ &amp;-29.08\\times\\text{I}_{\\text{Spring}}\\text{I}_{\\text{Female}} -30.41\\times\\text{I}_{\\text{Summer}}\\text{I}_{\\text{Female}} \\end{aligned} \\] On average, male bears are expected to weigh 221.14 lbs in the fall. On average male bears are expected to weigh 14 lbs less in the spring than in the fall. On average, male bears are expected to weigh 17.95 lbs less in the summer than in the fall. On average, female bears are expected to weigh 50.07 lbs less than male bears in the fall. On average, the female bears are expected to weigh 29.08 lbs. less relative to male bears in the spring, compared to the expected difference the fall. Thus, female bears are expected to weigh 50.07 + 29.08 = 79.17 lbs less than male bears in the spring. On average, female bears are expected to weigh 30.41 lbs less, relative to male bears in the summer, compared to the expected difference in the fall. Thus, female bears are expected to weigh \\(50.07 + 30.41 = 80.48\\) lbs less than male bears in the summer. The interaction model explains about 10.6% of the variation in bear weights. 2.7.9 Predicting New Observations in R We can calculate predictions directly in R by putting the new data in a data.frame and calling the predict() function. Season &lt;- c(&quot;Fall&quot;, &quot;Fall&quot;, &quot;Spring&quot;, &quot;Spring&quot;, &quot;Summer&quot;, &quot;Summer&quot;) Sex &lt;- factor(c(1,2,1,2,1,2)) NewBears &lt;- data.frame(Season, Sex) predict(Bears_M_Season_Sex_Int, newdata=NewBears) ## 1 2 3 4 5 6 ## 221.1379 171.0714 207.1429 128.0000 203.1923 122.7143 2.7.10 Interaction Between Two Quantitative Variables Interactions are also possible between two quantitative variables. Suppose, for example, that we want to examine the relationship between age and length on weight of a bear. If we expect that the effect of an additional inch in length, on the weight of a bear, might be different depending on the bear’s age, then this would be an example of an interaction between age and length. 2.7.11 Models for Age and Length Model 1 Assumptions (No interaction): Allows weights to differ by age and length Assumes rate of change in weight with respect to age is the same, regardless of length, and rate of change in age with respect to length is same, regardless of age. Model 2 Assumptions: (Interaction between age and length) Allows for differences between seasons and sexes. Allows rate of change in weight with respect to age, to differ, depending on length, and rate of change in weight with respect to length, to differ, depending on age. 2.7.12 Model Equations Model 1 Equation (No Interaction) \\(\\widehat{\\text{Weight}} = b_0+b_1\\times\\text{Age}+b_2\\times\\text{Length}\\) Model 2 Equation (Interaction) \\(\\widehat{\\text{Weight}} = b_0+b_1\\times\\text{Age}+b_2\\times\\text{Length} + b_3 \\times\\text{Age}\\times\\text{Length}\\) \\(b_3\\) is an interaction effects. 2.7.13 6 and 24 Month Old Bears In the interaction model (Model 2), the effect of length on weight depends on age: For a 6-month old bear: \\[ \\begin{aligned} \\widehat{\\text{Weight}} &amp; = b_0+6b_1+b_2\\times\\text{Length} + 6b_3 \\times\\text{Length} \\\\ &amp; = (b_0+6b_1)+(b_2+6b_3)\\times\\text{Length} \\end{aligned} \\] For each additional inch in length, the weight of a 6-month old bear is expected to increase by \\(b_2+6b_3\\) pounds. For a 24-month old bear: \\[ \\begin{aligned} \\widehat{\\text{Weight}} &amp; = b_0+6b_1+b_2\\times\\text{Length} + 6b_3 \\times\\text{Length} \\\\ &amp; = (b_0+24b_1)+(b_2+24b_3)\\times\\text{Length} \\end{aligned} \\] For each additional inch in length, the weight of a 24-month old bear is expected to increase by \\(b_2+24b_3\\) pounds. 2.7.14 Interaction Model in R Bears_M_Age_Length_Int &lt;- lm(data=Bears_Subset, Weight~ Age*Length) summary(Bears_M_Age_Length_Int) ## ## Call: ## lm(formula = Weight ~ Age * Length, data = Bears_Subset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -121.93 -29.90 -7.37 16.25 130.22 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -165.82578 54.76390 -3.028 0.003826 ** ## Age -7.66320 1.97286 -3.884 0.000292 *** ## Length 5.62418 1.00480 5.597 0.000000826 *** ## Age:Length 0.12577 0.02848 4.416 0.000051090 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 49.1 on 52 degrees of freedom ## (41 observations deleted due to missingness) ## Multiple R-squared: 0.8456, Adjusted R-squared: 0.8367 ## F-statistic: 94.94 on 3 and 52 DF, p-value: &lt; 0.00000000000000022 2.7.15 Age, Length Interaction Interpretations For a 6-month old bear: \\[ \\begin{aligned} \\widehat{\\text{Weight}} &amp;= (-165.83+6(-7.66))+(5.62+6(0.13))\\times\\text{Length} \\\\ &amp; = -211.79+6.4\\times\\text{Length} \\end{aligned} \\] For a 24-month old bear: \\[ \\begin{aligned} \\widehat{\\text{Weight}} &amp;= (-165.83+24(-7.66))+(5.62+24(0.13))\\times\\text{Length} \\\\ &amp; = -349.67+8.65\\times\\text{Length} \\end{aligned} \\] For each additional inch in length, the weight of a 6-month old bear is expected to increase by \\(5.62+6(0.13)=6.4\\) pounds. For each additional inch in length, the weight of a 24-month old bear is expected to increase by \\(5.62+24(0.13)=8.65\\) pounds. The interaction model explains about 85% of total variability in bear weight. We could, of course, perform similar calculations for bears of any age/length. The key is that the effect of length on weight depends on age (and likewise, the effect of age on weight depends on length). 2.7.16 Interaction vs Correlation It is easy to confuse the concept of interaction with that of correlation (seen in Section 1.2). A correlation between two variables means that as one increases, the other is more likely to increase or decrease. An interaction between two explanatory variables means that the effect of one on the response depends on the other. For example, we might expect that bigger cars will get lower gas mileage. This is an example of a (negative) correlation between gas mileage and size of the car. In the example, there would only be an interaction, if the effect of gas mileage on a response variable (say price) depends on the size of the car. If for example, better gas mileage was associated with higher price for small cars, but had no impact on price for large cars, then this would be an example of an interaction between size and gas mileage. "],["hypothesis-testing-via-permutation.html", "Chapter 3 Hypothesis Testing via Permutation 3.1 Test for Difference in Means 3.2 Test for Difference in Standard Deviation 3.3 Test for Regression Slope 3.4 Test for Comparing Multiple Groups", " Chapter 3 Hypothesis Testing via Permutation Learning Outcomes: State null and alternative hypotheses associated with models involving categorical and quantitative explanatory variables. Explain how to use permutation tests for hypotheses involving means, medians, F-statistics, slopes, and other regression coefficients, as well as functions of these statistics. Interpret p-values in context. Explain the conclusions we should draw from from a hypothesis test, while accounting for other information available in a dataset. Explain how to simultaneously test for differences between multiple groups. Distinguish between statistical significance and practical importance. 3.1 Test for Difference in Means 3.1.1 Mercury Levels in Florida Lakes A 2004 study by Lange, T., Royals, H. and Connor, L. examined Mercury accumulation in large-mouth bass, taken from a sample of 53 Florida Lakes. If Mercury accumulation exceeds 0.5 ppm, then there are environmental concerns. In fact, the legal safety limit in Canada is 0.5 ppm, although it is 1 ppm in the United States. Figure 3.1: https://www.maine.gov/ifw/fish-wildlife/fisheries/species-information/largemouth-bass.html 3.1.2 Florida Lakes Dataset data(&quot;FloridaLakes&quot;) glimpse(FloridaLakes) ## Rows: 53 ## Columns: 12 ## $ ID &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1… ## $ Lake &lt;chr&gt; &quot;Alligator&quot;, &quot;Annie&quot;, &quot;Apopka&quot;, &quot;Blue Cypress&quot;, &quot;Bri… ## $ Alkalinity &lt;dbl&gt; 5.9, 3.5, 116.0, 39.4, 2.5, 19.6, 5.2, 71.4, 26.4, 4… ## $ pH &lt;dbl&gt; 6.1, 5.1, 9.1, 6.9, 4.6, 7.3, 5.4, 8.1, 5.8, 6.4, 5.… ## $ Calcium &lt;dbl&gt; 3.0, 1.9, 44.1, 16.4, 2.9, 4.5, 2.8, 55.2, 9.2, 4.6,… ## $ Chlorophyll &lt;dbl&gt; 0.7, 3.2, 128.3, 3.5, 1.8, 44.1, 3.4, 33.7, 1.6, 22.… ## $ AvgMercury &lt;dbl&gt; 1.23, 1.33, 0.04, 0.44, 1.20, 0.27, 0.48, 0.19, 0.83… ## $ NumSamples &lt;int&gt; 5, 7, 6, 12, 12, 14, 10, 12, 24, 12, 12, 12, 7, 43, … ## $ MinMercury &lt;dbl&gt; 0.85, 0.92, 0.04, 0.13, 0.69, 0.04, 0.30, 0.08, 0.26… ## $ MaxMercury &lt;dbl&gt; 1.43, 1.90, 0.06, 0.84, 1.50, 0.48, 0.72, 0.38, 1.40… ## $ ThreeYrStdMercury &lt;dbl&gt; 1.53, 1.33, 0.04, 0.44, 1.33, 0.25, 0.45, 0.16, 0.72… ## $ AgeData &lt;int&gt; 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1… We are interested in whether mercury levels are higher or lower, on average, in Northern Florida compared to Southern Florida. We’ll divide the state along route 50, which runs East-West, passing through Northern Orlando. Figure 3.2: from Google Maps We add a variable indicating whether each lake lies in the northern or southern part of the state. library(Lock5Data) data(FloridaLakes) #Location relative to rt. 50 FloridaLakes$Location &lt;- as.factor(c(&quot;S&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;S&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;S&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;N&quot;,&quot;S&quot;,&quot;S&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;S&quot;,&quot;S&quot;,&quot;S&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;)) print.data.frame(data.frame(FloridaLakes%&gt;% select(Lake, Location, AvgMercury)), row.names = FALSE) ## Lake Location AvgMercury ## Alligator S 1.23 ## Annie S 1.33 ## Apopka N 0.04 ## Blue Cypress S 0.44 ## Brick S 1.20 ## Bryant N 0.27 ## Cherry N 0.48 ## Crescent N 0.19 ## Deer Point N 0.83 ## Dias N 0.81 ## Dorr N 0.71 ## Down S 0.50 ## Eaton N 0.49 ## East Tohopekaliga S 1.16 ## Farm-13 N 0.05 ## George N 0.15 ## Griffin N 0.19 ## Harney N 0.77 ## Hart S 1.08 ## Hatchineha S 0.98 ## Iamonia N 0.63 ## Istokpoga S 0.56 ## Jackson N 0.41 ## Josephine S 0.73 ## Kingsley N 0.34 ## Kissimmee S 0.59 ## Lochloosa N 0.34 ## Louisa S 0.84 ## Miccasukee N 0.50 ## Minneola N 0.34 ## Monroe N 0.28 ## Newmans N 0.34 ## Ocean Pond N 0.87 ## Ocheese Pond N 0.56 ## Okeechobee S 0.17 ## Orange N 0.18 ## Panasoffkee N 0.19 ## Parker S 0.04 ## Placid S 0.49 ## Puzzle N 1.10 ## Rodman N 0.16 ## Rousseau N 0.10 ## Sampson N 0.48 ## Shipp S 0.21 ## Talquin N 0.86 ## Tarpon S 0.52 ## Tohopekaliga S 0.65 ## Trafford S 0.27 ## Trout S 0.94 ## Tsala Apopka N 0.40 ## Weir N 0.43 ## Wildcat N 0.25 ## Yale N 0.27 3.1.3 Comparing Northern and Southern Lakes We are interested in investigating whether average mercury levels are higher in either Northern Florida or Southern Florida than the other. LakesBP &lt;- ggplot(data=FloridaLakes, aes(x=Location, y=AvgMercury, fill=Location)) + geom_boxplot() + geom_jitter() + ggtitle(&quot;Mercury Levels in Florida Lakes&quot;) + xlab(&quot;Location&quot;) + ylab(&quot;Mercury Level&quot;) + theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + coord_flip() LakesBP LakesTable &lt;- FloridaLakes %&gt;% group_by(Location) %&gt;% summarize(MeanHg=mean(AvgMercury), StDevHg=sd(AvgMercury), N=n()) kable(LakesTable) Location MeanHg StDevHg N N 0.4245455 0.2696652 33 S 0.6965000 0.3838760 20 3.1.4 Model for Northern and Southern Lakes \\(\\widehat{\\text{Hg}} = b_0 +b_1\\text{I}_{\\text{South}}\\) \\(b_0\\) represents the mean mercury level for lakes in North Florida, and \\(b_1\\) represents the mean difference in mercury level for lakes in South Florida, compared to North Florida 3.1.5 Model for Lakes R Output Lakes_M &lt;- lm(data=FloridaLakes, AvgMercury ~ Location) summary(Lakes_M) ## ## Call: ## lm(formula = AvgMercury ~ Location, data = FloridaLakes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.65650 -0.23455 -0.08455 0.24350 0.67545 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.42455 0.05519 7.692 0.000000000441 *** ## LocationS 0.27195 0.08985 3.027 0.00387 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3171 on 51 degrees of freedom ## Multiple R-squared: 0.1523, Adjusted R-squared: 0.1357 ## F-statistic: 9.162 on 1 and 51 DF, p-value: 0.003868 3.1.6 Interpreting Lakes Regression Output \\(\\widehat{\\text{Hg}} = 0.4245455 +0.2719545\\text{I}_{\\text{South}}\\) \\(b_1 = 0.27915= 0.6965 - 0.4245\\) is equal to the difference in mean mercury levels between Northern and Southern lakes. (We’ve already seen that for categorical variables, the least-squares estimate is the mean, so this makes sense.) We can use \\(b_1\\) to assess the size of the difference in mean mercury concentration levels. Since the lakes we observed are only a sample of all lakes, we cannot assume the difference in mercury concentrations is exactly 0.4245 for all Northern vs Southern Florida lakes. 3.1.7 Evidence of Difference? Do these results provide evidence that among all Florida lakes, the mean mercury level is higher in the South than in the North? Possible Explanations: 1. There really is a difference in average mercury level between lakes in Northern and Southern Florida. 2. There really is no difference in average mercury levels between lakes in Northern and Southern Florida, and we just happened, by chance, to select more lakes with higher mercury concentrations in Southern Florida. Question: Which of these explanations do you think is more reasonable? 3.1.8 Permutation Test Key Question: How likely is it that we would have observed a difference in means (i.e. a value of \\(b_1\\)) as extreme as 0.6965-0.4245 = 0.27195 ppm, merely by chance, if there is really no relationship between location and mercury level? We can answer the key question using a procedure known as a permutation test. In a permutation test, we randomly permute our data to simulate a situation where there is no relationship between our explanatory and response variable. We observe whether it is plausible to observe values of a statistic (in this case the difference in means) as extreme or more extreme than what we saw in the actual data. We’ll simulate situations where there is no relationship between location and mercury level, and see how often we observe a difference in means (\\(b_1\\)) as extreme as 0.27195. Procedure: Randomly shuffle the locations of the lakes, so that any relationship between location and mercury level is due only to chance. Calculate the difference in mean mercury levels (i.e. value of \\(b_1\\)) in “Northern” and “Southern” lakes, using the shuffled data. Repeat steps 1 and 2 many (say 10,000) times, recording the difference in means (i.e. value of \\(b_1\\)) each time. Analyze the distribution of mean differences, simulated under the assumption that there is no relationship between location and mercury level. Look whether the actual difference we observed is consistent with the simulation results. 3.1.9 Applet for Permutation Tests An Art of Stat Web App performs the steps listed above. Follow these steps to perform the simulation. 1. Open the app. 2. Under “Enter Data”, Choose “Provide Own”. 3. For “Response Variable”, type in “Mercury Concentration.” 4. For “Group 1 Label”, enter “North”, and for “Group 2 Label”, enter “South”. 5. Copy/paste the following values into the “Group 1 Data” box. These are the mercury concentrations in the lakes in North Florida. NLakesHg &lt;- as.data.frame(FloridaLakes %&gt;% filter(Location==&quot;N&quot;))$AvgMercury kable(NLakesHg, fill = getOption(&quot;width&quot;)) x 0.04 0.27 0.48 0.19 0.83 0.81 0.71 0.49 0.05 0.15 0.19 0.77 0.63 0.41 0.34 0.34 0.50 0.34 0.28 0.34 0.87 0.56 0.18 0.19 1.10 0.16 0.10 0.48 0.86 0.40 0.43 0.25 0.27 Copy/paste the following values into the “Group 2 Data” box. These are the mercury concentrations in the lakes in North Florida. NLakesHg &lt;- as.data.frame(FloridaLakes %&gt;% filter(Location==&quot;S&quot;))$AvgMercury kable(NLakesHg, fill = getOption(&quot;width&quot;)) x 1.23 1.33 0.44 1.20 0.50 1.16 1.08 0.98 0.56 0.73 0.59 0.84 0.17 0.04 0.49 0.21 0.52 0.65 0.27 0.94 For the question: “Select how many permutations you want to generate:” Choose 1. Click “Generate” The app will randomly assign the lakes to groups, with sizes matching the original data, and calculate the mean for each group. Observe means in the “Permuted Sample” and the difference in means. Is the difference as extreme as we observed in the actual data? STOP HERE UNTIL FURTHER INSTRUCTED. Now generate 5 more permutations. Did you get any differences in means as extreme as we observed in the original data? What does this tell us about the likelihood of observing a difference in means as extreme as 0.27 by chance? STOP HERE UNTIL FURTHER INSTRUCTED. Now generate 10,000 permutations. How often do you get a difference as extreme as we observed in the original data? What does this tell us about the likelihood of observing a difference in means as extreme as 0.27 by chance? 3.1.10 Five Permutations in R We’ll use R to perform permutation tests in the same manner as is done in the Art of Stat App. First Permutation Recall these groups were randomly assigned, so the only differences in averages are due to random chance. ShuffledLakes &lt;- FloridaLakes ## create copy of dataset ShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] Shuffle1df &lt;- data.frame(FloridaLakes$Lake, FloridaLakes$Location, FloridaLakes$AvgMercury, ShuffledLakes$Location) names(Shuffle1df) &lt;- c(&quot;Lake&quot;, &quot;Location&quot;, &quot;AvgMercury&quot;, &quot;Shuffled Location&quot;) kable(head(Shuffle1df)) Lake Location AvgMercury Shuffled Location Alligator S 1.23 N Annie S 1.33 S Apopka N 0.04 S Blue Cypress S 0.44 N Brick S 1.20 N Bryant N 0.27 N LakesPerm &lt;- ggplot(data=Shuffle1df, aes(x=`Shuffled Location`, y=AvgMercury, fill=`Shuffled Location`)) + geom_boxplot() + geom_jitter() + ggtitle(&quot;Mercury Levels in Florida Lakes&quot;) + xlab(&quot;Location&quot;) + ylab(&quot;Mercury Level&quot;) + theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + coord_flip() LakesPerm LakesPermTable &lt;- Shuffle1df %&gt;% group_by(`Shuffled Location`) %&gt;% summarize(MeanHg=mean(AvgMercury), StDevHg=sd(AvgMercury), N=n()) kable(LakesPermTable) Shuffled Location MeanHg StDevHg N N 0.5460606 0.3552986 33 S 0.4960000 0.3225784 20 Second Permutation ShuffledLakes &lt;- FloridaLakes ## create copy of dataset ShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] kable(head(Shuffle1df)) Lake Location AvgMercury Shuffled Location Alligator S 1.23 N Annie S 1.33 S Apopka N 0.04 S Blue Cypress S 0.44 N Brick S 1.20 N Bryant N 0.27 N Shuffle1df &lt;- data.frame(FloridaLakes$Lake, FloridaLakes$Location, FloridaLakes$AvgMercury, ShuffledLakes$Location) names(Shuffle1df) &lt;- c(&quot;Lake&quot;, &quot;Location&quot;, &quot;AvgMercury&quot;, &quot;Shuffled Location&quot;) LakesPerm &lt;- ggplot(data=Shuffle1df, aes(x=`Shuffled Location`, y=AvgMercury, fill=`Shuffled Location`)) + geom_boxplot() + geom_jitter() + ggtitle(&quot;Mercury Levels in Florida Lakes&quot;) + xlab(&quot;Location&quot;) + ylab(&quot;Mercury Level&quot;) + theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + coord_flip() LakesPerm LakesPermTable &lt;- Shuffle1df %&gt;% group_by(`Shuffled Location`) %&gt;% summarize(MeanHg=mean(AvgMercury), StDevHg=sd(AvgMercury), N=n()) kable(LakesPermTable) Shuffled Location MeanHg StDevHg N N 0.4839394 0.3316431 33 S 0.5985000 0.3527975 20 Third Permutation ShuffledLakes &lt;- FloridaLakes ## create copy of dataset ShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] kable(head(Shuffle1df)) Lake Location AvgMercury Shuffled Location Alligator S 1.23 S Annie S 1.33 S Apopka N 0.04 N Blue Cypress S 0.44 S Brick S 1.20 N Bryant N 0.27 S Shuffle1df &lt;- data.frame(FloridaLakes$Lake, FloridaLakes$Location, FloridaLakes$AvgMercury, ShuffledLakes$Location) names(Shuffle1df) &lt;- c(&quot;Lake&quot;, &quot;Location&quot;, &quot;AvgMercury&quot;, &quot;Shuffled Location&quot;) LakesPerm &lt;- ggplot(data=Shuffle1df, aes(x=`Shuffled Location`, y=AvgMercury, fill=`Shuffled Location`)) + geom_boxplot() + geom_jitter() + ggtitle(&quot;Mercury Levels in Florida Lakes&quot;) + xlab(&quot;Location&quot;) + ylab(&quot;Mercury Level&quot;) + theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + coord_flip() LakesPerm LakesPermTable &lt;- Shuffle1df %&gt;% group_by(`Shuffled Location`) %&gt;% summarize(MeanHg=mean(AvgMercury), StDevHg=sd(AvgMercury), N=n()) kable(LakesPermTable) Shuffled Location MeanHg StDevHg N N 0.5324242 0.3863777 33 S 0.5185000 0.2583607 20 Fourth Permutation ShuffledLakes &lt;- FloridaLakes ## create copy of dataset ShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] kable(head(Shuffle1df)) Lake Location AvgMercury Shuffled Location Alligator S 1.23 N Annie S 1.33 N Apopka N 0.04 N Blue Cypress S 0.44 S Brick S 1.20 N Bryant N 0.27 S Shuffle1df &lt;- data.frame(FloridaLakes$Lake, FloridaLakes$Location, FloridaLakes$AvgMercury, ShuffledLakes$Location) names(Shuffle1df) &lt;- c(&quot;Lake&quot;, &quot;Location&quot;, &quot;AvgMercury&quot;, &quot;Shuffled Location&quot;) LakesPerm &lt;- ggplot(data=Shuffle1df, aes(x=`Shuffled Location`, y=AvgMercury, fill=`Shuffled Location`)) + geom_boxplot() + geom_jitter() + ggtitle(&quot;Mercury Levels in Florida Lakes&quot;) + xlab(&quot;Location&quot;) + ylab(&quot;Mercury Level&quot;) + theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + coord_flip() LakesPerm LakesPermTable &lt;- Shuffle1df %&gt;% group_by(`Shuffled Location`) %&gt;% summarize(MeanHg=mean(AvgMercury), StDevHg=sd(AvgMercury), N=n()) kable(LakesPermTable) Shuffled Location MeanHg StDevHg N N 0.4833333 0.3314803 33 S 0.5995000 0.3527109 20 Fifth Permutation ShuffledLakes &lt;- FloridaLakes ## create copy of dataset ShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] kable(head(Shuffle1df)) Lake Location AvgMercury Shuffled Location Alligator S 1.23 N Annie S 1.33 S Apopka N 0.04 N Blue Cypress S 0.44 N Brick S 1.20 N Bryant N 0.27 S Shuffle1df &lt;- data.frame(FloridaLakes$Lake, FloridaLakes$Location, FloridaLakes$AvgMercury, ShuffledLakes$Location) names(Shuffle1df) &lt;- c(&quot;Lake&quot;, &quot;Location&quot;, &quot;AvgMercury&quot;, &quot;Shuffled Location&quot;) LakesPerm &lt;- ggplot(data=Shuffle1df, aes(x=`Shuffled Location`, y=AvgMercury, fill=`Shuffled Location`)) + geom_boxplot() + geom_jitter() + ggtitle(&quot;Mercury Levels in Florida Lakes&quot;) + xlab(&quot;Location&quot;) + ylab(&quot;Mercury Level&quot;) + theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + coord_flip() LakesPerm LakesPermTable &lt;- Shuffle1df %&gt;% group_by(`Shuffled Location`) %&gt;% summarize(MeanHg=mean(AvgMercury), StDevHg=sd(AvgMercury), N=n()) kable(LakesPermTable) Shuffled Location MeanHg StDevHg N N 0.5642424 0.3259508 33 S 0.4660000 0.3647551 20 3.1.11 R Code for Permutation Test We’ll write a for loop to perform 10,000 permutations and record the value of \\(b_1\\) (the difference in sample means) for each simulation. b1 &lt;- Lakes_M$coef[2] ## record value of b1 from actual data ## perform simulation b1Sim &lt;- rep(NA, 10000) ## vector to hold results ShuffledLakes &lt;- FloridaLakes ## create copy of dataset for (i in 1:10000){ #randomly shuffle locations ShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] ShuffledLakes_M&lt;- lm(data=ShuffledLakes, AvgMercury ~ Location) #fit model to shuffled data b1Sim[i] &lt;- ShuffledLakes_M$coef[2] ## record b1 from shuffled model } NSLakes_SimulationResults &lt;- data.frame(b1Sim) #save results in dataframe 3.1.12 Permutation Tests Results NSLakes_SimulationResultsPlot &lt;- ggplot(data=NSLakes_SimulationResults, aes(x=b1Sim)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + geom_vline(xintercept=c(b1, -1*b1), color=&quot;red&quot;) + xlab(&quot;Lakes: Simulated Value of b1&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Distribution of b1 under assumption of no relationship&quot;) NSLakes_SimulationResultsPlot It appears unlikely that we would observe a difference in sample mean (\\(b_1\\)) as extreme as 0.27195 ppm by chance, if there is really no relationship between location and mercury level. 3.1.13 Conclusions Number of simulations (out of 10,000) resulting in difference in means more extreme than 0.27195. sum(abs(b1Sim) &gt; abs(b1)) ## [1] 39 Proportion of simulations resulting in difference in means more extreme than 0.27195. mean(abs(b1Sim) &gt; abs(b1)) ## [1] 0.0039 The probability of observing a difference in means as extreme as 0.27195 by chance, when there is no relationship between location and mercury level is very low. There is strong evidence of a relationship between location and mercury level. In this case, there is strong evidence that mercury level is higher in Southern Lakes than northern Lakes. 3.1.14 Hypothesis Testing Terminology We can think of the simulation as a test of the following hypotheses: Hypothesis 1: Among all Florida lakes, average mercury level is the same for lakes in Northern Florida, as in Southern Florida. (Thus the difference of 0.27 we observed in our data occurred just by chance). Hypothesis 2: Among all Florida lakes, there is a difference in average mercury level between lakes in Northern Florida and Southern Florida. The “no difference,” or “chance alone” hypothesis is called the null hypothesis. The other hypothesis is called the alternative hypothesis. We used \\(b_1\\) to measure difference in average mercury levels between the locations in our observed data. We found that the probability of observing a difference in means as extreme as 0.27 when Hypothesis 1 is true is very low (approximately 0.0023) The statistic used to measure the difference or relationship we are interested in is called a test statistic. In this case, the test statistic is the difference in sample means (\\(b_1\\)) The p-value is the probability of observing a test statistic as extreme or more extreme than we did due to chance, when the null hypothesis is true. - A low p-value provides evidence against the null hypothesis. - A high p-value means that the data could have plausibly been obtained when the null hypothesis is true, and thus the null hypothesis cannot be ruled out. - A high p-value does not mean that the null hypothesis is true or probably true. A p-value can only tell us the strength of evidence against the null hypothesis, and should never be interpreted as support for the null hypothesis. 3.1.15 How Low Should the p-value Be knitr::include_graphics(&quot;pvals.png&quot;) 3.1.16 Practical Importance A low p-value tells us that the difference in average Mercury levels that we saw in our sample is unlikely to have occurred by chance, providing evidence that there is indeed a difference in average Mercury levels between Northern and Southern lakes. The p-value does not tell us anything about the size of the difference! If the difference is really small (say 0.001 ppm), perhaps there is no need to worry about it. It’s possible to get a small p-value even when the true difference is very small (especially when our sample size is large). In addition to a p-value, we should consider whether a difference is big enough to be meaningful in a practical way, before making any policy decisions. For now, we can use the difference in sample means of 0.27 ppm as an estimate of the size of the difference. Based on our limited knowledge of mercury levels, this does seem big enough to merit further investigation, and possible action. 3.2 Test for Difference in Standard Deviation 3.2.1 Standard Deviation Northern and Southern Lakes Recall that in our sample, the standard deviation was higher for the lakes in Southern Florida than Northern Florida. Note: for a sample of \\(n\\) observations, \\(y_1, \\ldots, y_n\\), standard deviation is a measure of spread, is calculated using the formula: \\[ SD=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(y_i-\\bar{y})^2} \\] LakesBP &lt;- ggplot(data=FloridaLakes, aes(x=Location, y=AvgMercury, fill=Location)) + geom_boxplot() + geom_jitter() + ggtitle(&quot;Mercury Levels in Florida Lakes&quot;) + xlab(&quot;Location&quot;) + ylab(&quot;Mercury Level&quot;) + theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + coord_flip() LakesBP LakesTable &lt;- FloridaLakes %&gt;% group_by(Location) %&gt;% summarize(MeanHg=mean(AvgMercury), StDevHg=sd(AvgMercury), N=n()) kable(LakesTable) Location MeanHg StDevHg N N 0.4245455 0.2696652 33 S 0.6965000 0.3838760 20 Does this provide evidence that there is really more variability in mercury levels for lakes in Southern Florida than in Northern Florida, or could we have just by chance picked lakes with more variability in South Florida? 3.2.2 Hypotheses Null Hypothesis: Standard deviation in mercury levels among all lakes in Northern Florida is the same as the standard deviation in mercury levels among all lakes in Southern Florida. Alternative Hypothesis: Standard deviation in mercury levels among all lakes in Northern Florida is different than the standard deviation in mercury levels among all lakes in Southern Florida. 3.2.3 Permutation Test Steps Procedure: Randomly shuffle the locations of the lakes, so that any relationship between location and mercury level is due only to chance. Calculate the difference in standard deviation in mercury levels (i.e. value of \\(b_1\\)) in “Northern” and “Southern” lakes, using the shuffled data. Repeat steps 1 and 2 many (say 10,000) times, recording the difference in standard deviations each time. Analyze the distribution of differences in standard deviation, simulated under the assumption that there is no relationship between location and mercury level. Look whether the actual difference we observed is consistent with the simulation results. Question: Looking back at the 5 simulations performed in the previous section, does it seem plausible that we could have observed a difference in standard deviations as extreme as \\(0.3839-0.2697 = 0.1142\\) by chance? 3.2.4 R Code for Permutation Test SDTab &lt;- FloridaLakes %&gt;% group_by(Location) %&gt;% summarize(SD=sd(AvgMercury)) DiffSD &lt;- SDTab$SD[2] - SDTab$SD[1] ## perform simulation DiffSim &lt;- rep(NA, 10000) ## vector to hold results ShuffledLakes &lt;- FloridaLakes ## create copy of dataset for (i in 1:10000){ #randomly shuffle locations ShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] SDTabSim &lt;- ShuffledLakes %&gt;% group_by(Location) %&gt;% summarize(SD=sd(AvgMercury)) DiffSim[i] &lt;- SDTabSim$SD[2] - SDTabSim$SD[1] #record difference in SD for simulated data } NSLakes_SDSimResults &lt;- data.frame(DiffSim) #save results in dataframe 3.2.5 Permutation Tests Results NSLakes_SDSimResultsPlot &lt;- ggplot(data=NSLakes_SDSimResults, aes(x=DiffSim)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + geom_vline(xintercept=c(DiffSD, -1*DiffSD), color=&quot;red&quot;) + xlab(&quot;Lakes: Difference in SD&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Distribution of Difference in SD under assumption of no relationship&quot;) NSLakes_SDSimResultsPlot Number of simulations (out of 10,000) resulting in standard deviations greater the 0.1142. sum(abs(DiffSim) &gt; abs(DiffSD)) ## [1] 601 Proportion of simulations (out of 10,000) resulting in standard deviations greater the 0.1142. mean(abs(DiffSim) &gt; abs(DiffSD)) ## [1] 0.0601 This p-value represents the probability of observing a difference in sample standard deviations as extreme as 0.1142 in a samples of size 33 and 20 by chance, if in fact, the standard deviation in mercury concentration levels is the same for lakes in Northern Florida as in Southern Florida. 3.2.6 Conclusions It is unlikely that we would observe a difference in standard deviations as extreme as 0.1142 by chance. There is evidence that lakes in Southern Florida exhibit more variability in mercury levels than lakes in Northern Florida (though the evidence is not as strong as it was when we were testing for a difference in means). Again, a p-value does not tell us whether a difference is practically meaningful. Without knowing a lot about mercury levels, and their impact on the ecosystem, it’s harder to tell wheter an estimated difference in standard deviations of 0.11 ppm is meaningful or not. It would be good to consult a biologist before making any decisions based on these results. 3.3 Test for Regression Slope 3.3.1 2015 Cars Dataset We consider data from the Kelly Blue Book, pertaining to new cars, released in 2015. We’ll investigate the relationship between price, length, and time it takes to accelerate from 0 to 60 mph. data(Cars2015) glimpse(Cars2015) ## Rows: 110 ## Columns: 20 ## $ Make &lt;fct&gt; Chevrolet, Hyundai, Kia, Mitsubishi, Nissan, Dodge, Chevrole… ## $ Model &lt;fct&gt; Spark, Accent, Rio, Mirage, Versa Note, Dart, Cruze LS, 500L… ## $ Type &lt;fct&gt; Hatchback, Hatchback, Sedan, Hatchback, Hatchback, Sedan, Se… ## $ LowPrice &lt;dbl&gt; 12.270, 14.745, 13.990, 12.995, 14.180, 16.495, 16.170, 19.3… ## $ HighPrice &lt;dbl&gt; 25.560, 17.495, 18.290, 15.395, 17.960, 23.795, 25.660, 24.6… ## $ Drive &lt;fct&gt; FWD, FWD, FWD, FWD, FWD, FWD, FWD, FWD, FWD, FWD, FWD, AWD, … ## $ CityMPG &lt;int&gt; 30, 28, 28, 37, 31, 23, 24, 24, 28, 30, 27, 27, 25, 27, 30, … ## $ HwyMPG &lt;int&gt; 39, 37, 36, 44, 40, 35, 36, 33, 38, 35, 33, 36, 36, 37, 39, … ## $ FuelCap &lt;dbl&gt; 9.0, 11.4, 11.3, 9.2, 10.9, 14.2, 15.6, 13.1, 12.4, 11.1, 11… ## $ Length &lt;int&gt; 145, 172, 172, 149, 164, 184, 181, 167, 179, 154, 156, 180, … ## $ Width &lt;int&gt; 63, 67, 68, 66, 67, 72, 71, 70, 72, 67, 68, 69, 70, 68, 69, … ## $ Wheelbase &lt;int&gt; 94, 101, 101, 97, 102, 106, 106, 103, 104, 99, 98, 104, 104,… ## $ Height &lt;int&gt; 61, 57, 57, 59, 61, 58, 58, 66, 58, 59, 58, 58, 57, 58, 59, … ## $ UTurn &lt;int&gt; 34, 37, 37, 32, 37, 38, 38, 37, 39, 34, 35, 38, 37, 36, 37, … ## $ Weight &lt;int&gt; 2345, 2550, 2575, 2085, 2470, 3260, 3140, 3330, 2990, 2385, … ## $ Acc030 &lt;dbl&gt; 4.4, 3.7, 3.5, 4.4, 4.0, 3.4, 3.7, 3.9, 3.4, 3.9, 3.9, 3.7, … ## $ Acc060 &lt;dbl&gt; 12.8, 10.3, 9.5, 12.1, 10.9, 9.3, 9.8, 9.5, 9.2, 10.8, 11.1,… ## $ QtrMile &lt;dbl&gt; 19.4, 17.8, 17.3, 19.0, 18.2, 17.2, 17.6, 17.4, 17.1, 18.3, … ## $ PageNum &lt;int&gt; 123, 148, 163, 188, 196, 128, 119, 131, 136, 216, 179, 205, … ## $ Size &lt;fct&gt; Small, Small, Small, Small, Small, Small, Small, Small, Smal… 3.3.2 Car Price and Acceleration Time LowPrice represents the price of a standard (non-luxury) model of a car. Acc060 represents time it takes to accelerate from 0 to 60 mph. data(Cars2015) CarsA060 &lt;- ggplot(data=Cars2015, aes(x=Acc060, y=LowPrice)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) CarsA060 3.3.3 Modeling Price using Acc060 \\(\\widehat{Price} = b_0 + b_1\\times\\text{Acc. Time}\\) Model assumes expected price is a linear function of acceleration time. Interpretations: \\(b_0\\) represents intercept of regression line, i.e. expected price of a car that can accelerate from 0 to 60 mph in no time. This is not a meaningful interpretation in context. \\(b_1\\) represents slope of regression line, i.e. expected change in price for each additional second it takes to accelerate from 0 to 60 mph. 3.3.4 Modeling for Car Price and Acceleration Cars_M_A060 &lt;- lm(data=Cars2015, LowPrice~Acc060) summary(Cars_M_A060) ## ## Call: ## lm(formula = LowPrice ~ Acc060, data = Cars2015) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.512 -6.544 -1.265 4.759 27.195 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 89.9036 5.0523 17.79 &lt;0.0000000000000002 *** ## Acc060 -7.1933 0.6234 -11.54 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.71 on 108 degrees of freedom ## Multiple R-squared: 0.5521, Adjusted R-squared: 0.548 ## F-statistic: 133.1 on 1 and 108 DF, p-value: &lt; 0.00000000000000022 3.3.5 Acc060 Model Interpretations \\(\\widehat{Price} = b_0 + b_1\\times\\text{Acc. Time}\\) \\(\\widehat{Price} = 89.90 - 7.193\\times\\text{Acc. Time}\\) Intercept \\(b_0\\) might be interpreted as the price of a car that can accelerate from 0 to 60 in no time, but this is not a meaningful interpretation since there are no such cars. \\(b_1=-7.1933\\) tells us that on average, the price of a car is expected to decrease by 7.19 thousand dollars for each additional second it takes to accelerate from 0 to 60 mph. \\(R^2 = 0.5521\\) tells us that 55% of the variation in price is explained by the linear model using acceleration time as the explanatory variable. 3.3.6 Is Car Price Associated with Acceleration Time? Is it possible that there is really no relationship between price and acceleration time, and we just happened to choose a sample that led to a slope of -7.1933, by chance? Is it possible that among all cars, the picture looks like the one below, and we just happened to draw a sample of 110 cars, showing a downward trend by chance? 3.3.7 Acc060 Key Question and Hypotheses If there is really no relationship between price and acceleration time, then we would expect a slope (i.e value of \\(b_1\\)) equal to 0. Key Question: How likely is it that we would have observed a slope (i.e. a value of \\(b_1\\)) as extreme as -7.1933 merely by chance, if there is really no relationship between price and acceleration time? Null Hypothesis: Among all 2015 cars, there is no relationship between price and acceleration time, and the slope we observed occurred merely by chance. Alternative Hypothesis: The slope we observed is due to more than chance, and there is a relationship between price and acceleration time among all 2015 cars. 3.3.8 Permutation Test for Slope Procedure: Randomly shuffle the acceleration times, so that any relationship between acceleration time and price is due only to chance. Fit a regression line to the shuffled data and record the slope of the regression line. Repeat steps 1 and 2 many (say 10,000) times, recording the slope (i.e. value of \\(b_1\\)) each time. Analyze the distribution of slopes, simulated under the assumption that there is no relationship between price and acceleration time. Look whether the actual slope we observed is consistent with the simulation results. 3.3.9 Five Permutations First Permutation ShuffledCars &lt;- Cars2015 ## create copy of dataset ShuffledCars$Acc060 &lt;- ShuffledCars$Acc060[sample(1:nrow(ShuffledCars))] Shuffle1df &lt;- data.frame(Cars2015$Make, Cars2015$Model, Cars2015$LowPrice, Cars2015$Acc060, ShuffledCars$Acc060) names(Shuffle1df) &lt;- c(&quot;Make&quot;, &quot;Model&quot;, &quot;LowPrice&quot;, &quot;Acc060&quot;, &quot;ShuffledAcc060&quot;) kable(head(Shuffle1df)) Make Model LowPrice Acc060 ShuffledAcc060 Chevrolet Spark 12.270 12.8 6.8 Hyundai Accent 14.745 10.3 10.1 Kia Rio 13.990 9.5 9.4 Mitsubishi Mirage 12.995 12.1 9.7 Nissan Versa Note 14.180 10.9 8.1 Dodge Dart 16.495 9.3 7.2 ggplot(data=Shuffle1df, aes(x=ShuffledAcc060, y=LowPrice)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) Slope of regression line from permuted data: M_Cars_Shuffle &lt;- lm(data=ShuffledCars, LowPrice~Acc060) summary(M_Cars_Shuffle)$coef[2] ## [1] 0.5801725 Second Permutation ShuffledCars &lt;- Cars2015 ## create copy of dataset ShuffledCars$Acc060 &lt;- ShuffledCars$Acc060[sample(1:nrow(ShuffledCars))] Shuffle1df &lt;- data.frame(Cars2015$Make, Cars2015$Model, Cars2015$LowPrice, Cars2015$Acc060, ShuffledCars$Acc060) names(Shuffle1df) &lt;- c(&quot;Make&quot;, &quot;Model&quot;, &quot;LowPrice&quot;, &quot;Acc060&quot;, &quot;ShuffledAcc060&quot;) kable(head(Shuffle1df)) Make Model LowPrice Acc060 ShuffledAcc060 Chevrolet Spark 12.270 12.8 6.9 Hyundai Accent 14.745 10.3 8.0 Kia Rio 13.990 9.5 10.5 Mitsubishi Mirage 12.995 12.1 7.2 Nissan Versa Note 14.180 10.9 8.4 Dodge Dart 16.495 9.3 11.0 ggplot(data=Shuffle1df, aes(x=ShuffledAcc060, y=LowPrice)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) Slope of regression line from permuted data: M_Cars_Shuffle &lt;- lm(data=ShuffledCars, LowPrice~Acc060) summary(M_Cars_Shuffle)$coef[2] ## [1] -0.9289392 Third Permutation ShuffledCars &lt;- Cars2015 ## create copy of dataset ShuffledCars$Acc060 &lt;- ShuffledCars$Acc060[sample(1:nrow(ShuffledCars))] Shuffle1df &lt;- data.frame(Cars2015$Make, Cars2015$Model, Cars2015$LowPrice, Cars2015$Acc060, ShuffledCars$Acc060) names(Shuffle1df) &lt;- c(&quot;Make&quot;, &quot;Model&quot;, &quot;LowPrice&quot;, &quot;Acc060&quot;, &quot;ShuffledAcc060&quot;) kable(head(Shuffle1df)) Make Model LowPrice Acc060 ShuffledAcc060 Chevrolet Spark 12.270 12.8 6.8 Hyundai Accent 14.745 10.3 8.7 Kia Rio 13.990 9.5 6.7 Mitsubishi Mirage 12.995 12.1 8.8 Nissan Versa Note 14.180 10.9 7.2 Dodge Dart 16.495 9.3 5.5 ggplot(data=Shuffle1df, aes(x=ShuffledAcc060, y=LowPrice)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) Slope of regression line from permuted data: M_Cars_Shuffle &lt;- lm(data=ShuffledCars, LowPrice~Acc060) summary(M_Cars_Shuffle)$coef[2] ## [1] -1.428531 3.3.10 Fourth Permutation ShuffledCars &lt;- Cars2015 ## create copy of dataset ShuffledCars$Acc060 &lt;- ShuffledCars$Acc060[sample(1:nrow(ShuffledCars))] Shuffle1df &lt;- data.frame(Cars2015$Make, Cars2015$Model, Cars2015$LowPrice, Cars2015$Acc060, ShuffledCars$Acc060) names(Shuffle1df) &lt;- c(&quot;Make&quot;, &quot;Model&quot;, &quot;LowPrice&quot;, &quot;Acc060&quot;, &quot;ShuffledAcc060&quot;) kable(head(Shuffle1df)) Make Model LowPrice Acc060 ShuffledAcc060 Chevrolet Spark 12.270 12.8 7.4 Hyundai Accent 14.745 10.3 7.0 Kia Rio 13.990 9.5 7.7 Mitsubishi Mirage 12.995 12.1 7.6 Nissan Versa Note 14.180 10.9 6.2 Dodge Dart 16.495 9.3 8.7 ggplot(data=Shuffle1df, aes(x=ShuffledAcc060, y=LowPrice)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) Slope of regression line from permuted data: M_Cars_Shuffle &lt;- lm(data=ShuffledCars, LowPrice~Acc060) summary(M_Cars_Shuffle)$coef[2] ## [1] 0.01197376 Fifth Permutation ShuffledCars &lt;- Cars2015 ## create copy of dataset ShuffledCars$Acc060 &lt;- ShuffledCars$Acc060[sample(1:nrow(ShuffledCars))] Shuffle1df &lt;- data.frame(Cars2015$Make, Cars2015$Model, Cars2015$LowPrice, Cars2015$Acc060, ShuffledCars$Acc060) names(Shuffle1df) &lt;- c(&quot;Make&quot;, &quot;Model&quot;, &quot;LowPrice&quot;, &quot;Acc060&quot;, &quot;ShuffledAcc060&quot;) kable(head(Shuffle1df)) Make Model LowPrice Acc060 ShuffledAcc060 Chevrolet Spark 12.270 12.8 8.8 Hyundai Accent 14.745 10.3 7.0 Kia Rio 13.990 9.5 7.5 Mitsubishi Mirage 12.995 12.1 7.8 Nissan Versa Note 14.180 10.9 8.1 Dodge Dart 16.495 9.3 7.9 ggplot(data=Shuffle1df, aes(x=ShuffledAcc060, y=LowPrice)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) Slope of regression line from permuted data: M_Cars_Shuffle &lt;- lm(data=ShuffledCars, LowPrice~Acc060) summary(M_Cars_Shuffle)$coef[2] ## [1] -0.4476822 3.3.11 R Code for Permutation Test b1 &lt;- Cars_M_A060$coef[2] ## record value of b1 from actual data ## perform simulation b1Sim &lt;- rep(NA, 10000) ## vector to hold results ShuffledCars &lt;- Cars2015 ## create copy of dataset for (i in 1:10000){ #randomly shuffle acceleration times ShuffledCars$Acc060 &lt;- ShuffledCars$Acc060[sample(1:nrow(ShuffledCars))] ShuffledCars_M&lt;- lm(data=ShuffledCars, LowPrice ~ Acc060) #fit model to shuffled data b1Sim[i] &lt;- ShuffledCars_M$coef[2] ## record b1 from shuffled model } Cars_A060SimulationResults &lt;- data.frame(b1Sim) #save results in dataframe 3.3.12 Permutation Test Results b1 &lt;- Cars_M_A060$coef[2] ## record value of b1 from actual data Cars_A060SimulationResultsPlot &lt;- ggplot(data=Cars_A060SimulationResults, aes(x=b1Sim)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + geom_vline(xintercept=c(b1, -1*b1), color=&quot;red&quot;) + xlab(&quot;Simulated Value of b1&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Distribution of b1 under assumption of no relationship&quot;) Cars_A060SimulationResultsPlot It is extremely unlikely that we would observe a value of \\(b_1\\) as extreme as -7.1933 by chance, if there is really no relationship between price and acceleration time. 3.3.13 P-value and Conclusion Proportion of simulations resulting in simulation value of \\(b_2\\) more extreme than -7.1933. mean(abs(b1Sim) &gt; abs(b1)) ## [1] 0 The p-value represents the probability of observing a slope as extreme or more extreme than -7.1933 by chance when there is actually no relationship between price and acceleration time. The probability of observing a slope as extreme as -7.1933 by chance, when there is no relationship between location and mercury level practically zero. There is very strong evidence of a relationship between price and acceleration time. A low p-value tells us only that there is evidence of a relationship, not that it is practically meaningful. But an estimated difference of more than $7 thousand for each additional second seems pretty important and would likely influence a buyer’s decision. 3.4 Test for Comparing Multiple Groups 3.4.1 Relationship Price and Car Size Continuing with the sample of 110 cars, seen in the previous section, let’s compare prices of small, midsized, and large cars. ggplot(data=Cars2015, aes(x=Size, y=LowPrice, fill=Size)) + geom_boxplot() + geom_jitter() + coord_flip() Cars2015 %&gt;% group_by(Size) %&gt;% summarize(MeanPrice = mean(LowPrice), StDevPrice=sd(LowPrice), N=n()) ## # A tibble: 3 × 4 ## Size MeanPrice StDevPrice N ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Large 42.3 17.9 29 ## 2 Midsized 33.2 12.0 34 ## 3 Small 26.7 14.4 47 3.4.2 Cars Questions of Interest Do the data provide evidence of a relationship between price and size of vehicle? Is there evidence of a difference in average price between… large and midsized cars? large and small cars? small and midsized cars? 3.4.3 Cars Price and Size Model Cars_M_Size = lm(data=Cars2015, LowPrice~Size) summary(Cars_M_Size) ## ## Call: ## lm(formula = LowPrice ~ Size, data = Cars2015) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.516 -11.190 -4.005 9.064 57.648 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.311 2.737 15.460 &lt; 0.0000000000000002 *** ## SizeMidsized -9.098 3.725 -2.442 0.0162 * ## SizeSmall -15.659 3.480 -4.499 0.0000174 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.74 on 107 degrees of freedom ## Multiple R-squared: 0.1593, Adjusted R-squared: 0.1436 ## F-statistic: 10.14 on 2 and 107 DF, p-value: 0.00009271 3.4.4 Relationship between Size and Price Do the data provide evidence of a relationship between price and size of vehicle? \\(\\widehat{\\text{Price}} = b_0 +b_1\\times\\text{I}_{\\text{Midsized}}+ b_2\\times\\text{I}_{\\text{Large}}\\) \\(b_0\\) represents expected price of large cars. \\(b_1\\) represents expected difference in price between large and midsized cars. \\(b_2\\) represents expected difference in price between large and small cars. Unfortunately, none of these measure whether there is an overall relationship between price and size. Question:What statistic can we use to assess the size of differences between more than two groups? 3.4.5 Test Statistic for Car Size and Price Cars_A_Size &lt;- aov(data=Cars2015, LowPrice~Size) summary(Cars_A_Size) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Size 2 4405 2202.7 10.14 0.0000927 *** ## Residuals 107 23242 217.2 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 3.4.6 Key Question in Car Size Investigation Null Hypothesis: Average price, among all 2015 cars, is the same between small, midsized, and large cars. Alternative Hypothesis: Average price among all 2015 cars differs between at least two of these sizes. Key Question: How likely is it that we would have obtained an F-statistic as extreme as 10.14 by chance, if there is really no difference in price between small, medium, and large sized cars, among all 2015 cars? 3.4.7 Simulation-Based Test for F-Statistic We’ll simulate situations where there is no relationship between size and price, and see how often we observe an F-statistic as extreme as 10.14. Procedure: Randomly shuffle the sizes of the vehicles, so that any relationship between size and price is due only to chance. Fit a model, using the shuffled data, with price as the response variable, and size as the explanatory variable. Record the F-statistic. Repeat steps 1 and 2 many (say 10,000) times, recording the F-statistic each time. Analyze the distribution of F-statistics, simulated under the assumption that there is no relationship between size and price. Look whether the actual F-statistic we observed is consistent with the simulation results. 3.4.8 Five Permutations First Permutation ShuffledCars &lt;- Cars2015 ## create copy of dataset ShuffledCars$Size &lt;- ShuffledCars$Size[sample(1:nrow(ShuffledCars))] Shuffle1df &lt;- data.frame(Cars2015$Make, Cars2015$Model, Cars2015$LowPrice, Cars2015$Size, ShuffledCars$Size) names(Shuffle1df) &lt;- c(&quot;Make&quot;, &quot;Model&quot;, &quot;LowPrice&quot;, &quot;Size&quot;, &quot;ShuffledSize&quot;) kable(head(Shuffle1df)) Make Model LowPrice Size ShuffledSize Chevrolet Spark 12.270 Small Small Hyundai Accent 14.745 Small Large Kia Rio 13.990 Small Large Mitsubishi Mirage 12.995 Small Midsized Nissan Versa Note 14.180 Small Midsized Dodge Dart 16.495 Small Small Recall this model was fit under an assumption of no relationship between price and size. ggplot(data=ShuffledCars, aes(x=Size, y=LowPrice, fill=Size)) + geom_boxplot() + geom_jitter() + coord_flip() + ggtitle(&quot;Shuffled Cars&quot;) Cars_A_Size_Shuffle &lt;- aov(data=ShuffledCars, LowPrice~Size) summary(Cars_A_Size_Shuffle) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Size 2 746 372.9 1.483 0.232 ## Residuals 107 26902 251.4 Second Permutation ShuffledCars &lt;- Cars2015 ## create copy of dataset ShuffledCars$Size &lt;- ShuffledCars$Size[sample(1:nrow(ShuffledCars))] Shuffle1df &lt;- data.frame(Cars2015$Make, Cars2015$Model, Cars2015$LowPrice, Cars2015$Size, ShuffledCars$Size) names(Shuffle1df) &lt;- c(&quot;Make&quot;, &quot;Model&quot;, &quot;LowPrice&quot;, &quot;Size&quot;, &quot;ShuffledSize&quot;) kable(head(Shuffle1df)) Make Model LowPrice Size ShuffledSize Chevrolet Spark 12.270 Small Large Hyundai Accent 14.745 Small Small Kia Rio 13.990 Small Large Mitsubishi Mirage 12.995 Small Large Nissan Versa Note 14.180 Small Midsized Dodge Dart 16.495 Small Large ggplot(data=ShuffledCars, aes(x=Size, y=LowPrice, fill=Size)) + geom_boxplot() + geom_jitter() + coord_flip() + ggtitle(&quot;Shuffled Cars&quot;) Cars_A_Size_Shuffle &lt;- aov(data=ShuffledCars, LowPrice~Size) summary(Cars_A_Size_Shuffle) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Size 2 688 344.1 1.366 0.26 ## Residuals 107 26960 252.0 Third Permutation ShuffledCars &lt;- Cars2015 ## create copy of dataset ShuffledCars$Size &lt;- ShuffledCars$Size[sample(1:nrow(ShuffledCars))] Shuffle1df &lt;- data.frame(Cars2015$Make, Cars2015$Model, Cars2015$LowPrice, Cars2015$Size, ShuffledCars$Size) names(Shuffle1df) &lt;- c(&quot;Make&quot;, &quot;Model&quot;, &quot;LowPrice&quot;, &quot;Size&quot;, &quot;ShuffledSize&quot;) kable(head(Shuffle1df)) Make Model LowPrice Size ShuffledSize Chevrolet Spark 12.270 Small Small Hyundai Accent 14.745 Small Large Kia Rio 13.990 Small Small Mitsubishi Mirage 12.995 Small Small Nissan Versa Note 14.180 Small Midsized Dodge Dart 16.495 Small Midsized ggplot(data=ShuffledCars, aes(x=Size, y=LowPrice, fill=Size)) + geom_boxplot() + geom_jitter() + coord_flip() + ggtitle(&quot;Shuffled Cars&quot;) Cars_A_Size_Shuffle &lt;- aov(data=ShuffledCars, LowPrice~Size) summary(Cars_A_Size_Shuffle) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Size 2 42 21.01 0.081 0.922 ## Residuals 107 27606 258.00 Fourth Permutation ShuffledCars &lt;- Cars2015 ## create copy of dataset ShuffledCars$Size &lt;- ShuffledCars$Size[sample(1:nrow(ShuffledCars))] Shuffle1df &lt;- data.frame(Cars2015$Make, Cars2015$Model, Cars2015$LowPrice, Cars2015$Size, ShuffledCars$Size) names(Shuffle1df) &lt;- c(&quot;Make&quot;, &quot;Model&quot;, &quot;LowPrice&quot;, &quot;Size&quot;, &quot;ShuffledSize&quot;) kable(head(Shuffle1df)) Make Model LowPrice Size ShuffledSize Chevrolet Spark 12.270 Small Small Hyundai Accent 14.745 Small Small Kia Rio 13.990 Small Midsized Mitsubishi Mirage 12.995 Small Small Nissan Versa Note 14.180 Small Small Dodge Dart 16.495 Small Midsized Recall this model was fit under an assumption of no relationship between price and size. ggplot(data=ShuffledCars, aes(x=Size, y=LowPrice, fill=Size)) + geom_boxplot() + geom_jitter() + coord_flip() + ggtitle(&quot;Shuffled Cars&quot;) Cars_A_Size_Shuffle &lt;- aov(data=ShuffledCars, LowPrice~Size) summary(Cars_A_Size_Shuffle) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Size 2 885 442.6 1.77 0.175 ## Residuals 107 26763 250.1 Fifth Permutation ShuffledCars &lt;- Cars2015 ## create copy of dataset ShuffledCars$Size &lt;- ShuffledCars$Size[sample(1:nrow(ShuffledCars))] Shuffle1df &lt;- data.frame(Cars2015$Make, Cars2015$Model, Cars2015$LowPrice, Cars2015$Size, ShuffledCars$Size) names(Shuffle1df) &lt;- c(&quot;Make&quot;, &quot;Model&quot;, &quot;LowPrice&quot;, &quot;Size&quot;, &quot;ShuffledSize&quot;) kable(head(Shuffle1df)) Make Model LowPrice Size ShuffledSize Chevrolet Spark 12.270 Small Midsized Hyundai Accent 14.745 Small Small Kia Rio 13.990 Small Small Mitsubishi Mirage 12.995 Small Midsized Nissan Versa Note 14.180 Small Midsized Dodge Dart 16.495 Small Large ggplot(data=ShuffledCars, aes(x=Size, y=LowPrice, fill=Size)) + geom_boxplot() + geom_jitter() + coord_flip() + ggtitle(&quot;Shuffled Cars&quot;) Cars_A_Size_Shuffle &lt;- aov(data=ShuffledCars, LowPrice~Size) summary(Cars_A_Size_Shuffle) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Size 2 745 372.3 1.481 0.232 ## Residuals 107 26903 251.4 3.4.9 R Code For Permutation Test We’ll simulate 10,000 permutations and record the F-statistic for each set of permuted data. Fstat &lt;- summary(Cars_M_Size)$fstatistic[1] ## record value of F-statistic from actual data ## perform simulation FSim &lt;- rep(NA, 10000) ## vector to hold results ShuffledCars &lt;- Cars2015 ## create copy of dataset for (i in 1:10000){ #randomly shuffle acceleration times ShuffledCars$Size &lt;- ShuffledCars$Size[sample(1:nrow(ShuffledCars))] ShuffledCars_M&lt;- lm(data=ShuffledCars, LowPrice ~ Size) #fit model to shuffled data FSim[i] &lt;- summary(ShuffledCars_M)$fstatistic[1] ## record F from shuffled model } CarSize_SimulationResults &lt;- data.frame(FSim) #save results in dataframe 3.4.10 F-statistic for Size Simulation Results CarSize_SimulationResults_Plot &lt;- ggplot(data=CarSize_SimulationResults, aes(x=FSim)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + geom_vline(xintercept=c(Fstat), color=&quot;red&quot;) + xlab(&quot;Simulated Value of F&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Distribution of F under assumption of no relationship&quot;) CarSize_SimulationResults_Plot p-value: mean(FSim &gt; Fstat) ## [1] 0 The p-value represents the probability of observing an F-statistic as extreme as 10.14 by chance, in samples of size 29, 34, and 47, if in fact there is no relationship between price and size of car. The data provide strong evidence of a relationship between price and size. 3.4.11 Differences Between Different Sizes Now that we have evidence that car price is related to size, we might want to know which sizes differ from each other. Is there evidence of a difference in average price between… a) large and midsized cars? b) large and small cars? c) small and midsized cars? 3.4.12 Regression Coefficients for Tests Between Sizes \\(\\widehat{\\text{Price}} = b_0 +b_1\\times\\text{I}_{\\text{Midsized}}+ b_2\\times\\text{I}_{\\text{Small}}\\) \\(b_0\\) represents expected price of large cars. \\(b_1\\) represents expected difference in price between large and midsized cars. \\(b_2\\) represents expected difference in price between large and small cars. Thus, we can answer each question by looking at the appropriate regression coefficient. a) large and midsized cars? (\\(b_1\\)) b) large and small cars? (\\(b_2\\)) c) small and midsized cars? (\\(b_1-b_2\\)) 3.4.13 Simulation for Differences between Types of Cars We’ll simulate situations where there is no relationship between size and price, and see how often we observe results for \\(b_1\\), \\(b_2\\), and \\(b_1-b_2\\) as extreme as we did in the actual data. Procedure: Randomly shuffle the sizes of the vehicles, so that any relationship between size and price is due only to chance. Fit a model, using the shuffled data, with price as the response variable, and size as the explanatory variable. Record the values of \\(b_1\\), \\(b_2\\), and \\(b_1-b_2\\). Repeat steps 1 and 2 many (say 10,000) times, recording the values of \\(b_1\\), \\(b_2\\), and \\(b_1-b_2\\) each time. Analyze the distribution of \\(b_1\\), \\(b_2\\), \\(b_1-b_2\\), simulated under the assumption that there is no relationship between size and price. Look whether the actual values we observed are consistent with the simulation results. 3.4.14 Code for Simulation-Based Test of Prices by Size b1 &lt;- Cars_M_Size$coefficients[2] #record b1 from actual data b2 &lt;- Cars_M_Size$coefficients[3] #record b2 from actual data ## perform simulation b1Sim &lt;- rep(NA, 10000) ## vector to hold results b2Sim &lt;- rep(NA, 10000) ## vector to hold results ShuffledCars &lt;- Cars2015 ## create copy of dataset for (i in 1:10000){ #randomly shuffle acceleration times ShuffledCars$Size &lt;- ShuffledCars$Size[sample(1:nrow(ShuffledCars))] ShuffledCars_M&lt;- lm(data=ShuffledCars, LowPrice ~ Size) #fit model to shuffled data b1Sim[i] &lt;- ShuffledCars_M$coefficients[2] ## record b1 from shuffled model b2Sim[i] &lt;- ShuffledCars_M$coefficients[3] ## record b2 from shuffled model } Cars_Size2_SimulationResults &lt;- data.frame(b1Sim, b2Sim) #save results in dataframe 3.4.15 Car Size Simulation-Based Results for \\(b_1\\) Cars_Size2_SimulationResultsPlot_b1 &lt;- ggplot(data=Cars_Size2_SimulationResults, aes(x=b1Sim)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + geom_vline(xintercept=c(b1, -1*b1), color=&quot;red&quot;) + xlab(&quot;Simulated Value of b1&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Large vs Midsize Cars: Distribution of b1 under assumption of no relationship&quot;) Cars_Size2_SimulationResultsPlot_b1 p-value: mean(abs(b1Sim)&gt;abs(b1)) ## [1] 0.0243 The p-value represents the probability of observing a difference in mean prices as extreme as 9.1 by chance, in samples of size 29 and 34 cars, if in fact there is no difference in average prices of large and midsized cars. 3.4.16 Car Size Simulation-Based Results for \\(b_2\\) Cars_Size2_SimulationResultsPlot_b2 &lt;- ggplot(data=Cars_Size2_SimulationResults, aes(x=b2Sim)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + geom_vline(xintercept=c(b2, -1*b2), color=&quot;red&quot;) + xlab(&quot;Simulated Value of b2&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Large vs Small Cars: Distribution of b2 under assumption of no relationship&quot;) Cars_Size2_SimulationResultsPlot_b2 p-value: mean(abs(b2Sim)&gt;abs(b2)) ## [1] 0 The p-value represents the probability of observing a difference in mean prices as extreme as 15.4 by chance, in samples of size 29 and 47 cars, if in fact there is no difference in average prices of large and small cars. 3.4.17 Car Size Simulation-Based Results for \\(b_1-b_2\\) Cars_Size2_SimulationResultsPlot_b1_b2 &lt;- ggplot(data=Cars_Size2_SimulationResults, aes(x=b1Sim-b2Sim)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + geom_vline(xintercept=c(b1-b2, -1*(b1-b2)), color=&quot;red&quot;) + xlab(&quot;Simulated Value of b1-b2&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Small vs Midsize Cars: Distribution of b1-b2 under assumption of no relationship&quot;) Cars_Size2_SimulationResultsPlot_b1_b2 p-value: mean(abs(b1Sim-b2Sim)&gt;abs(b1-b2)) ## [1] 0.0666 The p-value represents the probability of observing a difference in mean prices as extreme as 6.5 by chance, in samples of size 34 and 47 cars, if in fact there is no difference in average prices of midsized and small cars. 3.4.18 Bonferroni Correction We might normally conclude that there is evidence of differences in group means in the p-value is less than 0.05. However, since we are performing multiple tests simultaneously, there is an increased chance that at least one of them will yield a small p-value just by chance. Thus, we should be more strict in deciding what constitutes evidence against the null hypothesis. A commone rule (known as the Bonferroni correction) is to divide the values usually used as criteria for evidence by the number of tests. In this example: - Here, we would say there is some evidence of differences between groups if the p-value is less than 0.10/3=0.0333. - We would say there is strong evidence of differences if the p-value is less than 0.05/3=0.0167 Comparison Coefficient p-value Evidence of Difference large vs midsize \\(b_1\\) 0.0243 Some evidence large vs small \\(b_2\\) 0 Strong evidence small vs midsize \\(b_1-b_2\\) 0.0666 No evidence 3.4.19 Summary of Tests Between Multiple Groups When testing for differences between more than two groups: Perform an overall test, using the F-statistic. A large F-statistic and small p-value tell us there is evidence of differences between at least some of the groups. If the F-tests yields evidence evidence of differences, perform tests on individual model coefficients to determine which groups differ. Use a more strict cutoff criteria, such as the Bonferroni correction. 3.4.20 Bear Weights by Season ggplot(data=Bears_Subset, aes(y=Weight, x=Season, fill=Season)) + geom_boxplot() + geom_jitter() + coord_flip() 3.4.21 Bear Weights by Season Model Bears_M_Season &lt;- lm(data=Bears_Subset, Weight~Season) summary(Bears_M_Season) ## ## Call: ## lm(formula = Weight ~ Season, data = Bears_Subset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -178.84 -79.84 -29.02 54.98 309.16 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 204.84 17.16 11.939 &lt;0.0000000000000002 *** ## SeasonSpring -37.27 34.62 -1.076 0.284 ## SeasonSummer -29.81 24.71 -1.206 0.231 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 112.5 on 94 degrees of freedom ## Multiple R-squared: 0.02034, Adjusted R-squared: -0.0005074 ## F-statistic: 0.9757 on 2 and 94 DF, p-value: 0.3807 3.4.22 F-Statistic for Bear Weights by Season Bears_A_Season &lt;- aov(data=Bears_Subset, Weight~Season) summary(Bears_A_Season) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Season 2 24699 12350 0.976 0.381 ## Residuals 94 1189818 12658 3.4.23 Hypotheses for Bears Seasons F-Test Null Hypothesis: Among all bears of this type, mean weight is the same in each season. Alternative Hypothesis: Among all bears of this type, mean weight differs between at least two of the seasons. Key Question: What is the probability of observing an F-statistic as extreme as 0.976 if there is really no relationship between weight and season? 3.4.24 Simulation-Based Test for Bears F-Statistic We’ll simulate situations where there is no relationship between size and price, and see how often we observe an F-statistic as extreme as 0.976. Procedure: Randomly shuffle the seasons, so that any relationship between weight and season is due only to chance. Fit a model, using the shuffled data, with weight as the response variable, and season as the explanatory variable. Record the F-statistic. Repeat steps 1 and 2 many (say 10,000) times, recording the F-statistic each time. Analyze the distribution of F-statistics, simulated under the assumption that there is no relationship between season and weight Look whether the actual F-statistic we observed is consistent with the simulation results. 3.4.25 Bears F-Statistic Simulation Fstat &lt;- summary(Bears_M_Season)$fstatistic[1] ## record value of F-statistic from actual data ## perform simulation FSim &lt;- rep(NA, 10000) ## vector to hold results ShuffledBears &lt;- Bears_Subset ## create copy of dataset for (i in 1:10000){ #randomly shuffle acceleration times ShuffledBears$Season &lt;- ShuffledBears$Season[sample(1:nrow(ShuffledBears))] ShuffledBears_M&lt;- lm(data=ShuffledBears, Weight ~ Season) #fit model to shuffled data FSim[i] &lt;- summary(ShuffledBears_M)$fstatistic[1] ## record F from shuffled model } Bears_Seasons_SimulationResults &lt;- data.frame(FSim) #save results in dataframe 3.4.26 F-statistic for Bears Season Simulation Bears_Seasons_SimulationResultsPlot &lt;- ggplot(data=Bears_Seasons_SimulationResults, aes(x=FSim)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + geom_vline(xintercept=c(Fstat), color=&quot;red&quot;) + xlab(&quot;Simulated Value of F&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Distribution of F under assumption of no relationship&quot;) Bears_Seasons_SimulationResultsPlot mean(FSim &gt; Fstat) ## [1] 0.3762 The p-value represents the probability of observing and F-statistic as extreme as 0.976 by chance, in a sample of 97, if in fact there is no difference in average weights of bears between seasons. It is not at all unusual to observe an F-statistic as extreme or more extreme than we did if there is really no relationship between weight and season. There is no evidence that average bear weights differ between seasons. 3.4.27 Don’t Accept Null Hypothesis In the previous example, we concluded that there is no evidence that average bear weights differ between seasons. This is different than saying that bear weights are the same in each season. Why would it be inappropriate to say this? 3.4.28 Comparison of Weights by Season Bears_Season_Table &lt;- Bears_Subset %&gt;% group_by(Season) %&gt;% summarize(MeanWeight = mean(Weight), StDevWeight = sd(Weight), N=n()) kable(Bears_Season_Table) Season MeanWeight StDevWeight N Fall 204.8372 125.71414 43 Spring 167.5714 108.74155 14 Summer 175.0250 97.70796 40 3.4.29 Don’t Accept Null Hypothesis (Cont.) The data do show differences in average weight between seasons. It’s just that we can’t rule out the possibility that these differences are due to chance alone. A hypothesis test can only tell us the strength of evidence against the null hypothesis. The absence of evidence against the null hypothesis should not be interpreted as evidence for the null hypothesis. We should never say that the data support/prove/confirm the null hypothesis. We can only say that the data do not provide evidence against the null hypothesis. "],["bootstrap-confindence-intervals.html", "Chapter 4 Bootstrap Confindence Intervals 4.1 Quantifying Sampling Variability 4.2 Bootstrapping Other Statistics 4.3 Difference in Sample Means 4.4 Bootstrapping Regression Coefficients 4.5 Bootstrapping Coefficients in Multiple Regression 4.6 Bootstrap Standard Error Confidence Intervals 4.7 Standard Error Formulas", " Chapter 4 Bootstrap Confindence Intervals Learning Outcomes: Explain how to obtain a bootstrap distribution for a statistic (such as a sample mean, median, standard deviation, proportion, difference in means or proportions, or regression coefficient). Explain the purpose of bootstrapping. Interpret confidence intervals, or explain why it is inappropriate to do so. Explain whether or not the results of a confidence interval are. consistent with the conclusion of a hypothesis test. Define standard error of a statistic and interpret it in context. Explain how sample size and level of confidence impact the width of a confidence interval. Explain how sample size impacts variability in individual observations, and the sampling distribution for a test statistic. 4.1 Quantifying Sampling Variability 4.1.1 Distribution of Mercury Levels in Florida Lakes Recall the example of mercury levels in a sample of 53 Florida Lakes, seen in the previous chapter. Lakes_Hist &lt;- ggplot(data=FloridaLakes, aes(x=AvgMercury)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;, binwidth = 0.2) + ggtitle(&quot;Mercury Levels in Sample of Florida Lakes&quot;) + xlab(&quot;Mercury Level&quot;) + ylab(&quot;Frequency&quot;) Lakes_Hist The table shows mean mercury level, median mercury level, standard deviation in mercury level between lakes, proportion of lakes with mercury level above 1 ppm Lakes_Stats &lt;- FloridaLakes %&gt;% summarize(MeanHg = mean(AvgMercury), MedianHg = median(AvgMercury), StDevHG = sd(AvgMercury), PropOver1 = mean(AvgMercury&gt;1), N=n()) Lakes_Stats ## # A tibble: 1 × 5 ## MeanHg MedianHg StDevHG PropOver1 N ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 0.527 0.48 0.341 0.113 53 If Mercury accumulation exceeds 0.5 ppm, then there are environmental concerns. In fact, the legal safety limit in Canada is 0.5 ppm, although it is 1 ppm in the United States. Since the mean mercury level is 0.527 ppm, this might be some cause for concern. Then again, this was based on just a sample of 53 lakes. We would expect mean mercury level to vary from one sample of 53 to the next, so we would not expect the mean of this sample to exactly match the mean mercury level among all Florida lakes. It would be helpful to know how might the mean mercury level could plausibly vary from one sample of size 53 to the next. We could use this information to determine a reasonable range for the mean mercury level of all Florida lakes. If we had lots of different samples of 53 lakes, we could calculate the mean of each sample, and then see how much these means differ from each other (i.e. calculate the standard deviation of the means). Unfortunately, we don’t have lots of samples of size 53, we only have one sample. 4.1.2 Simulation to Quantify Variability Somehow, we need to use information from just this one sample to estimate how much variability could plausibly occur between the mean of many different samples of 53 lakes. Idea: Let’s assume our sample is representative of all Florida lakes. Then, we’ll duplicate it many times to create a large set, that will look like the population of all Florida Lakes. We can then draw samples of 53 from that large population, and record the mean mercury level for each sample of 53. In fact, duplicating the sample many times and selecting new samples of 53 has the same effect as drawing samples of size 53 from the original sample, by putting the lake drawn back in each time. (i.e. sampling with replacement) This means that in each new sample, some lakes will be drawn multiple times and others not at all. It also ensures that each sample is different, allowing us to estimate variability in the sample mean between the different samples of size 53. The variability in sample means in our newly drawn samples is used to approximate the variability in sample means that would occur between different samples of 53 lakes, drawn from the population of all Florida Lakes. 4.1.3 Bootstrap Sampling The approach of sampling from the original sample, described on the previous page is called bootstrap sampling. The new samples drawn are referred to as bootstrap samples. Take a bootstrap sample of size 53, by sampling lakes with replacement. Calculate the mean mercury concentration in the bootstrap sample. Repeat steps 1 and 2 many (say 10,000) times, keeping track of the mean mercury concentrations in each bootstrap sample. Look at the distribution of mean concentrations from the bootstrap samples. The variability in this distribution can be used to approximate the variability in the sampling distribution for the sample mean. 4.1.4 Original Sample print.data.frame(data.frame(FloridaLakes %&gt;% select(Lake, AvgMercury)), row.names = FALSE) ## Lake AvgMercury ## Alligator 1.23 ## Annie 1.33 ## Apopka 0.04 ## Blue Cypress 0.44 ## Brick 1.20 ## Bryant 0.27 ## Cherry 0.48 ## Crescent 0.19 ## Deer Point 0.83 ## Dias 0.81 ## Dorr 0.71 ## Down 0.50 ## Eaton 0.49 ## East Tohopekaliga 1.16 ## Farm-13 0.05 ## George 0.15 ## Griffin 0.19 ## Harney 0.77 ## Hart 1.08 ## Hatchineha 0.98 ## Iamonia 0.63 ## Istokpoga 0.56 ## Jackson 0.41 ## Josephine 0.73 ## Kingsley 0.34 ## Kissimmee 0.59 ## Lochloosa 0.34 ## Louisa 0.84 ## Miccasukee 0.50 ## Minneola 0.34 ## Monroe 0.28 ## Newmans 0.34 ## Ocean Pond 0.87 ## Ocheese Pond 0.56 ## Okeechobee 0.17 ## Orange 0.18 ## Panasoffkee 0.19 ## Parker 0.04 ## Placid 0.49 ## Puzzle 1.10 ## Rodman 0.16 ## Rousseau 0.10 ## Sampson 0.48 ## Shipp 0.21 ## Talquin 0.86 ## Tarpon 0.52 ## Tohopekaliga 0.65 ## Trafford 0.27 ## Trout 0.94 ## Tsala Apopka 0.40 ## Weir 0.43 ## Wildcat 0.25 ## Yale 0.27 Sample Mean: mean(FloridaLakes$AvgMercury) ## [1] 0.5271698 4.1.5 Five Bootstrap Samples in R The sample_n() function samples the specified number rows from a data frame, with or without replacement. Bootstrap Sample 1 BootstrapSample1 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake) print.data.frame(data.frame(BootstrapSample1%&gt;% select(Lake, AvgMercury)), row.names = FALSE) ## Lake AvgMercury ## Annie 1.33 ## Apopka 0.04 ## Blue Cypress 0.44 ## Bryant 0.27 ## Deer Point 0.83 ## Deer Point 0.83 ## Dias 0.81 ## Dorr 0.71 ## Down 0.50 ## Down 0.50 ## Down 0.50 ## Down 0.50 ## Eaton 0.49 ## Farm-13 0.05 ## George 0.15 ## George 0.15 ## Harney 0.77 ## Harney 0.77 ## Hart 1.08 ## Hart 1.08 ## Hatchineha 0.98 ## Hatchineha 0.98 ## Istokpoga 0.56 ## Istokpoga 0.56 ## Istokpoga 0.56 ## Josephine 0.73 ## Kingsley 0.34 ## Kingsley 0.34 ## Kissimmee 0.59 ## Kissimmee 0.59 ## Miccasukee 0.50 ## Miccasukee 0.50 ## Newmans 0.34 ## Newmans 0.34 ## Orange 0.18 ## Panasoffkee 0.19 ## Placid 0.49 ## Puzzle 1.10 ## Sampson 0.48 ## Sampson 0.48 ## Talquin 0.86 ## Trafford 0.27 ## Trout 0.94 ## Weir 0.43 ## Weir 0.43 ## Weir 0.43 ## Wildcat 0.25 ## Wildcat 0.25 ## Yale 0.27 ## Yale 0.27 ## Yale 0.27 ## Yale 0.27 ## Yale 0.27 Sample Mean: mean(BootstrapSample1$AvgMercury) ## [1] 0.525283 Bootstrap Sample 2 BootstrapSample2 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake) print.data.frame(data.frame(BootstrapSample2%&gt;% select(Lake, AvgMercury)), row.names = FALSE) ## Lake AvgMercury ## Alligator 1.23 ## Apopka 0.04 ## Blue Cypress 0.44 ## Blue Cypress 0.44 ## Brick 1.20 ## Cherry 0.48 ## Crescent 0.19 ## Crescent 0.19 ## Crescent 0.19 ## Crescent 0.19 ## Dorr 0.71 ## Dorr 0.71 ## East Tohopekaliga 1.16 ## Eaton 0.49 ## Farm-13 0.05 ## George 0.15 ## Harney 0.77 ## Harney 0.77 ## Hart 1.08 ## Hatchineha 0.98 ## Hatchineha 0.98 ## Istokpoga 0.56 ## Jackson 0.41 ## Josephine 0.73 ## Kingsley 0.34 ## Kissimmee 0.59 ## Lochloosa 0.34 ## Lochloosa 0.34 ## Louisa 0.84 ## Miccasukee 0.50 ## Minneola 0.34 ## Monroe 0.28 ## Newmans 0.34 ## Ocean Pond 0.87 ## Okeechobee 0.17 ## Orange 0.18 ## Panasoffkee 0.19 ## Parker 0.04 ## Placid 0.49 ## Placid 0.49 ## Rodman 0.16 ## Shipp 0.21 ## Shipp 0.21 ## Talquin 0.86 ## Talquin 0.86 ## Tarpon 0.52 ## Trafford 0.27 ## Trout 0.94 ## Tsala Apopka 0.40 ## Weir 0.43 ## Weir 0.43 ## Wildcat 0.25 ## Yale 0.27 mean(BootstrapSample2$AvgMercury) ## [1] 0.4960377 Bootstrap Sample 3 BootstrapSample3 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake) print.data.frame(data.frame(BootstrapSample3%&gt;% select(Lake, AvgMercury)), row.names = FALSE) ## Lake AvgMercury ## Brick 1.20 ## Brick 1.20 ## Bryant 0.27 ## Bryant 0.27 ## Cherry 0.48 ## Crescent 0.19 ## Crescent 0.19 ## Deer Point 0.83 ## Deer Point 0.83 ## Dias 0.81 ## Dorr 0.71 ## Dorr 0.71 ## Down 0.50 ## Down 0.50 ## Farm-13 0.05 ## George 0.15 ## Griffin 0.19 ## Griffin 0.19 ## Harney 0.77 ## Hart 1.08 ## Hart 1.08 ## Hatchineha 0.98 ## Iamonia 0.63 ## Iamonia 0.63 ## Istokpoga 0.56 ## Istokpoga 0.56 ## Istokpoga 0.56 ## Jackson 0.41 ## Josephine 0.73 ## Josephine 0.73 ## Kingsley 0.34 ## Kingsley 0.34 ## Miccasukee 0.50 ## Minneola 0.34 ## Minneola 0.34 ## Newmans 0.34 ## Okeechobee 0.17 ## Orange 0.18 ## Orange 0.18 ## Parker 0.04 ## Sampson 0.48 ## Sampson 0.48 ## Shipp 0.21 ## Shipp 0.21 ## Shipp 0.21 ## Talquin 0.86 ## Talquin 0.86 ## Tarpon 0.52 ## Tohopekaliga 0.65 ## Tsala Apopka 0.40 ## Weir 0.43 ## Wildcat 0.25 ## Yale 0.27 mean(BootstrapSample3$AvgMercury) ## [1] 0.5016981 Bootstrap Sample 4 BootstrapSample4 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake) print.data.frame(data.frame(BootstrapSample4%&gt;% select(Lake, AvgMercury)), row.names = FALSE) ## Lake AvgMercury ## Annie 1.33 ## Blue Cypress 0.44 ## Blue Cypress 0.44 ## Brick 1.20 ## Brick 1.20 ## Bryant 0.27 ## Dias 0.81 ## Dias 0.81 ## Dorr 0.71 ## Dorr 0.71 ## East Tohopekaliga 1.16 ## Eaton 0.49 ## Farm-13 0.05 ## George 0.15 ## Harney 0.77 ## Harney 0.77 ## Hart 1.08 ## Hatchineha 0.98 ## Jackson 0.41 ## Jackson 0.41 ## Jackson 0.41 ## Jackson 0.41 ## Josephine 0.73 ## Kingsley 0.34 ## Kissimmee 0.59 ## Kissimmee 0.59 ## Lochloosa 0.34 ## Lochloosa 0.34 ## Louisa 0.84 ## Louisa 0.84 ## Louisa 0.84 ## Miccasukee 0.50 ## Minneola 0.34 ## Minneola 0.34 ## Monroe 0.28 ## Newmans 0.34 ## Newmans 0.34 ## Ocean Pond 0.87 ## Ocheese Pond 0.56 ## Okeechobee 0.17 ## Okeechobee 0.17 ## Parker 0.04 ## Parker 0.04 ## Rodman 0.16 ## Sampson 0.48 ## Shipp 0.21 ## Talquin 0.86 ## Tarpon 0.52 ## Tarpon 0.52 ## Trout 0.94 ## Trout 0.94 ## Tsala Apopka 0.40 ## Tsala Apopka 0.40 mean(BootstrapSample4$AvgMercury) ## [1] 0.5637736 Bootstrap Sample 5 BootstrapSample5 &lt;- sample_n(FloridaLakes, 53, replace=TRUE) %&gt;% arrange(Lake) print.data.frame(data.frame(BootstrapSample5%&gt;% select(Lake, AvgMercury)), row.names = FALSE) ## Lake AvgMercury ## Alligator 1.23 ## Alligator 1.23 ## Apopka 0.04 ## Brick 1.20 ## Bryant 0.27 ## Cherry 0.48 ## Cherry 0.48 ## Crescent 0.19 ## Crescent 0.19 ## Deer Point 0.83 ## Deer Point 0.83 ## Deer Point 0.83 ## Deer Point 0.83 ## Dias 0.81 ## Dorr 0.71 ## Dorr 0.71 ## Down 0.50 ## East Tohopekaliga 1.16 ## Eaton 0.49 ## Griffin 0.19 ## Iamonia 0.63 ## Istokpoga 0.56 ## Istokpoga 0.56 ## Jackson 0.41 ## Jackson 0.41 ## Jackson 0.41 ## Kingsley 0.34 ## Lochloosa 0.34 ## Lochloosa 0.34 ## Lochloosa 0.34 ## Miccasukee 0.50 ## Miccasukee 0.50 ## Minneola 0.34 ## Minneola 0.34 ## Monroe 0.28 ## Monroe 0.28 ## Newmans 0.34 ## Orange 0.18 ## Orange 0.18 ## Panasoffkee 0.19 ## Panasoffkee 0.19 ## Parker 0.04 ## Placid 0.49 ## Puzzle 1.10 ## Puzzle 1.10 ## Rodman 0.16 ## Rodman 0.16 ## Rousseau 0.10 ## Sampson 0.48 ## Tohopekaliga 0.65 ## Trafford 0.27 ## Weir 0.43 ## Wildcat 0.25 mean(BootstrapSample5$AvgMercury) ## [1] 0.4922642 4.1.6 Code for Bootstrap for Sample Mean Now, we’ll take 10,000 bootstrap samples, and record the mean mercury concentration in each sample. MeanHg &lt;- rep(NA, 10000) for (i in 1:10000){ BootstrapSample &lt;- sample_n(FloridaLakes, 53, replace=TRUE) MeanHg[i] &lt;- mean(BootstrapSample$AvgMercury) } Lakes_Bootstrap_Results_Mean &lt;- data.frame(MeanHg) 4.1.7 Bootstrap Distribution for Sample Mean Lakes_Bootstrap_Mean &lt;- ggplot(data=Lakes_Bootstrap_Results_Mean, aes(x=MeanHg)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;Mean Mercury in Bootstrap Sample &quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Bootstrap Distribution for Sample Mean in Florida Lakes&quot;) + theme(legend.position = &quot;none&quot;) Lakes_Bootstrap_Mean We’ll calculate the 0.025 and 0.975 quantiles of this distribution, that is, the values between which the middle 95% of the bootstrap sample means lie. q.025 &lt;- quantile(Lakes_Bootstrap_Results_Mean$MeanHg, 0.025) q.975 &lt;- quantile(Lakes_Bootstrap_Results_Mean$MeanHg, 0.975) c(q.025, q.975) ## 2.5% 97.5% ## 0.4379198 0.6179245 Lakes_Bootstrap_Mean + geom_segment(aes(x=q.025,xend=q.975, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) + geom_text(label=&quot;Middle 95%&quot;, x=(q.025 + q.975)/2 , y=50) We are 95% confident that the mean mercury level is all Florida Lakes is between 0.44 and 0.62 ppm. 4.1.8 Confidence Intervals We call the above interval a 95% bootstrap percentile confidence interval for the mean mercury level of all Florida Lakes. A confidence interval provides a reasonable range in which a quantity that pertains to the population could lie, based on data from a sample. The quantity we are estimating, which pertains to the entire population is called a parameter. In this case, the parameter of interest is the mean mercury level of all Florida lakes, and the data is our sample of 53 lakes. When we estimate a population parameter, using a statistic from a sample, we need to provide a confidence interval to account for variability in the statistic from one sample to the next. 4.1.9 What Bootstrapping Does and Doesn’t Do The purpose of bootstrapping is to quantify the amount of uncertainty associated with a statistic that was calculated from a sample. A common misperception is that bootstrapping somehow increases the size of a sample by creating copies (or sampling with replacement). This is wrong!!!! Bootstrap samples are obtained by sampling from the original data, so they contain no new information and do not increase sample size. They simply help us understand how much our sample result could reasonably differ from that of the full population. 4.2 Bootstrapping Other Statistics 4.2.1 General Bootstrapping Procedure The bootstrap procedure can be applied to quantify uncertainty associated with a wide range of statistics (for example, sample proportions, means, medians, standard deviations, regression coefficients, F-statistics, etc.) Given a statistic that was calculated from a sample… Procedure: Take a sample of the same size as the original sample, by sampling cases from the original sample, with replacement. Calculate the statistic of interest in the bootstrap sample. Repeat steps 1 and 2 many (say 10,000) times, keeping track of the statistic calculated in each bootstrap sample. Look at the distribution of statistics calculated from the bootstrap samples. The variability in this distribution can be used to approximate the variability in the sampling distribution for the statistic of interest. We might be interested in quantities other than mean mercury level for the lakes. For example: standard deviation in mercury level sd(FloridaLakes$AvgMercury) ## [1] 0.3410356 percentage of lakes with mercury level exceeding 1 ppm mean(FloridaLakes$AvgMercury&gt;1) ## [1] 0.1132075 Note: in R, taking the mean of a logical variable returns the proportion of this that the condition is TRUE. median mercury level median(FloridaLakes$AvgMercury) ## [1] 0.48 4.2.2 Bootstrapping for Other Quantities We can also use bootstrapping to obtain confidence intervals for the median and standard deviation in mercury levels in Florida lakes. StDevHg &lt;- rep(NA, 10000) PropOver1 &lt;- rep(NA, 10000) MedianHg &lt;- rep(NA, 10000) for (i in 1:10000){ BootstrapSample &lt;- sample_n(FloridaLakes, 53, replace=TRUE) StDevHg[i] &lt;- sd(BootstrapSample$AvgMercury) PropOver1[i] &lt;- mean(BootstrapSample$AvgMercury&gt;1) MedianHg[i] &lt;- median(BootstrapSample$AvgMercury) } Lakes_Bootstrap_Results_Other &lt;- data.frame(MedianHg, PropOver1, StDevHg) 4.2.3 Lakes Bootstrap Percentile CI for St. Dev. q.025 &lt;- quantile(Lakes_Bootstrap_Results_Other$StDevHg, 0.025) q.975 &lt;- quantile(Lakes_Bootstrap_Results_Other$StDevHg, 0.975) c(q.025, q.975) ## 2.5% 97.5% ## 0.2778717 0.3910469 Lakes_Bootstrap_SD &lt;- ggplot(data=Lakes_Bootstrap_Results_Other, aes(x=StDevHg)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;Standard Devation in Bootstrap Sample &quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Bootstrap Distribution for Standard Deviation in Florida Lakes&quot;) Lakes_Bootstrap_SD + geom_segment(aes(x=q.025,xend=q.975, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We are 95% confident that the standard deviation in mercury level for all Florida Lakes is between 0.28 and 0.39 ppm. 4.2.4 Bootstrap Percentile CI for Prop. &gt; 1 ppm q.025 &lt;- quantile(Lakes_Bootstrap_Results_Other$PropOver1, 0.025) q.975 &lt;- quantile(Lakes_Bootstrap_Results_Other$PropOver1, 0.975) c(q.025, q.975) ## 2.5% 97.5% ## 0.03773585 0.20754717 Lakes_Bootstrap_PropOver1 &lt;- ggplot(data=Lakes_Bootstrap_Results_Other, aes(x=PropOver1)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;Proportion of Lakes over 1 ppm &quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Bootstrap Distribution for Prop. Lakes &gt; 1.0 ppm&quot;) Lakes_Bootstrap_PropOver1 + geom_segment(aes(x=q.025,xend=q.975, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We are 95% confident that the proportion of all Florida lakes with mercury level over 1 ppm is between 0.04 and 0.21. 4.2.5 Bootstrap Percentile CI for Median q.025 &lt;- quantile(Lakes_Bootstrap_Results_Other$MedianHg, 0.025) q.975 &lt;- quantile(Lakes_Bootstrap_Results_Other$MedianHg, 0.975) Lakes_Bootstrap_Median &lt;- ggplot(data=Lakes_Bootstrap_Results_Other, aes(x=MedianHg)) + geom_histogram(fill=&quot;lightblue&quot;)+ xlab(&quot;Median Mercury in Bootstrap Sample &quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Bootstrap Distribution for Sample Median in Florida Lakes&quot;) Lakes_Bootstrap_Median + geom_segment(aes(x=q.025,xend=q.975, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We should not draw conclusions from this bootstrap distribution. The bootstrap is unreliable when we see the same values coming up repeatedly in clusters, with large gaps in between. This can be an issue for statistics that are a single value from the dataset (for example median) 4.2.6 When Gaps are/ aren’t OK Sometimes, ggplot() shows gaps in a histogram, due mainly to binwidth. If the points seems to follow a fairly smooth trend (such as for prop &gt; 1), then bootstrapping is ok. If there are large clusters and gaps (such as for median), bootstrapping is inadvisable. Jitter plots can help us look for clusters and gaps. V1 &lt;- ggplot(data=Lakes_Bootstrap_Results_Other, aes(y=PropOver1, x=1)) + geom_jitter() V2 &lt;- ggplot(data=Lakes_Bootstrap_Results_Other, aes(y=MedianHg, x=1)) + geom_jitter() grid.arrange(V1, V2, ncol=2) 4.2.7 Changing Binwidth in Histogram Sometimes, default settings in geom_histogram() lead to less that optimal graphs. ( For example, oddly-placed gaps that do not accurately represent the shape of the data) When a histogram shows undesired gaps, that are not really indivative of large gaps in the data, we can sometimes get rid of them by adjusting the binwidth. Before you do this, explore the data, such as through jitter plots. Do not change binwidth to intentionally manipulate or hide undesirable information. Your goal should be to find a plot that accurately displays the shape/trend in the data. ggplot(data=Lakes_Bootstrap_Results_Other, aes(x=PropOver1)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;, binwidth=0.02) + xlab(&quot;Proportion of Lakes over 1 ppm &quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Bootstrap Distribution for Prop. Lakes &gt; 1.0 ppm&quot;) 4.3 Difference in Sample Means 4.3.1 Mercury Levels in Northern vs Southern Florida Lakes We previously used a permutation test and determined that there was strong evidence the the average mercury level for lakes in Southern Florida is higher than the average mercury level for lakes in Northern Florida. library(Lock5Data) data(FloridaLakes) #Location relative to rt. 50 FloridaLakes$Location &lt;- as.factor(c(&quot;S&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;S&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;S&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;N&quot;,&quot;S&quot;,&quot;S&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;S&quot;,&quot;S&quot;,&quot;S&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;)) head(FloridaLakes %&gt;% select(Lake, AvgMercury, Location)) ## # A tibble: 6 × 3 ## Lake AvgMercury Location ## &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 Alligator 1.23 S ## 2 Annie 1.33 S ## 3 Apopka 0.04 N ## 4 Blue Cypress 0.44 S ## 5 Brick 1.2 S ## 6 Bryant 0.27 N LakesBP &lt;- ggplot(data=FloridaLakes, aes(x=Location, y=AvgMercury, fill=Location)) + geom_boxplot() + geom_jitter() + ggtitle(&quot;Mercury Levels in Florida Lakes&quot;) + xlab(&quot;Location&quot;) + ylab(&quot;Mercury Level&quot;) + theme(axis.text.x = element_text(angle = 90)) + coord_flip() LakesBP LakesTable &lt;- FloridaLakes %&gt;% group_by(Location) %&gt;% summarize(MeanHg=mean(AvgMercury), StDevHg=sd(AvgMercury), N=n()) LakesTable ## # A tibble: 2 × 4 ## Location MeanHg StDevHg N ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 N 0.425 0.270 33 ## 2 S 0.696 0.384 20 In our sample of 33 Northern Lakes and 20 Southern Lakes, we saw a difference of 0.27 ppm. We might want to estimate how big or small this difference could be among all Florida lakes. 4.3.2 Model for Northern and Southern Lakes \\(\\widehat{\\text{Hg}} = b_0 +b_1\\text{I}_{\\text{South}}\\) \\(b_0\\) represents the mean mercury level for lakes in North Florida, and \\(b_1\\) represents the mean difference in mercury level for lakes in South Florida, compared to North Florida 4.3.3 Model for Lakes R Output Lakes_M &lt;- lm(data=FloridaLakes, AvgMercury ~ Location) summary(Lakes_M) ## ## Call: ## lm(formula = AvgMercury ~ Location, data = FloridaLakes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.65650 -0.23455 -0.08455 0.24350 0.67545 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.42455 0.05519 7.692 0.000000000441 *** ## LocationS 0.27195 0.08985 3.027 0.00387 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3171 on 51 degrees of freedom ## Multiple R-squared: 0.1523, Adjusted R-squared: 0.1357 ## F-statistic: 9.162 on 1 and 51 DF, p-value: 0.003868 4.3.4 Bootstrapping for Northern vs Southern Lakes Bootstrapping Procedure Take bootstrap samples of 33 northern Lakes, and 20 southern Lakes, by sampling with replacement. Fit a model and record regression coefficient \\(b_1\\), which represents the difference in mean mercury levels between the samples. Repeat steps 1 and 2 10,000 times, keeping track of the regression coefficient estimates in each bootstrap sample. Look at the distribution of regression coefficients in the bootstrap samples. The variability in this distribution can be used to approximate the variability in the sampling distributions for the \\(b_1\\). 4.3.5 Code for Bootstrapping for N vs S Lakes b1 &lt;- rep(NA, 10000) #vector to store b1 values for (i in 1:10000){ NLakes &lt;- sample_n(FloridaLakes %&gt;% filter(Location==&quot;N&quot;), 33, replace=TRUE) ## sample 33 northern lakes SLakes &lt;- sample_n(FloridaLakes %&gt;% filter(Location==&quot;S&quot;), 20, replace=TRUE) ## sample 20 southern lakes BootstrapSample &lt;- rbind(NLakes, SLakes) ## combine Northern and Southern Lakes M &lt;- lm(data=BootstrapSample, AvgMercury ~ Location) ## fit linear model b1[i] &lt;- coef(M)[2] ## record b1 } NS_Lakes_Bootstrap_Results &lt;- data.frame(b1) #save results as dataframe 4.3.6 Lakes: Bootstrap Percentile CI for Avg. Diff. q.025 &lt;- quantile(NS_Lakes_Bootstrap_Results$b1, 0.025) q.975 &lt;- quantile(NS_Lakes_Bootstrap_Results$b1, 0.975) c(q.025, q.975) ## 2.5% 97.5% ## 0.08475265 0.46174886 NS_Lakes_Bootstrap_Plot_b1 &lt;- ggplot(data=NS_Lakes_Bootstrap_Results, aes(x=b1)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;b1 in Bootstrap Sample&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Northern vs Southern Lakes: Bootstrap Distribution for b1&quot;) NS_Lakes_Bootstrap_Plot_b1 + geom_segment(aes(x=q.025,xend=q.975, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We are 95% confident the average mercury level in Southern Lakes is between 0.08 and 0.46 ppm higher than in Northern Florida. Question: We previously performed a hypothesis test and concluded that there was evidence that mean mercury level was higher for lakes in South Florida than Northern Florida. Is this confidence interval consistent with the result of the hypothesis test? Why or why not? 4.4 Bootstrapping Regression Coefficients 4.4.1 Bootstrapping Regression Slope for All Cars Since \\(b_0\\) and \\(b_1\\) were calculated from a sample of 110 new 2015 cars, we do not expect them to be exactly the same as what we would have obtained if we had data on all 2015 new cars. We’ll use bootstrapping to get a sense for how much variability is associated with the slope and intercept of the regression line. Bootstrapping Procedure Take a bootstrap sample of size 110, by sampling cars with replacement. Fit a linear regression model to the bootstrap sample with price as the response variable, and Acc060 as the explanatory variable. Record \\(b_0\\) and \\(b_1\\). Repeat steps 1 and 2 10,000 times, keeping track of the values on \\(b_0\\) and \\(b_1\\) in each bootstrap sample. Look at the distribution of \\(b_0\\) and \\(b_1\\) from the bootstrap samples. The variability in these distributions can be used to approximate the variability in the sampling distribution for the intercept and slope. 4.4.2 Code for Cars Regression Bootstrap b0 &lt;- rep(NA, 10000) b1 &lt;- rep(NA, 10000) for (i in 1:10000){ BootstrapSample &lt;- sample_n(Cars2015, 110, replace=TRUE) Model_Bootstrap &lt;- lm(data=BootstrapSample, LowPrice~Acc060) b0[i] &lt;- Model_Bootstrap$coeff[1] b1[i] &lt;-Model_Bootstrap$coeff[2] } Cars_Bootstrap_Results_Acc060 &lt;- data.frame(b0, b1) 4.4.3 First 10 Bootstrap Regression Lines ggplot(data=Cars2015, aes(x=Acc060,y=LowPrice))+ geom_point()+ theme_bw() + geom_abline(data=Cars_Bootstrap_Results_Acc060[1:10, ],aes(slope=b1,intercept=b0),color=&quot;red&quot;) + stat_smooth( method=&quot;lm&quot;, se=FALSE) 4.4.4 Bootstrap Percentile CI for Slope of Cars Reg. Line q.025 &lt;- quantile(Cars_Bootstrap_Results_Acc060$b1, 0.025) q.975 &lt;- quantile(Cars_Bootstrap_Results_Acc060$b1, 0.975) c(q.025, q.975) ## 2.5% 97.5% ## -8.797594 -5.696359 Cars_Acc060_B_Slope_Plot &lt;- ggplot(data=Cars_Bootstrap_Results_Acc060, aes(x=b1)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;Cars Acc060: Slope in Bootstrap Sample &quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Bootstrap Distribution for Slope&quot;) Cars_Acc060_B_Slope_Plot + geom_segment(aes(x=q.025,xend=q.975, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We are 95% confident that the average price of a new 2015 car decreases between -8.8 and -5.7 thousand dollars for each additional second it takes to accelerate from 0 to 60 mph. Question: Why shouldn’t we make a bootstrap confidence interval for \\(b_0\\), the intercept, in this context? 4.5 Bootstrapping Coefficients in Multiple Regression 4.5.1 Model for Predicting Bear Weights We previously used a linear regression model to predict the weights of wild bears, using a sample of 97 bears. Recall the model and its interpretations. ggplot(data=Bears_Subset, aes(x=Age, y=Weight, color=Sex)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) 4.5.2 Model for Predicting Bear Weights (cont.) Bear_M_Age_Sex_Int &lt;- lm(data=Bears_Subset, Weight~ Age*Sex) summary(Bear_M_Age_Sex_Int) ## ## Call: ## lm(formula = Weight ~ Age * Sex, data = Bears_Subset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -207.583 -38.854 -9.574 23.905 174.802 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 70.4322 17.7260 3.973 0.000219 *** ## Age 3.2381 0.3435 9.428 0.000000000000765 *** ## Sex2 -31.9574 35.0314 -0.912 0.365848 ## Age:Sex2 -1.0350 0.6237 -1.659 0.103037 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 70.18 on 52 degrees of freedom ## (41 observations deleted due to missingness) ## Multiple R-squared: 0.6846, Adjusted R-squared: 0.6664 ## F-statistic: 37.62 on 3 and 52 DF, p-value: 0.0000000000004552 4.5.3 Model Interpretations in Bears Interaction Model \\(\\widehat{\\text{Weight}}= 70.43+ 3.24 \\times\\text{Age}- 31.95\\times\\text{I}_{Female} -1.04\\times\\text{Age}\\times\\text{I}_{Female}\\) This model was fit using a sample of 97 wild bears. If we were to take a different sample, and fit a regression model, we would obtain different values for \\(b_0\\), \\(b_1\\), \\(b_2\\), and \\(b_3\\), as well as relevant quantities \\(b_0-b_2\\) and \\(b_1-b_3\\), due to sampling variability. Questions of interest: Find a reasonable range for the following quantities: the mean monthly weight gain among all male bears the mean monthly weight gain among all female bears the mean weight among all 24 month old male bears the mean weight among all 24 month old female bears Just as we did for sample means and proportions, we can answer these questions via bootstrapping. 4.5.4 Quantities of Interest First, we need to find expressions for the quantities we are interested in, in terms of the model coefficients. \\(\\widehat{\\text{Weight}}= b_0+ b_1 \\times\\text{Age} - b_2\\times\\text{I}_{Female} + b_3\\times\\text{Age}\\times\\text{I}_{Female}\\) Expected Weight for Female Bears: \\[ \\begin{aligned} \\widehat{\\text{Weight}} &amp; = b_0+ b_1 \\times\\text{Age}+ b_2 + b_3\\times\\text{Age} \\\\ &amp; (b_0 + b_2) + (b_1+b_3)\\times\\text{Age} \\end{aligned} \\] Expected Weight for Male Bears: \\[ \\begin{aligned} \\widehat{\\text{Weight}}= b_0 + b_1 \\times\\text{Age} \\end{aligned} \\] Questions of interest: Find a reasonable range for the following quantities: the mean weight gain per month among all male bears (\\(b_1\\)) the mean weight gain per month among all female bears (\\(b_1 + b_3\\)) the mean weight among all 24 month old male bears (\\(b_0 + 24b_1\\)) the mean weight among all 24 month old female bears (\\(b_0 + b_2 + 24(b_1+b_3)\\)) 4.5.5 Bootstrapping for Bears Regression Coefficients Bootstrapping Procedure Take a bootstrap sample of size 97, by sampling bears with replacement. Fit a model and record regression coefficients \\(b_0\\), \\(b_1\\), \\(b_2\\), \\(b_3\\). Repeat steps 1 and 2 10,000 times, keeping track of the regression coefficient estimates in each bootstrap sample. Look at the distribution of regression coefficients in the bootstrap samples. The variability in this distribution can be used to approximate the variability in the sampling distributions for the regression coefficients. 4.5.6 Bootstrap Code for Bears Regression Coefficients b0 &lt;- rep(NA, 10000) b1 &lt;- rep(NA, 10000) b2 &lt;- rep(NA, 10000) b3 &lt;- rep(NA, 10000) for (i in 1:10000){ BootstrapSample &lt;- sample_n(Bears_Subset, 97, replace=TRUE) #take bootstrap sample M &lt;- lm(data=BootstrapSample, Weight ~ Age*Sex) ## fit linear model b0[i] &lt;- coef(M)[1] ## record b0 b1[i] &lt;- coef(M)[2] ## record b1 b2[i] &lt;- coef(M)[3] ## record b2 b3[i] &lt;- coef(M)[4] ## record b3 } Bears_Bootstrap_Results &lt;- data.frame(b0, b1, b2, b3) 4.5.7 Bootstrap Percentile CI for \\(b_1\\) in Bears Model The average weight gain per month for male bears is represented by \\(b_1\\). q.025 &lt;- quantile(Bears_Bootstrap_Results$b1, 0.025) q.975 &lt;- quantile(Bears_Bootstrap_Results$b1, 0.975) c(q.025, q.975) ## 2.5% 97.5% ## 2.365099 5.969894 Bears_plot_b1 &lt;- ggplot(data=Bears_Bootstrap_Results, aes(x=b1)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;b1 in Bootstrap Sample&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Bears Weight by Age and Sex: Bootstrap Distribution for b1&quot;) Bears_plot_b1 + geom_segment(aes(x=q.025,xend=q.975, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We are 95% confident that male bears gain between 2.37 and 5.97 pounds per month, on average. 4.5.8 Bootstrap Percentile CI for \\(b_1 + b_3\\) The average weight gain per month for female bears is represented by \\(b_1 + b3\\). q.025 &lt;- quantile(Bears_Bootstrap_Results$b1 + Bears_Bootstrap_Results$b3, 0.025) q.975 &lt;- quantile(Bears_Bootstrap_Results$b1 + Bears_Bootstrap_Results$b3, 0.975) c(q.025, q.975) ## 2.5% 97.5% ## 1.380019 3.443457 ggplot(data=Bears_Bootstrap_Results, aes(x=b1+b3, fill=!(b1+b3 &gt;=q.975 | b1 + b3 &lt;= q.025))) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;b1 + b3 in Bootstrap Sample&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Bears Weight by Age and Sex:Bootstrap Distribution for b1+b3&quot;) + geom_segment(aes(x=q.025,xend=q.975, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We are 95% confident that female bears gain between 1.3800186 and 3.4434571 pounds per month, on average. 4.5.9 Bootstrap Percentile CI for \\(b_0 + 24b_1\\) The mean weight of all 24 month old male bears is represented by \\(b_0 + 24b_1\\). q.025 &lt;- quantile(Bears_Bootstrap_Results$b0 + 24*Bears_Bootstrap_Results$b1, 0.025) q.975 &lt;- quantile(Bears_Bootstrap_Results$b0 + 24*Bears_Bootstrap_Results$b1, 0.975) c(q.025, q.975) ## 2.5% 97.5% ## 128.5003 169.0521 ggplot(data=Bears_Bootstrap_Results, aes(x=b0+24*b1)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;b0 + 24b1 in Bootstrap Sample&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Bears Weight by Age and Sex: Bootstrap Distribution for b0+24b1&quot;) + geom_segment(aes(x=q.025,xend=q.975, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We are 95% confident that the mean weight of all 24 month old bears is between 128.5002676 and 169.0521237 pounds. 4.5.10 Bootstrap Percentile CI for \\((b_0 + b_2) + 24(b_1+b_3)\\) The mean weight of all 24 month old female bears is represented by \\((b_0 + b_2) + 24(b_1+b_3)\\). q.025 &lt;- quantile(Bears_Bootstrap_Results$b0 + Bears_Bootstrap_Results$b2 + 24*(Bears_Bootstrap_Results$b1 + Bears_Bootstrap_Results$b3), 0.025) q.975 &lt;- quantile(Bears_Bootstrap_Results$b0 + Bears_Bootstrap_Results$b2 + 24*(Bears_Bootstrap_Results$b1 + Bears_Bootstrap_Results$b3), 0.975) c(q.025, q.975) ## 2.5% 97.5% ## 76.2520 112.5801 ggplot(data=Bears_Bootstrap_Results, aes(x=(b0+b2)+24*(b1+b3))) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;(b0+b2)+24*(b1+b3) in Bootstrap Sample&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Bears Weight by Age and Sex: Bootstrap Distribution for (b0+b2)+24*(b1+b3)&quot;) + geom_segment(aes(x=q.025,xend=q.975, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We are 95% confident that the mean weight of all 24 month old female bears is between 76.2519955 and 112.5800513 pounds. 4.6 Bootstrap Standard Error Confidence Intervals 4.6.1 Shape of Bootstrap Distributions Notice that most of the bootstrap distributions we’ve seen have been symmetric and bell-shaped. When a distribution is symmetric and bell-shaped, then approximately 95% of all observations lie within two standard deviations of the mean. The standard deviation in a distribution of a statistic(i.e. mean, proportion, median, regression coefficient, etc.) is called the standard error of that statistic. 95% bootstrap standard error confidence interval: \\[ \\text{Statistic} \\pm 2\\times\\text{Standard Error} \\] it is only appropriate to use the bootstrap standard error confidence interval method when a sampling distribution is symmetric and bell-shaped The \\(\\pm\\) \\(2\\times\\text{SE}\\) is called a margin of error, and the resulting range of plausible values for the parameter is called a 95% bootstrap stanadard error confidence interval. 4.6.2 Comparing SE and Percential Bootstrap Intervals Let’s compare calculate bootstrap standard error confidence intervals and compare them with some the bootstrap percentile confidence intervals that we’ve seen previously. 4.6.3 Mean Hg in Lakes Lakes_Bootstrap_Mean SE &lt;- sd(Lakes_Bootstrap_Results_Mean$MeanHg) SE ## [1] 0.04636432 Bootstrap Standard Error Confidence Interval Stat &lt;- mean(FloridaLakes$AvgMercury) c(Stat-2*SE, Stat+2*SE) ## [1] 0.4344412 0.6198985 Bootstrap Percentile Confidence Interval q.025 &lt;- quantile(Lakes_Bootstrap_Results_Mean$MeanHg, 0.025) q.975 &lt;- quantile(Lakes_Bootstrap_Results_Mean$MeanHg, 0.975) c(q.025, q.975) ## 2.5% 97.5% ## 0.4379198 0.6179245 Lakes_Bootstrap_Mean + geom_segment(aes(x=q.025,xend=q.975, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) + geom_segment(aes(x = Stat-2*SE, y = 150, xend = Stat+2*SE, yend = 150),color = &quot;darkgray&quot;,size=10, alpha=0.01) Percentile interval in gold, standard error interval in gray 4.6.4 Difference in Mean Hg in Lakes NS_Lakes_Bootstrap_Plot_b1 SE &lt;- sd(NS_Lakes_Bootstrap_Results$b1) SE ## [1] 0.09559846 Bootstrap Standard Error Confidence Interval Stat &lt;- mean(NS_Lakes_Bootstrap_Results$b1) c(Stat-2*SE, Stat+2*SE) ## [1] 0.08126845 0.46366231 Bootstrap Percentile Confidence Interval q.025 &lt;- quantile(NS_Lakes_Bootstrap_Results$b1, 0.025) q.975 &lt;- quantile(NS_Lakes_Bootstrap_Results$b1, 0.975) c(q.025, q.975) ## 2.5% 97.5% ## 0.08475265 0.46174886 NS_Lakes_Bootstrap_Plot_b1 + geom_segment(aes(x=q.025,xend=q.975, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) + geom_segment(aes(x = Stat-2*SE, y = 150, xend = Stat+2*SE, yend = 150),color = &quot;darkgray&quot;,size=10, alpha=0.01) Percentile interval in gold, standard error interval in gray 4.6.5 Proportion Lakes Exceeding 1 ppm Hg Lakes_Bootstrap_PropOver1 SE &lt;- sd(Lakes_Bootstrap_Results_Other$PropOver1) SE ## [1] 0.04360363 Bootstrap Standard Error Confidence Interval Stat &lt;- mean(Lakes_Bootstrap_Results_Other$PropOver1) c(Stat-2*SE, Stat+2*SE) ## [1] 0.02586822 0.20028273 Bootstrap Percentile Confidence Interval q.025 &lt;- quantile(Lakes_Bootstrap_Results_Other$PropOver1, 0.025) q.975 &lt;- quantile(Lakes_Bootstrap_Results_Other$PropOver1, 0.975) c(q.025, q.975) ## 2.5% 97.5% ## 0.03773585 0.20754717 Lakes_Bootstrap_PropOver1 + geom_segment(aes(x=q.025,xend=q.975, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) + geom_segment(aes(x = Stat-2*SE, y = 150, xend = Stat+2*SE, yend = 150),color = &quot;darkgray&quot;,size=10, alpha=0.01) Percentile interval in gold, standard error interval in gray. 4.6.6 Cars Regression Slope Cars_Acc060_B_Slope_Plot SE &lt;- sd(Cars_Bootstrap_Results_Acc060$b1) SE ## [1] 0.7873212 Bootstrap Standard Error Confidence Interval Stat &lt;- mean(Cars_Bootstrap_Results_Acc060$b1) c(Stat-2*SE, Stat+2*SE) ## [1] -8.787625 -5.638340 Bootstrap Percentile Confidence Interval q.025 &lt;- quantile(Cars_Bootstrap_Results_Acc060$b1, 0.025) q.975 &lt;- quantile(Cars_Bootstrap_Results_Acc060$b1, 0.975) c(q.025, q.975) ## 2.5% 97.5% ## -8.797594 -5.696359 Cars_Acc060_B_Slope_Plot + geom_segment(aes(x=q.025,xend=q.975, y=50, yend=50), color=&quot;gold&quot;, size=10) + geom_segment(aes(x = Stat-2*SE, y = 100, xend = Stat+2*SE, yend = 100),color = &quot;gray&quot;,size=10) Percentile interval in gold, standard error interval in gray 4.6.7 Bears \\(b1\\) Coefficient Bears_plot_b1 Bootstrap Standard Error Confidence Interval SE &lt;- sd(Bears_Bootstrap_Results$b1) SE ## [1] 1.01682 Stat &lt;- mean(Bears_Bootstrap_Results$b1) c(Stat-2*SE, Stat+2*SE) ## [1] 1.581729 5.649011 Bootstrap Percentile Confidence Interval q.025 &lt;- quantile(Bears_Bootstrap_Results$b1, 0.025) q.975 &lt;- quantile(Bears_Bootstrap_Results$b1, 0.975) c(q.025, q.975) ## 2.5% 97.5% ## 2.365099 5.969894 Bears_plot_b1 +xlim(c(1.5,6.5)) + geom_segment(aes(x=q.025,xend=q.975, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) + geom_segment(aes(x = Stat-2*SE, y = 150, xend = Stat+2*SE, yend = 150),color = &quot;gray&quot;,size=10, alpha=0.01) Percentile interval in gold, standard error interval in gray 4.7 Standard Error Formulas 4.7.1 What is Standard Error? Standard error is the standard deviation of the distribution of a statistic (sample mean, proportion, regression coefficient, etc.). It describes the amount of variability in this statistic between samples of a given size. This is different than the sample standard deviation, which pertains to the amount of variability between individuals in the sample. Distribution of Mercury Levels in Florida Lakes: Lakes_Hist Standard Deviation of Mercury Levels Between Lakes: sd(FloridaLakes$AvgMercury) ## [1] 0.3410356 The standard deviation in mercury levels between individual lakes is 0.341 ppm. This describes how much variability there is in mercury levels between individual lakes. Bootstrap Distribution for Mean Mercury Level (\\(n=53\\)) Lakes_Bootstrap_Mean + xlim(c(0,1.5)) Standard Error for Mean: SE &lt;- sd(Lakes_Bootstrap_Results_Mean$MeanHg); SE ## [1] 0.04636432 The standard deviation in the distribution for mean mercury levels between different samples of 53 lakes is approximately 0.0463643 ppm. This describes how much variability there is in mean mercury levels between different samples of 53 lakes. Question: Suppose the sample consisted of 100 lakes, instead of 53, and that the distribution of the new lakes in the sample was similar to that of the original 53. Would you expect the mercury level of individual lakes to increase, decrese, or stay about the same? What about the standard error of the mean mercury level? 4.7.2 Why Did We Use Simulation? We’ve used simulation (permutation tests and bootstrapping) to do two different things. To approximate the distribution of a statistic under the assumption that the null hypothesis is true. To determine how much a sample statistic might vary from one sample to the next. In some common situations, it is possible to use mathematical theory to calculate standard errors, without relying on simulation. 4.7.3 Theory-Based Standard Error Formulas Recall that standard error tells us about the variability in the distribution of a statistic between different samples size \\(n\\). In special cases, there are mathematical formulas for standard errors associated regression coefficients. Scenario Standard Error One Sample Mean \\(SE(\\bar{x})=\\frac{s}{\\sqrt{n}}\\) Difference in Sample Means \\(SE(\\bar{x}_1-\\bar{x}_2)=s\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}\\) Intercept in Simple Linear Regression \\(SE(b_0)=s\\sqrt{\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum(x_i-\\bar{x})^2}}\\) Slope in Simple Linear Regression \\(SE(b_1)=\\sqrt{\\frac{s^2}{\\sum(x_i-\\bar{x})^2}}\\) \\(s=\\sqrt{\\frac{\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{(n-(p+1))}}\\), (p is number of regression coefficients not including \\(b_0\\)) is sample standard deviation In the 2nd formula, the standard error estimate \\(s\\sqrt{\\frac{1}{n_1+n_2}}\\) is called a “pooled” estimate since it combines information from all groups. When there is reason to believe standard deviation differs between groups, we often use an “unpooled” standard error estimate of \\(\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}\\), where \\(s_1, s_2\\) represents the standard deviation for groups 1 and 2. 4.7.4 One-Sample Mean Example Standard Error of the mean, for sample of 53 lakes. \\(SE(\\bar{x})=\\frac{s}{\\sqrt{n}}\\) sd(FloridaLakes$AvgMercury)/sqrt(53) ## [1] 0.04684485 summary(lm(data=FloridaLakes, AvgMercury~1)) ## ## Call: ## lm(formula = AvgMercury ~ 1, data = FloridaLakes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.48717 -0.25717 -0.04717 0.24283 0.80283 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.52717 0.04684 11.25 0.00000000000000151 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.341 on 52 degrees of freedom Comparison to Bootstrap: Lakes_Bootstrap_Mean Standard Error for Mean: SE &lt;- sd(Lakes_Bootstrap_Results_Mean$MeanHg); SE ## [1] 0.04636432 4.7.5 Standard Error for Difference in Means (cont.) Standard Error for difference of means between 33 lakes in North Florida, and 20 lakes in South Florida \\[ SE(\\bar{x}_1-\\bar{x}_2)=s\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}, \\] s &lt;- sqrt(sum(Lakes_M$residuals^2)/(53-2)) SE &lt;- s*sqrt(1/20+1/33); SE ## [1] 0.08984774 summary(Lakes_M) ## ## Call: ## lm(formula = AvgMercury ~ Location, data = FloridaLakes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.65650 -0.23455 -0.08455 0.24350 0.67545 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.42455 0.05519 7.692 0.000000000441 *** ## LocationS 0.27195 0.08985 3.027 0.00387 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3171 on 51 degrees of freedom ## Multiple R-squared: 0.1523, Adjusted R-squared: 0.1357 ## F-statistic: 9.162 on 1 and 51 DF, p-value: 0.003868 Comparison to Bootstrap: NS_Lakes_Bootstrap_Plot_b1 &lt;- ggplot(data=NS_Lakes_Bootstrap_Results, aes(x=b1)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;b1 in Bootstrap Sample&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Northern vs Southern Lakes: Bootstrap Distribution for b1&quot;) NS_Lakes_Bootstrap_Plot_b1 sd(NS_Lakes_Bootstrap_Results$b1) ## [1] 0.09559846 4.7.6 Standard Error for Slope and Intercept in SLR Standard Errors pertaining to the variability in slope and intercept of regression line relating price and acceleration time in samples of 110 cars. Standard Error for Intercept of Regression Line: \\[ SE(b_0)=s\\sqrt{\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum(x_i-\\bar{x})^2}} \\] Standard Error for Slope of Regression Line: \\[ SE(b_1)=\\sqrt{\\frac{s^2}{\\sum(x_i-\\bar{x})^2}} \\] s &lt;- summary(Cars_M_A060)$sigma xbar &lt;- mean(Cars2015$Acc060) SSx &lt;- sum((Cars2015$Acc060-xbar)^2) SE for Intercept s*sqrt(1/110 + xbar^2/SSx) ## [1] 5.052325 SE for Slope: sqrt(s^2/SSx) ## [1] 0.6234005 summary(Cars_M_A060) ## ## Call: ## lm(formula = LowPrice ~ Acc060, data = Cars2015) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.512 -6.544 -1.265 4.759 27.195 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 89.9036 5.0523 17.79 &lt;0.0000000000000002 *** ## Acc060 -7.1933 0.6234 -11.54 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.71 on 108 degrees of freedom ## Multiple R-squared: 0.5521, Adjusted R-squared: 0.548 ## F-statistic: 133.1 on 1 and 108 DF, p-value: &lt; 0.00000000000000022 4.7.7 \\(\\text{SE}(b_j)\\) in MLR When there is more than one explanatory variable, estimates of regression coefficients and their standard errors become more complicated, and involves inversion of a “design matrix.” This link provides additional information on the topic. Understanding it will likely require experience with linear algebra (i.e MATH 250). Estimation in MLR goes beyond the scope of this class. For MLR in this class, you may use the estimates and standard errors reported in the R output, without being expected to calculate them yourself. 4.7.8 R Output for SE’s in MLR summary(Bear_M_Age_Sex_Int) ## ## Call: ## lm(formula = Weight ~ Age * Sex, data = Bears_Subset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -207.583 -38.854 -9.574 23.905 174.802 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 70.4322 17.7260 3.973 0.000219 *** ## Age 3.2381 0.3435 9.428 0.000000000000765 *** ## Sex2 -31.9574 35.0314 -0.912 0.365848 ## Age:Sex2 -1.0350 0.6237 -1.659 0.103037 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 70.18 on 52 degrees of freedom ## (41 observations deleted due to missingness) ## Multiple R-squared: 0.6846, Adjusted R-squared: 0.6664 ## F-statistic: 37.62 on 3 and 52 DF, p-value: 0.0000000000004552 4.7.9 Theory-Based Confidence Intervals If the sampling distribution for a statistic is symmetric and bell-shaped, we can obtain an approximate 95% confidence interval using the formula: \\[ \\text{Statistic} \\pm 2\\times{\\text{Standard Error}}, \\] where the standard error is calculated by formula, rather than via bootstrap simulations. 4.7.10 Comparison of CI Methods We’ve now seen 3 different ways to obtain confidence intervals based on statistics, calculated from data. The table below tells us what must be true of the sampling distribution for a statistic in order to use each technique. Technique No Gaps Bell-Shaped Known Formula for SE Bootstrap Percentile x Bootstrap Standard Error x x Theory-Based x x x 4.7.11 When to use Each CI Method Example 1 Mean mercury level for all Florida lakes: Lakes_Bootstrap_Mean It is appropriate to use any of the 3 CI methods since sampling distribution is symmetric and bell-shaped with no gaps there is a known formula to calculate standard error for a sample mean The methods should all produce similar results. Example 2 Slope of regression line in cars example Cars_Acc060_B_Slope_Plot It is appropriate to use any of the 3 CI methods since sampling distribution is symmetric and bell-shaped with no gaps there is a known formula to calculate standard error for a slope of regression line The methods should all produce similar results. Example 3 Standard deviation of mercury level in Florida Lakes Lakes_Bootstrap_SD It is appropriate to use the bootstrap percentile, and bootstrap standard error CI’s since the sampling distribution is symmetric and bell-shaped. We cannot use the theory-based interval because we do not have a formula to calculate the standard error, associated with an estimate of \\(\\sigma\\). Example 4 Regression slope in bears age model Bears_plot_b1 It is appropriate to use the bootstrap percentile CI, since the sampling distribution has no gaps. Since the distribution is not symmetric, it would be inappropriate to use the bootstrap standard error, or theory-based confidence interval (Although R does calculate a SE, using it to produce a CI would be unreliable). Example 5 Median mercury level in Florida Lakes Lakes_Bootstrap_Median Since the distribution has gaps, and is not symmetric, none of these procedures are appropriate. In some cases (usually with larger sample size), a bootstrap distribution for the median will not have these gaps. In these situations, the percentile bootstrap interval would be appropriate. If the distribution is bell-shaped, the standard error method would also be appropriate. We would not be able to use the theory-based approach, because there is no formula for the standard error associated with a sample median. "],["normal-error-regression-model.html", "Chapter 5 Normal Error Regression Model 5.1 The Normal Error Regression Model 5.2 Intervals and Tests in Normal Error Regression Model 5.3 Intervals for Predicted Values 5.4 Regression Model Assumptions 5.5 Transformations 5.6 Responsible Statistical Inference 5.7 The Regression Effect", " Chapter 5 Normal Error Regression Model Learning Outcomes: Explain when it is appropriate to use “theory-based” standard error formulas. Interpret estimates, standard errors, test statistics, and p-values resulting from linear model output in R. List the assumptions made in the normal error regression model. Calculate p-values corresponding to t-statistics and F-statistics in R. Interpret confidence intervals for an expected response, and prediction intervals, and distinguish between these two types of intervals. Assess the whether linear model assumptions are reasonably satisfied, using residual plots, histograms, and normal QQ plots. Explain when we should or should not expect p-values and confidence intervals obtained via “theory-based” approaches to agree with those obtained via simulation. Identify situations where a log transformation of the response variable is appropriate. Calculate predicted values for models involving a log transformation of the response variable. Interpret regression coefficients in models involving a log transformation of the response variable. Explain the regression effect. 5.1 The Normal Error Regression Model 5.1.1 Example: Ice Cream Dispensor Suppose an ice cream machine is manufacturered to dispense 2 oz. of ice cream per second, on average. If each person using the machine got exactly 2 oz. per second, the relationship between time and amount dispensed would look like this: In reality, however, the actual amount dispensed each time it is used will vary due to unknown factors like: force applied to dispensor temperature build-up of ice cream other unknown factors Thus, the data will actually look like this: 5.1.2 Signal and Noise We assume that there are two components that contribute to our response variable \\(Y_i\\). These are: A function that relates the expected (or average) value of \\(Y\\) to explanatory variables \\(X_1, X_2, \\ldots{X_p}\\). That is, \\(E(Y_i)= f(X_{i1}, X_{i2}, \\ldots, X_{ip})\\). This function is often assumed to be linear, that is \\(E(Y_i)= \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2}+ \\ldots+ \\beta_pX_{ip}\\) Random, unexplained, variability that results in an individual response \\(Y_i\\) differing from \\(E(Y_i)\\). The first component is often referred to as signal. The second is referred to as noise. 5.1.3 Normal Distribution In a linear regression model, we assume individual response values \\(Y_i\\) deviate from their expectation, according to a normal distribution. A normal distribution is defined by two parameters: - \\(\\mu\\) representing the center of the distribution - \\(\\sigma\\) representing the standard deviation This distribution is denoted \\(\\mathcal{N}(\\mu, \\sigma)\\). Note that for standard deviation \\(\\sigma\\), \\(\\sigma^2\\) is called the variance. Some books denote the normal distribution as \\(\\mathcal{N}(\\mu, \\sigma^2)\\), instead of \\(\\mathcal{N}(\\mu,\\sigma)\\). 5.1.4 Signal and Noise in Icecream Example In this example, I simulated the amount of ice cream dispensed by adding a random number from a normal distribution with mean 0 and standard deviation 0.5 to the expected amount dispensed, which is given by \\(2x\\), where \\(x\\) represents time pressing the dispenser. Thus, amount dispensed for person \\(i\\) is given by \\[Y_i = 2x_i+\\epsilon_i, \\text{ where } \\epsilon_i\\sim\\mathcal{N}(0, 0.5) \\] set.seed(10082020) time &lt;- c(1, 1.2, 1.5, 1.8, 2.1, 2.1, 2.3, 2.5, 2.6, 2.8, 2.9, 2.9, 3.1, 3.2, 3.6) signal &lt;- 2*time noise &lt;-rnorm(15, 0, 0.5) amount &lt;- 2*time + noise Icecream &lt;- data.frame(time, signal, noise, amount) kable(head(Icecream)) time signal noise amount 1.0 2.0 0.2318223 2.231822 1.2 2.4 -0.4895681 1.910432 1.5 3.0 0.5815205 3.581520 1.8 3.6 -0.0318453 3.568155 2.1 4.2 0.1716097 4.371610 2.1 4.2 -0.9288076 3.271192 5.1.5 What We Actually Get to See In reality, we do not see the signal and noise columns, we only see time and amount. From this, we need to estimate signal, without being thrown off by noise. ggplot(data=Icecream1, aes(x=time, y=amount)) + geom_point() + ggtitle(&quot;Icecream Dispensed&quot;) + xlab(&quot;Time Pressing Dispensor&quot;) + ylab(&quot;Amount Dispensed&quot;) The red line represents the “true” relationship between time and average amount dispensed. The blue line represents the least squares regression line, fit from the data. The blue line is an approximation of the red line. ggplot(data=Icecream1, aes(x=time, y=amount)) + geom_point() + ggtitle(&quot;Icecream Dispensed&quot;) + xlab(&quot;Time Pressing Dispensor&quot;) + ylab(&quot;Amount Dispensed&quot;) + stat_smooth(method=&quot;lm&quot;, se=FALSE) + geom_abline(slope=2, intercept=0, color=&quot;red&quot;) + annotate(&quot;text&quot;, label=&quot;y=2x&quot;, x= 3.5, y=6.5, size=10, color=&quot;red&quot;) 5.1.6 IceCream Model From Simulated Data IC_Model &lt;- lm(data=Icecream1, lm(amount~time)) summary(IC_Model) ## ## Call: ## lm(formula = lm(amount ~ time), data = Icecream1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.8645 -0.3553 0.0685 0.2252 0.6963 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.1299 0.3968 -0.327 0.749 ## time 2.0312 0.1598 12.714 0.0000000104 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4527 on 13 degrees of freedom ## Multiple R-squared: 0.9256, Adjusted R-squared: 0.9198 ## F-statistic: 161.6 on 1 and 13 DF, p-value: 0.00000001042 5.1.7 Mathematical Form of Normal Error Regression Model The mathematical form of a normal error linear regression model is \\(Y_i = \\beta_0 + \\beta_1X_{i1}+ \\ldots + \\beta_pX_{ip} + \\epsilon_i\\), with \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\). Note that in place of \\(X_{ip}\\), we could have indicators for categories, or functions of \\(X_{ip}\\), such as \\(X_{ip}^2\\), \\(\\text{log}(X_{ip})\\), or \\(\\text{sin}(X_{ip})\\). 5.1.8 Parameters and Statistics We call the quantities \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) parameters. They pertain to the true but unknown data generating mechanism. We call the estimates \\(b_0, b_1, \\ldots, b_p\\), statistics. They are calculated from our observed data. We use confidence intervals and hypothesis tests make statements about parameters, based on information provided by statistics. In reality, data almost never truly come from normal distributions, but the normal distributions are often useful in approximating real data distributions. As statistician George Box said, “All models are wrong, but some are useful.” 5.2 Intervals and Tests in Normal Error Regression Model 5.2.1 Symmetric, Bell-Shaped Distributions Notice that when we used simulation to approximate the sampling distributions of statistics, many (but not all) of these turned out to be symmetric and bell-shaped. grid.arrange(Lakes_Bootstrap_Mean, Cars_Acc060_B_Slope_Plot, NS_Lakes_Bootstrap_Plot_b1, Bears_plot_b1, ncol=2) In a normal error regression model, we assume that response \\(Y_i\\) deviates from its expectation, \\(E(Y_i) = \\beta_0 + \\beta_iX_{i1} + \\ldots + \\beta_p X_{ip}\\), according to a normally distributed random error term. When this assumption is valid we can use symmetric, bell-shaped curves to approximate the sampling distribution of regression coefficients. 5.2.2 t-distribution A t-distribution is a symmetric, bell-shaped curve, with thicker tails (hence more variability), than a \\(\\mathcal{N}(0,1)\\) distribution. 5.2.3 F-Distribution An [F distribution] is a right-skewed distribution. It is defined by two parameters, \\(\\nu_1, \\nu_2\\), called numerator and denominator degrees of freedom. 5.2.4 Distribution of \\(t= \\frac{{b_j}-\\beta_j}{\\text{SE}(b_j)}\\) Important Fact: If \\(Y_i = \\beta_0 + \\beta_1X_{i1}+ \\ldots + \\beta_pX_{ip} + \\epsilon_i\\), with \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\), then \\[ t= \\frac{{b_j}-\\beta_j}{\\text{SE}(b_j)} \\] follows a t-distribution with \\(n-(p+1)\\) degrees of freedom. Confidence Interval for \\(\\beta_j\\) A 95% confidence interval for \\(\\beta_j\\) is given by \\(b_j \\pm t^*\\left({\\text{SE}(b_j)}\\right)\\), where \\(t^*\\) is chosen to achieve the desired confidence level. For a 95% confidence interval, use \\(t^*=2\\). Hypothesis test for \\(\\beta_j=\\gamma\\) If \\(Y_i = \\beta_0 + \\beta_1X_{i1}+ \\ldots + \\beta_pX_{ip} + \\epsilon_i\\), with \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\), then a test statistic for the null hypothesis: \\(\\beta_j = \\gamma\\) is given by: \\[ t=\\frac{{b_j}-\\gamma}{\\text{SE}(b_j)}, \\] and calculate a p-value using a t-distribution with \\(n-(p+1)\\) df. 5.2.5 Distribution of F-Statistic If \\(Y_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2{X_i2} + \\ldots + \\beta_qX_{iq} + \\epsilon_i\\), with \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\), and \\(Y_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2{X_i2} + \\ldots + \\beta_qX_{iq} + \\beta_{q+1}X_{i{q+1}} \\ldots + \\beta_pX_{ip}+ \\epsilon_i\\), is another proposed model, then \\[ F=\\frac{\\frac{\\text{Unexplained Variability in Reduced Model}-\\text{Unexplained Variability in Full Model}}{p-q}}{\\frac{\\text{Unexplained Variability in Full Model}}{n-(p+1)}} \\] follows an F-distribution with (p-q) and (n-(p+1)) degrees of freedom. We have seen that for a categorical variable with \\(g\\) groups, the proposed models reduce to \\(Y_i = \\beta_0 + \\epsilon_i\\), with \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\), and \\(Y_i = \\beta_0 + \\beta_1\\text{I}_{\\text{Group2 }{i}} + \\ldots + \\beta_{g-1}\\text{I}_{\\text{Groupg }{i}}+ \\epsilon_i\\), and the F-statistic is equivalent to \\[ F= \\frac{\\text{Variability between Groups}}{\\text{Variability within Groups}}= \\frac{\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}n_i(y_{i\\cdot}-\\bar{y}_{\\cdot\\cdot})^2}{g-1}}{\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i\\cdot})^2}{n-g}} \\] and this statistic follows and F-distribution with (g-1) and (n-g) degrees of freedom. 5.2.6 Mercury Levels in Lakes in Northern and Southern Florida LakesBP &lt;- ggplot(data=FloridaLakes, aes(x=Location, y=AvgMercury, fill=Location)) + geom_boxplot() + geom_jitter() + ggtitle(&quot;Mercury Levels in Florida Lakes&quot;) + xlab(&quot;Location&quot;) + ylab(&quot;Mercury Level&quot;) + theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + coord_flip() LakesBP LakesTable &lt;- FloridaLakes %&gt;% group_by(Location) %&gt;% summarize(MeanHg=mean(AvgMercury), StDevHg=sd(AvgMercury), N=n()) kable(LakesTable) Location MeanHg StDevHg N N 0.4245455 0.2696652 33 S 0.6965000 0.3838760 20 summary(Lakes_M) ## ## Call: ## lm(formula = AvgMercury ~ Location, data = FloridaLakes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.65650 -0.23455 -0.08455 0.24350 0.67545 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.42455 0.05519 7.692 0.000000000441 *** ## LocationS 0.27195 0.08985 3.027 0.00387 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3171 on 51 degrees of freedom ## Multiple R-squared: 0.1523, Adjusted R-squared: 0.1357 ## F-statistic: 9.162 on 1 and 51 DF, p-value: 0.003868 5.2.7 Hypothesis Test for Lakes in North and South Florida Null Hypothesis: There is no difference in average mercury levels between Northern and Southern Florida (\\(\\beta_1=0\\)). Alternative Hypothesis: There is a difference in average mercury levels in Northern and Southern Florida (\\(\\beta_1\\neq 0\\)). Test Statistic: \\(t=\\frac{{b_j}-\\gamma}{\\text{SE}(b_j)} = \\frac{0.27195-0}{0.08985} = 3.027\\) on \\(53-2 = 51\\) degrees of freedom. Key Question: What is the probability of getting a t-statistic as extreme as 3.027 if \\(\\beta_1=0\\) (i.e. there is no difference in mercury levels between northern and southern lakes). 5.2.8 t-statistic and p-value ts=3.027 gf_dist(&quot;t&quot;, df=51, geom = &quot;area&quot;, fill = ~ (abs(x)&lt; abs(ts)), show.legend=FALSE) + geom_vline(xintercept=c(ts, -ts), color=&quot;red&quot;) + xlab(&quot;t&quot;) 2*pt(-abs(ts), df=51) ## [1] 0.003866374 The low p-value gives us strong evidence of a difference in average mercury levels between lakes in Northern and Southern Florida. 5.2.9 Theory-Based CI for \\(\\beta_1\\) in Florida Lakes 95% confidence interval for \\(\\beta_0\\): \\[ \\text{Statistic} \\pm 2\\times\\text{Standard Error} \\] \\[ 0.27195 \\pm 2(0.08985) \\] We can be 95% confident that average mercury level is between 0.09 and 0.45 ppm higher in Southern Florida, than Northern Florida. 5.2.10 Comparison to Simulation Let’s compare these results to those given by the permutation test and bootstrap confidence interval. Permutation Test NSLakes_SimulationResultsPlot p-value: b1 &lt;- Lakes_M$coef[2] ## record value of b1 from actual data mean(abs(NSLakes_SimulationResults$b1Sim) &gt; abs(b1)) ## [1] 0.0039 Percentile Bootstrap Confidence Interval q.025 &lt;- quantile(NS_Lakes_Bootstrap_Results$b1, 0.025) q.975 &lt;- quantile(NS_Lakes_Bootstrap_Results$b1, 0.975) c(q.025, q.975) ## 2.5% 97.5% ## 0.08475265 0.46174886 NS_Lakes_Bootstrap_Plot_b1 &lt;- ggplot(data=NS_Lakes_Bootstrap_Results, aes(x=b1)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;b1 in Bootstrap Sample&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Northern vs Southern Lakes: Bootstrap Distribution for b1&quot;) NS_Lakes_Bootstrap_Plot_b1 + geom_segment(aes(x=q.025,xend=q.975, y=50, yend=50), color=&quot;gold&quot;, size=10, alpha=0.01) We are 95% confident the average mercury level in Southern Lakes is between 0.08 and 0.46 ppm higher than in Northern Florida. As long as the normal error regression model is a reasonable approximation of the phenomenon we’re modeling, then we would expect the results of the t-test to closely match those obtained using simulation. 5.2.11 Coefficients Table in R Output For \\(\\hat{Y} = b_0 + b_1 X_{i1} + b_2X_{i2}+ \\ldots + b_pX_{ip}\\), Estimate gives the least-squares estimates \\(b_0, b_1, \\ldots, b_p\\) Standard Error gives estimates of the standard deviation in the sampling distribution for estimate. (i.e. how much uncertainty is there about the estimate?) t value is the estimate divided by its standard error. Pr(&gt;|t|) is a p-value for the hypothesis test of whether quantity represented \\(b_j\\) could plausibly be 0. This p-value is an approximation of the kind of p-value we have obtained through simulation. It is reliable only when certain assumptions are reasonable. 5.2.12 Bear Weight by Season Recall Bear Weights by Season ggplot(data=Bears_Subset, aes(y=Weight, x=Season, fill=Season)) + geom_boxplot() + geom_jitter() summary(Bears_M_Season) ## ## Call: ## lm(formula = Weight ~ Season, data = Bears_Subset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -178.84 -79.84 -29.02 54.98 309.16 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 204.84 17.16 11.939 &lt;0.0000000000000002 *** ## SeasonSpring -37.27 34.62 -1.076 0.284 ## SeasonSummer -29.81 24.71 -1.206 0.231 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 112.5 on 94 degrees of freedom ## Multiple R-squared: 0.02034, Adjusted R-squared: -0.0005074 ## F-statistic: 0.9757 on 2 and 94 DF, p-value: 0.3807 Bears_A_Season &lt;- aov(data=Bears_Subset, Weight~Season) summary(Bears_A_Season) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Season 2 24699 12350 0.976 0.381 ## Residuals 94 1189818 12658 5.2.13 Bears F-Test Illustration ts=0.976 gf_dist(&quot;f&quot;, df1=2, df2=94, geom = &quot;area&quot;, fill = ~ (abs(x)&lt; abs(ts)), show.legend=FALSE) + geom_vline(xintercept=c(ts), color=&quot;red&quot;) + xlab(&quot;t&quot;) p-value: 1-pf(ts, df1=2, df2=94) ## [1] 0.3806007 5.2.14 Comparison to Simulation Fstat &lt;- summary(Bears_M_Season)$fstatistic[1] Bears_Seasons_SimulationResultsPlot mean(FSim &gt; Fstat) ## [1] 0.3762 The p-value we obtained is very similar to the one we obtained using the simulation-based test. In this case, even though we had concerns about normality, they did not have much impact on the p-value from the F-distribution. The F-test is fairly robust to minor departures from normality. 5.2.15 Car Size and Price ggplot(data=Cars2015, aes(x=Size, y=LowPrice)) + geom_boxplot(aes(fill=Size)) + coord_flip() Cars_A_Size &lt;- aov(data=Cars2015, LowPrice~Size) summary(Cars_A_Size) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Size 2 4405 2202.7 10.14 0.0000927 *** ## Residuals 107 23242 217.2 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 5.2.16 Bears F-Test Illustration ts=10.14 gf_dist(&quot;f&quot;, df1=2, df2=107, geom = &quot;area&quot;, fill = ~ (abs(x)&gt; abs(ts)), show.legend=FALSE) + geom_vline(xintercept=c(ts), color=&quot;red&quot;) + xlab(&quot;t&quot;) p-value: 1-pf(ts, df1=2, df2=107) ## [1] 0.00009276052 5.2.17 Simulation-Based F-test Fstat &lt;- summary(Cars_M_Size)$fstatistic[1] CarSize_SimulationResults_Plot mean(CarSize_SimulationResults$FSim &gt; Fstat) ## [1] 0 The data provide strong evidence of a relationship between price and size. The results of the simulation based F-test and theory-based approximation are consistent with one-another. 5.3 Intervals for Predicted Values 5.3.1 Estimation and Prediction Recall the icecream dispensor that is known to dispense icecream at a rate of 2 oz. per second on average, with individual amounts varying according to a normal distribution with mean 0 and standard deviation 0.5 Consider the following two questions: On average, how much icecream will be dispensed for people who press the dispensor for 1.5 seconds? If a single person presses the dispensor for 1.5 seconds, how much icecream will be dispensed? The first question is one of estimation. The second pertains to prediction. 5.3.2 Uncertainty in Estimation and Prediction In estimation and prediction, we must think about two sources of variability. We are using data to estimate \\(\\beta_0\\) and \\(\\beta_1\\), which introduces sampling variability. Even if we did know \\(\\beta_0\\) and \\(\\beta_1\\), there is variability in individual observations, which follows a \\(\\mathcal{N}(0, \\sigma)\\) distribution. In an estimation problem, we only need to think about (1). When predicting the value of a single new observation, we need to think about both (1) and (2). Thus, intervals for predictions of individual observations carry more uncertainty and are wider than confidence intervals for \\(E(Y|X)\\). 5.3.3 Estimation in IC Example kable(t(round(Icecream1, 2))) time 1.00 1.20 1.50 1.80 2.10 2.10 2.30 2.50 2.60 2.80 2.90 2.90 3.1 3.20 3.60 amount 2.23 1.91 3.58 3.57 4.37 3.27 4.65 4.63 4.74 5.77 5.21 5.92 6.2 7.07 7.25 ggplot(data=Icecream1, aes(x=time, y=amount)) + geom_point() + ggtitle(&quot;Icecream Dispensed&quot;) + xlab(&quot;Time Pressing Dispensor&quot;) + ylab(&quot;Amount Dispensed&quot;) In the estimation setting, we are trying o determine the location of the regression line for the entire population. Uncertainty comes from the fact that we only have data from a sample. 5.3.4 Estimation in IC Example kable(t(round(Icecream1, 2))) time 1.00 1.20 1.50 1.80 2.10 2.10 2.30 2.50 2.60 2.80 2.90 2.90 3.1 3.20 3.60 amount 2.23 1.91 3.58 3.57 4.37 3.27 4.65 4.63 4.74 5.77 5.21 5.92 6.2 7.07 7.25 ggplot(data=Icecream1, aes(x=time, y=amount)) + geom_point() + ggtitle(&quot;Icecream Dispensed&quot;) + xlab(&quot;Time Pressing Dispensor&quot;) + ylab(&quot;Amount Dispensed&quot;) + geom_abline(slope=2, intercept=0, color=&quot;red&quot;) + stat_smooth(method=&quot;lm&quot;) 5.3.5 Recall Ice Cream Model Output summary(IC_Model) ## ## Call: ## lm(formula = lm(amount ~ time), data = Icecream1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.8645 -0.3553 0.0685 0.2252 0.6963 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.1299 0.3968 -0.327 0.749 ## time 2.0312 0.1598 12.714 0.0000000104 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4527 on 13 degrees of freedom ## Multiple R-squared: 0.9256, Adjusted R-squared: 0.9198 ## F-statistic: 161.6 on 1 and 13 DF, p-value: 0.00000001042 b0 &lt;- IC_Model$coefficients[1] b1 &lt;- IC_Model$coefficients[2] s &lt;- sigma(IC_Model) 5.3.6 Estimation in SLR The first question: “On average, how much icecream will be dispensed for people who press the dispensor for 1.5 seconds?” is a question of estimation. It is of the form, for a given \\(X\\), on average what do we expect to be true of \\(Y\\). In the ice cream question, we can answer this exactly, since we know \\(\\beta_0\\) and \\(\\beta_1\\). In a real situation, we don’t know these and have to estimate them from the data, which introduces uncertainty. Confidence interval for \\(E(Y | (X=x))\\): \\[ \\begin{aligned} &amp; b_0+b_1x^* \\pm t^*SE(\\hat{Y}|X=x^*) \\\\ &amp; b_0+b_1x^* \\pm t^*\\sqrt{\\widehat{Var}(\\hat{Y}|X=x^*)} \\end{aligned} \\] The second question is a question of prediction. Even if we knew the true values of \\(beta_0\\) and \\(\\beta_1\\), we would not be able to given the exact amount dispensed for an individual user, since this varies between users. Prediction interval for \\(E(Y | (X=x))\\): \\[ \\begin{aligned} &amp; b_0+b_1x^* \\pm t^*\\sqrt{\\widehat{Var}(\\hat{Y}|X=x^*) + s^2} \\end{aligned} \\] The extra \\(s^2\\) in the calculation of prediction variance comes from the uncertainty associated with individual observations. 5.3.7 Confidence Interval in R predict(IC_Model, newdata=data.frame(time=1.5), interval = &quot;confidence&quot;, level=0.95) ## fit lwr upr ## 1 2.916965 2.523728 3.310201 We are 95% confident that the mean amount of ice cream dispensed when the dispensor is held for 1.5 seconds is between 2.52 and 3.31 oz. 5.3.8 Prediction Interval in R predict(IC_Model, newdata=data.frame(time=1.5), interval = &quot;prediction&quot;, level=0.95) ## fit lwr upr ## 1 2.916965 1.862832 3.971097 We are 95% confident that in individual who holds the dispensor for 1.5 seconds will get between 1.86 and 3.97 oz of ice cream. 5.3.9 Confidence and Prediction Intervals The prediction interval (in red) is wider than the confidence interval (in blue), since it must account for variability between individuals, in addition to sampling variability. 5.3.10 Confidence and Prediction Bands 5.3.11 Calculations (Optional) In simple linear regression, \\[ \\begin{aligned} SE(\\hat{Y}|X=x^*) = \\sqrt{\\frac{1}{n}+ \\frac{(x^*-\\bar{x})^2}{\\displaystyle\\sum_{i=1}^n(x_i-\\bar{x})^2}} \\end{aligned} \\] Thus a confidence interval for \\(E(Y | (X=x))\\) is: \\[ \\begin{aligned} &amp; b_0+b_1x^* \\pm t^*SE(\\hat{Y}|X=x^*) \\\\ &amp; = b_0+b_1x^* \\pm 2s\\sqrt{\\frac{1}{n}+ \\frac{(x^*-\\bar{x})^2}{\\displaystyle\\sum_{i=1}^n(x_i-\\bar{x})^2}} \\ \\end{aligned} \\] A prediction interval for \\(E(Y | (X=x))\\) is: \\[\\beta_0 + \\beta_1x^* \\pm t^* s\\sqrt{\\left(\\frac{1}{n}+ \\frac{(x^*-\\bar{x})^2}{\\displaystyle\\sum_{i=1}^n(x_i-\\bar{x})^2}\\right) + 1} \\] Calculations in Icecream example For \\(x=1.5\\), a confidence interval is: \\[ \\begin{aligned} &amp; b_0+b_1x^* \\pm t^*SE(\\hat{Y}|X=x^*) \\\\ &amp; = b_0+b_1x^* \\pm 2s\\sqrt{\\frac{1}{n}+ \\frac{(x^*-\\bar{x})^2}{\\displaystyle\\sum_{i=1}^n(x_i-\\bar{x})^2}} \\\\ &amp; = -0.1299087 + 2.0312489 \\pm 20.4527185 \\sqrt{\\frac{1}{15}+ \\frac{(1.5-2.3733)^2}{8.02933}} \\end{aligned} \\] A prediction interval is: \\[ \\begin{aligned} &amp; b_0+b_1x^* \\pm t^*SE(\\hat{Y}|X=x^*) \\\\ &amp; = b_0+b_1x^* \\pm 2s\\sqrt{\\frac{1}{n}+ \\frac{(x^*-\\bar{x})^2}{\\displaystyle\\sum_{i=1}^n(x_i-\\bar{x})^2}} \\\\ &amp; = -0.1299087 + 2.0312489 \\pm 20.4527185 \\sqrt{\\left(\\frac{1}{15}+ \\frac{(1.5-2.3733)^2}{8.02933}\\right)+1} \\end{aligned} \\] 5.3.12 Intervals in Cars Model What is a reasonable range for the average price of all new 2015 cars that can accelerate from 0 to 60 mph in 7 seconds? If a car I am looking to buy can accelerate from 0 to 60 mph in 7 seconds, what price range should I expect? 5.3.13 Cars Confidence Interval What is a reasonable range for the average price of all new 2015 cars that can accelerate from 0 to 60 mph in 7 seconds? predict(Cars_M_A060, newdata=data.frame(Acc060=7), interval=&quot;confidence&quot;, level=0.95) ## fit lwr upr ## 1 39.5502 37.21856 41.88184 We are 95% confident that the average price of new 2015 cars that accelerate from 0 to 60 mph in 7 seconds is between 37.2 and 41.9 thousand dollars. Note: this is a confidence interval for \\(\\beta_0 -7\\beta_1\\). 5.3.14 Cars Prediction Interval If a car I am looking to buy can accelerate from 0 to 60 mph in 7 seconds, what price range should I expect? predict(Cars_M_A060, newdata=data.frame(Acc060=7), interval=&quot;prediction&quot;, level=0.95) ## fit lwr upr ## 1 39.5502 18.19826 60.90215 We are 95% confident that a single new 2015 car that accelerates from 0 to 60 mph in 7 seconds will cost between 18.2 and 60.9 thousand dollars. 5.3.15 Cars Interval Visualization 5.3.16 Lakes Questions of Interest Calculate an interval that we are 95% confident contains the mean mercury concentration for all lakes in Northern Florida. Do the same for Southern Florida. Calculate an interval that we are 95% confident contains the mean mercury concentration for an individual lake in Northern Florida. Do the same for a lake in Southern Florida. 5.3.17 Lakes Confidence Interval predict(Lakes_M, newdata=data.frame(Location=c(&quot;N&quot;, &quot;S&quot;)), interval=&quot;confidence&quot;, level=0.95) ## fit lwr upr ## 1 0.4245455 0.3137408 0.5353501 ## 2 0.6965000 0.5541689 0.8388311 We are 95% confident that the mean mercury level in North Florida is between 0.31 and 0.54 ppm. We are 95% confident that the mean mercury level in South Florida is between 0.55 and 0.84 ppm. Note: these are confidence intervals for \\(\\beta_0\\), and \\(\\beta_0 + \\beta_1\\), respectively. 5.3.18 Lakes Prediction Interval predict(Lakes_M, newdata=data.frame(Location=c(&quot;N&quot;, &quot;S&quot;)), interval=&quot;prediction&quot;, level=0.95) ## fit lwr upr ## 1 0.4245455 -0.22155101 1.070642 ## 2 0.6965000 0.04425685 1.348743 We are 95% confident that an individual lake in North Florida will have mercury level between 0 and 1.07 ppm. We are 95% confident that the mean mercury level in South Florida is between 0.04 and 1.35 ppm. Note that the normality assumption, which allows for negative mercury levels leads to a somewhat nonsensical result. 5.4 Regression Model Assumptions 5.4.1 What We’re Assuming Let’s think carefully about what we are assuming in order to use the hypothesis tests and confidence intervals provided in R. The statement \\(Y_i = \\beta_0 + \\beta_1X_{i1}+ \\ldots + \\beta_pX_{ip} + \\epsilon_i\\), with \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\) implies the following: Linearity: the expected value of \\(Y\\) is a linear function of \\(X_1, X_2, \\ldots, X_p\\). Normality: Given the values of \\(X_1, X_2, \\ldots, X_p\\), \\(Y\\) follows a normal distribution. Constant Variance: Regardless of the values of \\(X_1, X_2, \\ldots, X_p\\), the variance (or standard deviation) in the normal distribution for \\(Y\\) is the same. Independence: each observation is independent of the rest. 5.4.2 Illustration of Model Assumptions We know that these assumptions held true in the ice cream example, because we generated the data in a way that was consistent with these. In practice, we will have only the data, without knowing the exact mechanism that produced it. We should only rely on the t-distribution based p-values and confidence intervals in the R output if these appear to be reasonable assumptions. Of course, these assumptions will almost never be truly satisfied, but they should at least be a reasonable approximation if we are to draw meaningful conclusions. 5.4.3 Checking Model Assumptions The following plots are useful when assessing the appropriateness of the normal error regression model. Scatterplot of residuals against predicted values Histogram of standardized residuals heavy skewness indicates a problem with normality assumption Normal quantile plot severe departures from diagonal line indicate problem with normality assumption 5.4.4 Residual vs Predicted Plots Useful for detecting issues with the linearity or constant variance assumption. curvature indicates a problem with linearity assumption “funnel” or “megaphone” shape indicates problem with constant variance assumption P1 &lt;- ggplot(data=Violations, aes(y=no_viol_Model$residuals, x=no_viol_Model$fitted.values)) + geom_point() + ggtitle(&quot;No Violation&quot;) + xlab(&quot;Predicted Values&quot;) + ylab(&quot;Residuals&quot;) P2 &lt;- ggplot(data=Violations, aes(y=lin_viol_Model$residuals, x=no_viol_Model$fitted.values)) + geom_point() + ggtitle(&quot;Violation of Linearity Assumption&quot;)+ xlab(&quot;Predicted Values&quot;) + ylab(&quot;Residuals&quot;) P3 &lt;- ggplot(data=Violations, aes(y=cvar_viol_Model$residuals, x=no_viol_Model$fitted.values)) + geom_point() + ggtitle(&quot;Violation of Constant Variance Assumption&quot;)+ xlab(&quot;Predicted Values&quot;) + ylab(&quot;Residuals&quot;) grid.arrange(P1, P2, P3, ncol=3) If there is only one explanatory variable, plotting the residuals against that variable reveals the same information. 5.4.5 Histogram of Residuals Useful for assessing normality assumption. Severe skewness indicates violation of normality assumption P1 &lt;- ggplot(data=Violations, aes(x=no_viol_Model$residuals)) + geom_histogram() + ggtitle(&quot;No Violation&quot;) +xlab(&quot;Residual&quot;) P2 &lt;- ggplot(data=Violations, aes(x=norm_viol_Model$residuals)) + geom_histogram() + ggtitle(&quot;Violation of Normality Assumption&quot;) + xlab(&quot;Residual&quot;) grid.arrange(P1, P2, ncol=2) 5.4.6 Normal Quantile-Quantile (QQ) Plot Sometimes histograms can be inconclusive, especially when sample size is smaller. A Normal quantile-quantile plot displays quantiles of the residuals against the expected quantiles of a normal distribution. Severe departures from diagonal line indicate a problem with normality assumption. P1 &lt;- ggplot(data=Violations, aes(sample = scale(no_viol_Model$residuals))) + stat_qq() + stat_qq_line() + xlab(&quot;Normal Quantiles&quot;) + ylab(&quot;Residual Quantiles&quot;) + ggtitle(&quot;No Violation&quot;) + xlim(c(-4,4)) + ylim(c(-4,4)) P2 &lt;- ggplot(data=Violations, aes(sample = scale(norm_viol_Model$residuals))) + stat_qq() + stat_qq_line() + xlab(&quot;Normal Quantiles&quot;) + ylab(&quot;Residual Quantiles&quot;) + ggtitle(&quot;Violation of Normality Assumption&quot;) + xlim(c(-4,4)) + ylim(c(-4,4)) grid.arrange(P1, P2, ncol=2) 5.4.7 Checking Model Assumptions - Independence Independence is often difficult to assess through plots of data, but it is important to think about whether there were factors in the data collection that would cause some observations to be more highly correlated than others. For example: People in the study who are related. Some plants grown in the same greenhouse and others in different greenhouses. Some observations taken in same time period and others at different times. All of these require more complicated models that account for correlation using spatial and time structure. 5.4.8 Summary of Checks for Model Assumptions Model assumption How to detect violation Linearity Curvature in residual plot Constant Variance Funnel shape in residual plot Normality Skewness is histogram of residuals or departure from diag. line in QQ plot Independence No graphical check, carefully examine data collection 5.4.9 Impact of Model Assumption Violations Model assumption Impact Linearity predictions and intervals unreliable Constant Variance predictions still reliable; some intervals will be too wide and others too narrow. Normality predictions still reliable; intervals will be symmetric when they shouldn’t be Independence predictions unreliable and intervals unreliable 5.4.10 Cars Model Assumptions Recall the regression line estimating the relationship between a car’s price and acceleration time. This line was calculated using a sample of 110 cars, released in 2015. \\(\\text{Price}_i = \\beta_0 + \\beta_1\\times\\text{Acc. Time}_i + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\\). The model assumes: Linearity: the expected price of a car is a linear function of its acceleration time. Normality: for any given acceleration time, the prices of actual cars follow a normal distribution. For example the distribution of prices for all cars that accelerate from 0 to 60 in 8 seconds is normal, and so is the distribution of prices of cars that accelerate from 0 to 60 in 10 seconds (though these normal distributions have different means.) Constant Variance: the normal distribution for prices is the same for all acceleration times. Independence: no two cars are any more alike than any others. We should only use the p-values and confidence intervals provided by R, which depend on the t-distribution approximation, if we believe these assumptions are reasonable. 5.4.11 Cars Assumptions Check P1 &lt;- ggplot(data=Cars2015, aes(y=Cars_M_A060$residuals, x=Cars_M_A060$fitted.values)) + geom_point() + ggtitle(&quot;Cars Model Residual Plot&quot;) + xlab(&quot;Predicted Values&quot;) + ylab(&quot;Residuals&quot;) P2 &lt;- ggplot(data=Cars2015, aes(x=Cars_M_A060$residuals)) + geom_histogram() + ggtitle(&quot;Histogram of Residuals&quot;) + xlab(&quot;Residual&quot;) P3 &lt;- ggplot(data=Cars2015, aes(sample = scale(Cars_M_A060$residuals))) + stat_qq() + stat_qq_line() + xlab(&quot;Normal Quantiles&quot;) + ylab(&quot;Residual Quantiles&quot;) + ggtitle(&quot;Cars Model QQ Plot&quot;) grid.arrange(P1, P2, P3, ncol=3) There is a funnel-shape in the residual plot, indicating a concern about the constant variance assumption. There appears to be more variability in prices for more expensive cars than for cheaper cars. There is also some concern about the normality assumption, as the histogram and QQ plot indicate right-skew in the residuals. 5.4.12 Model for Mercury Florida Lakes Recall our sample of 53 Florida Lakes, 33 in the north, and 20 in the south. \\(\\text{Mercury}_i = \\beta_0 + \\beta_1\\times\\text{I}_{\\text{South}_i} + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0, \\sigma)\\). LakesBP 5.4.13 Lakes Model Assumptions Linearity: there is an expected mercury concentration for lakes in North Florida, and another for lakes in South Florida. Normality: mercury concentrations of individual lakes in the north are normally distributed, and so are mercury concentrations in the south. These normal distributions might have different means. Constant Variance: the normal distribution for mercury concentrations in North Florida has the same standard deviation as the normal distribution for mercury concentrations in South Florida Independence: no two lakes are any more alike than any others. We might have concerns about this, do to some lakes being geographically closer to each other than others. We should only use the p-values and confidence intervals provided by R, which depend on the t-distribution approximation, if we believe these assumptions are reasonable. 5.4.14 Lakes Assumptions Check P1 &lt;- ggplot(data=FloridaLakes, aes(y=Lakes_M$residuals, x=Lakes_M$fitted.values)) + geom_point() + ggtitle(&quot;Lakes Model Residual Plot&quot;) + xlab(&quot;Predicted Values&quot;) + ylab(&quot;Residuals&quot;) P2 &lt;- ggplot(data=FloridaLakes, aes(x=Lakes_M$residuals)) + geom_histogram() + ggtitle(&quot;Lakes of Residuals&quot;) + xlab(&quot;Residual&quot;) P3 &lt;- ggplot(data=FloridaLakes, aes(sample = scale(Lakes_M$residuals))) + stat_qq() + stat_qq_line() + xlab(&quot;Normal Quantiles&quot;) + ylab(&quot;Residual Quantiles&quot;) + ggtitle(&quot;Lakes Model QQ Plot&quot;) grid.arrange(P1, P2, P3, ncol=3) Notice that we see two lines of predicted values and residuals. This makes sense since all lakes in North Florida will have the same predicted value, as will all lakes in Southern Florida. There appears to be a little more variability in residuals for Southern Florida (on the right), than Northern Florida, causing some concern about the constant variance assumption. Overall, though, the assumptions seem mostly reasonable. 5.4.15 General Comments on Model Assumptions We shouldn’t think about model assumptions being satisfied as a yes/no question. In reality assumptions are never perfectly satisfied, so it’s a question of how severe violations must be in order to impact results. This is context dependent. A model might be reasonable for certain purposes (i.e. confidence interval for \\(\\beta_1\\)) but not for others (i.e. prediction of response value for new observation). When model assumptions are a concern, consider a using a transformation of the data or a more flexible technique, such as a nonparametric method or statistical machine learning algorithm. We’ll talk more about these soon. Remember that all statistical techniques are approximations 5.5 Transformations 5.5.1 Cars Assumptions Check P1 &lt;- ggplot(data=Cars2015, aes(y=Cars_M_A060$residuals, x=Cars_M_A060$fitted.values)) + geom_point() + ggtitle(&quot;Cars Model Residual Plot&quot;) + xlab(&quot;Predicted Values&quot;) + ylab(&quot;Residuals&quot;) P2 &lt;- ggplot(data=Cars2015, aes(x=Cars_M_A060$residuals)) + geom_histogram() + ggtitle(&quot;Histogram of Residuals&quot;) + xlab(&quot;Residual&quot;) P3 &lt;- ggplot(data=Cars2015, aes(sample = scale(Cars_M_A060$residuals))) + stat_qq() + stat_qq_line() + xlab(&quot;Normal Quantiles&quot;) + ylab(&quot;Residual Quantiles&quot;) + ggtitle(&quot;Cars Model QQ Plot&quot;) grid.arrange(P1, P2, P3, ncol=3) There is a funnel-shape in the residual plot, indicating a concern about the constant variance assumption. There appears to be more variability in prices for more expensive cars than for cheaper cars. There is also some concern about the normality assumption, as the histogram and QQ plot indicate right-skew in the residuals. 5.5.2 Confidence Interval for \\(\\beta_1\\) in Cars Example confint(Cars_M_A060, level=0.95) ## 2.5 % 97.5 % ## (Intercept) 79.888995 99.918163 ## Acc060 -8.429027 -5.957651 We are 95% confident that the average price of a new 2015 car decreases between 8.43 and 5.96 thousand dollars for each additional second it takes to accelerate from 0 to 60 mph. Bootstrap Confidence Interval for \\(\\beta_1\\): q.025 &lt;- quantile(Cars_Bootstrap_Results_Acc060$b1, 0.025) q.975 &lt;- quantile(Cars_Bootstrap_Results_Acc060$b1, 0.975) c(q.025, q.975) ## 2.5% 97.5% ## -8.797594 -5.696359 The bootstrap confidence interval is slightly wider than the one based on the t-approximation. This difference can be attributed to the questions about the constant variance and normality assumptions. 5.5.3 Confidence and Prediction Intervals predict(Cars_M_A060, newdata=data.frame(Acc060=7), interval=&quot;confidence&quot;) ## fit lwr upr ## 1 39.5502 37.21856 41.88184 predict(Cars_M_A060, newdata=data.frame(Acc060=10), interval=&quot;confidence&quot;) ## fit lwr upr ## 1 17.97018 14.71565 21.22472 We are 95% confident that the mean price for all cars that can accelerate from 0 to 60 mph in 7 seconds is between 37.2 and 41.9 thousand dollars. We are 95% confident that the mean price for all cars that can accelerate from 0 to 60 mph in 10 seconds is between 14.7 and 22.2 thousand dollars. 5.5.4 Prediction Intervals for Expected Price Given Acc060 predict(Cars_M_A060, newdata=data.frame(Acc060=7), interval=&quot;prediction&quot;) ## fit lwr upr ## 1 39.5502 18.19826 60.90215 predict(Cars_M_A060, newdata=data.frame(Acc060=10), interval=&quot;prediction&quot;) ## fit lwr upr ## 1 17.97018 -3.502148 39.44252 We are 95% confident that a single car that can accelerate from 0 to 60 mph in 7 seconds will cost between 18.2 thousand and 60.9 thousand dollars. We are 95% confident that a single car that can accelerate from 0 to 60 mph in 10 seconds will cost between 0 thousand and 39.4 thousand dollars. 5.5.5 Confidence and Prediction Interval Illustration 5.5.6 Concerns about Intervals and Model Assumptions The confidence and prediction intervals for cars that take 7 and 10 seconds to accelerate have similar widths. This seems inconsistent with the data, which showed more variability about prices for more expensive cars than less expensive ones. - The intervals are computed using same value for \\(s\\), which is a result of the constant variance assumption. Our residual plot showed us this assumption might not be valid in this situation. The confidence and prediction intervals are symmetric about the expected price, even though the distribution of residuals was right-skewed. - This is the result of the normality assumption, which our histogram and QQ-plot showed might not be valid here. Since we had concerns about the model assumptions, the intervals might not be reliable. We saw that the confidence interval for \\(\\beta_1\\) differed somewhat, but not terribly, from the one produced via Bootstrap. It is harder to tell the degree to which the confidence and prediction intervals for price for a given acceleration time might be off, but we should treat these with caution. 5.5.7 Modeling Log Price When residual plots yield model inadequacy, we might try to correct these by applying a transformation to the response variable. When working a nonnegative, right-skewed response variable, it is often helpful to work with the logarithm of the response variable. Note: In R, log() denotes the natural (base e) logarithm, often denoted ln(). We can actually use any logarithm, but the natural logarithm is commonly used. 5.5.8 Plot of LogPrice and Acc060 ggplot(data=Cars2015, aes(x=Acc060, y=log(LowPrice))) + geom_point() + xlab(&quot;Acceleration Time&quot;) + ylab(&quot;Log of Price&quot;) + ggtitle(&quot;Acceleration Time and Log Price&quot;) + stat_smooth(method=&quot;lm&quot;, se=FALSE) 5.5.9 Model for Log Transform Cars_M_Log &lt;- lm(data=Cars2015, log(LowPrice)~Acc060) summary(Cars_M_Log) ## ## Call: ## lm(formula = log(LowPrice) ~ Acc060, data = Cars2015) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.84587 -0.19396 0.00908 0.18615 0.53350 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.13682 0.13021 39.45 &lt;0.0000000000000002 *** ## Acc060 -0.22064 0.01607 -13.73 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.276 on 108 degrees of freedom ## Multiple R-squared: 0.6359, Adjusted R-squared: 0.6325 ## F-statistic: 188.6 on 1 and 108 DF, p-value: &lt; 0.00000000000000022 5.5.10 LogPrice Model: What We’re Assuming Linearity: the log of expected price of a car is a linear function of its acceleration time. Normality: for any given acceleration time, the log of prices of actual cars follow a normal distribution. Constant Variance: the normal distribution for log of price is the same for all acceleration times. Independence: no two cars are any more alike than any others. We should only use the p-values and confidence intervals provided by R, which depend on the t-distribution approximation, if we believe these assumptions are reasonable. 5.5.11 Model Assumption Check for Transformed Model P1 &lt;- ggplot(data=Cars2015, aes(y=Cars_M_Log$residuals, x=Cars_M_Log$fitted.values)) + geom_point() + ggtitle(&quot;Cars Log Model Residual Plot&quot;) + xlab(&quot;Predicted Values&quot;) + ylab(&quot;Residuals&quot;) P2 &lt;- ggplot(data=Cars2015, aes(x=Cars_M_Log$residuals)) + geom_histogram() + ggtitle(&quot;Histogram of Residuals&quot;) + xlab(&quot;Residual&quot;) P3 &lt;- ggplot(data=Cars2015, aes(sample = scale(Cars_M_Log$residuals))) + stat_qq() + stat_qq_line() + xlab(&quot;Normal Quantiles&quot;) + ylab(&quot;Residual Quantiles&quot;) + ggtitle(&quot;Cars Model QQ Plot&quot;) grid.arrange(P1, P2, P3, ncol=3) There is still some concern about constant variance, though perhaps not as much. The normality assumption appears more reasonable. 5.5.12 Model for Log of Car Price \\[ \\widehat{\\text{Log Price}} = b_0 + b_1\\times \\text{Acc060} \\] Thus \\[ \\begin{aligned} \\widehat{\\text{Price}} &amp; = e^{b_0 + b_1\\times \\text{Acc060} } \\\\ &amp; e^{b_0}e^{b_1 \\times \\text{Acc060}} \\\\ &amp; e^{b_0}(e^{b_1})^\\text{Acc060} \\end{aligned} \\] 5.5.13 Log Model Predictions Prediction Equation: \\[ \\begin{aligned} \\widehat{\\text{Price}} &amp; = e^{5.13582}e^{-0.22064 \\times \\text{Acc060}} \\end{aligned} \\] Predicted price for car that takes 7 seconds to accelerate: \\[ \\begin{aligned} \\widehat{\\text{Price}} &amp; = e^{5.13582}e^{-0.22064 \\times \\text{7}} = 36.3 \\end{aligned} \\] Predicted price for car that takes 10 seconds to accelerate: \\[ \\begin{aligned} \\widehat{\\text{Price}} &amp; = e^{5.13582}e^{-0.22064 \\times \\text{10}}= 18.7 \\end{aligned} \\] Predictions are for log(Price), so we need to exponentiate. predict(Cars_M_Log, newdata=data.frame(Acc060=c(7))) ## 1 ## 3.592343 exp(predict(Cars_M_Log, newdata=data.frame(Acc060=c(7)))) ## 1 ## 36.31908 A car that accelerates from 0 to 60 mph in 7 seconds is expected to cost 36.3 thousand dollars. 5.5.14 Log Model Interpretations \\[ \\begin{aligned} \\widehat{\\text{Price}} &amp; = e^{b_0 + b_1\\times \\text{Acc060} } \\\\ &amp; e^{b_0}e^{b_1 \\times \\text{Acc060}} \\\\ &amp; e^{b_0}(e^{b_1})^\\text{Acc060} \\end{aligned} \\] \\(e^{b_0}\\) is theoretically the expected price of a car that can accelerate from 0 to 60 mph in no time, but this is not a meaningful interpretation. For each additional second it takes a car to accelerate, price is expected to multiply by a factor of \\(e^{b_1}\\). For each additional second in acceleration time, price is expected to multiply by a a factor of \\(e^{-0.22} = 0.80\\). Thus, each 1-second increase in acceleration time is estimated to be associated with a 20% drop in price, on average. 5.5.15 Log Model CI for \\(\\beta_0\\), \\(\\beta_1\\) confint(Cars_M_Log) ## 2.5 % 97.5 % ## (Intercept) 4.8787105 5.3949208 ## Acc060 -0.2524862 -0.1887916 We are 95% confident that the price of a car changes, on average, by multiplicative factor between \\(e^{-0.252} = 0.7773\\) and \\(e^{-0.189}=0.828\\) for each additional second in acceleration time. That is, we believe the price decreases between 17% and 23% on average for each additional second in acceleration time. 5.5.16 Log Model CI for Expected Response predict(Cars_M_Log, newdata=data.frame(Acc060=c(7)), interval=&quot;confidence&quot;) ## fit lwr upr ## 1 3.592343 3.53225 3.652436 exp(predict(Cars_M_Log, newdata=data.frame(Acc060=c(7)), interval=&quot;confidence&quot;)) ## fit lwr upr ## 1 36.31908 34.20083 38.56852 We are 95% confident that the mean price amoung all cars that accelerate from 0 to 60 mph in 7 seconds is between \\(e^{3.53225} =34.2\\) and \\(e^{3.652436}=38.6\\) thousand dollars. 5.5.17 Log Model Prediction Interval predict(Cars_M_Log, newdata=data.frame(Acc060=c(7)), interval=&quot;prediction&quot;) ## fit lwr upr ## 1 3.592343 3.042041 4.142645 exp(predict(Cars_M_Log, newdata=data.frame(Acc060=c(7)), interval=&quot;prediction&quot;)) ## fit lwr upr ## 1 36.31908 20.94796 62.96917 We are 95% confident that the expected price for a car that accelerates from 0 to 60 mph in 7 seconds is between \\(e^{3.04} =20.9\\) and \\(e^{4.14}=63.9\\) thousand dollars. 5.5.18 Confidence Interval Comparison 95% Confidence interval for average price of cars that take 7 seconds to accelerate: Original Model: predict(Cars_M_A060, newdata=data.frame(Acc060=7), interval=&quot;confidence&quot;, level=0.95) ## fit lwr upr ## 1 39.5502 37.21856 41.88184 Transformed Model: exp(predict(Cars_M_Log, newdata=data.frame(Acc060=c(7)), interval=&quot;confidence&quot;, level=0.95)) ## fit lwr upr ## 1 36.31908 34.20083 38.56852 5.5.19 Prediction Interval Comparison 95% Prediction interval for price of an individual car that takes 7 seconds to accelerate: Original Model: predict(Cars_M_A060, newdata=data.frame(Acc060=7), interval=&quot;prediction&quot;, level=0.95) ## fit lwr upr ## 1 39.5502 18.19826 60.90215 Transformed Model: exp(predict(Cars_M_Log, newdata=data.frame(Acc060=c(7)), interval=&quot;prediction&quot;, level=0.95)) ## fit lwr upr ## 1 36.31908 20.94796 62.96917 Notice that the transformed interval is not symmetric and allows for a longer “tail” on the right than the left. 5.5.20 Log Model Visualization The log model suggests an nonlinear trend in price with respect to acceleration time and gives wider confidence and prediction intervals for cars that accelerate faster and tend to be more expensive. It also gives non-symmetric intervals. These results appear to be consistent with the observed data. 5.5.21 Comments on Transformations We could have used another transformation, such as \\(\\sqrt{\\text{Price}}\\) The log tranform leads to a nice interpretation involving percent change. Other transformations might yield better predictions, but are often hard to interpret. There is often a tradeoff between model complexity and interpretability. We’ll talk more about this. We did an example of a transformation in a model with a single explanatory variable. If the explanatory variable is categorical: - \\(e^{b_0}\\) represents the expected response in the baseline category - \\(e^{b_j}\\) represents the number of times larger the expected response in category \\(j\\) is, compared to the baseline category. When working with multiple regression models, it is still important to mention holding other variables constant when interpreting parameters associated with one of the variables. 5.6 Responsible Statistical Inference 5.6.1 Statistical Significance vs Practical Importance “(S)cientists have embraced and even avidly pursued meaningless differences solely because they are statistically significant, and have ignored important effects because they failed to pass the screen of statistical significance…It is a safe bet that people have suffered or died because scientists (and editors, regulators, journalists and others) have used significance tests to interpret results, and have consequently failed to identify the most beneficial courses of action.” -ASA statement on p-values, 2016 5.6.2 What a p-value tells us Performing responsible statistical inference requires understanding what p-values do and do not tell us, and how they should and should not be interpreted. A low p-value tells us that the data we observed are inconsistent with our null hypothesis or some assumption we make in our model. A large p-value tells us that the data we observed could have plausibly been obtained under our supposed model and null hypothesis. A p-value never provides evidence supporting the null hypothesis, it only tells us the strength of evidence against it. A p-value is impacted by the size of the difference between group, or change per unit increase (effect size) the amount of variability in the data the sample size Sometimes, a p-value tells us more about sample size, than relationship we’re actually interested in. A p-value does not tell us the “size” of a difference or effect, or whether it is practically meaningful. 5.6.3 Flights from New York to Chicago A travelor lives in New York and wants to fly to Chicago. They consider flying out of two New York airports: Newark (EWR) LaGuardia (LGA) We have data on the times of flights from both airports to Chicago’s O’Hare airport from 2013 (more than 14,000 flights). Assuming these flights represent a random sample of all flights from these airports to Chicago, consider how the traveler might use this information to decide which airport to fly out of. library(nycflights13) data(flights) flights$origin &lt;- as.factor(flights$origin) flights$dest &lt;- as.factor(flights$dest) We’ll create a dataset containing only flights from Newark and Laguardia to O’Hare, and only the variables we’re interested in. Flights_NY_CHI &lt;- flights %&gt;% filter(origin %in% c(&quot;EWR&quot;, &quot;LGA&quot;) &amp; dest ==&quot;ORD&quot;) %&gt;% select(origin, dest, air_time) 5.6.4 Visualizing New York to Chicago Flights p1 &lt;- ggplot(data=Flights_NY_CHI, aes(x=air_time, fill=origin, color=origin)) + geom_density(alpha=0.2) + ggtitle(&quot;Flight Time&quot;) p2 &lt;- ggplot(data=Flights_NY_CHI, aes(x=air_time, y=origin)) + geom_boxplot() + ggtitle(&quot;Flight Time&quot;) grid.arrange(p1, p2, ncol=2) library(knitr) T &lt;- Flights_NY_CHI %&gt;% group_by(origin) %&gt;% summarize(Mean_Airtime = mean(air_time, na.rm=TRUE), SD = sd(air_time, na.rm=TRUE), n=sum(!is.na(air_time))) kable(T) origin Mean_Airtime SD n EWR 113.2603 9.987122 5828 LGA 115.7998 9.865270 8507 Question: If you were flying from New York to Chicago, would this information influence which airport you would fly out of? If so, which would you be more likely to choose? 5.6.5 Model for Airlines Data M_Flights &lt;- lm(data=Flights_NY_CHI, air_time~origin) summary(M_Flights) ## ## Call: ## lm(formula = air_time ~ origin, data = Flights_NY_CHI) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.26 -7.26 -1.26 5.20 84.74 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 113.2603 0.1299 872.06 &lt;0.0000000000000002 *** ## originLGA 2.5395 0.1686 15.06 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.915 on 14333 degrees of freedom ## (622 observations deleted due to missingness) ## Multiple R-squared: 0.01558, Adjusted R-squared: 0.01551 ## F-statistic: 226.9 on 1 and 14333 DF, p-value: &lt; 0.00000000000000022 5.6.6 Confidence Interval for Flights confint(M_Flights) ## 2.5 % 97.5 % ## (Intercept) 113.00572 113.514871 ## originLGA 2.20905 2.869984 Flights from LGA are estimated to take 2.5 minutes longer than flights from EWR on average. The very low p-value provides strong evidence of a difference in mean flight time. We are 95% confident that flights from LGA to ORD take between 2.2 and 2.9 minutes longer, on average, than flights from EWR to ORD. 5.6.7 Flights Conclusions? Although we have a low p-value, indicating a discernable difference, the size of this difference (2-3 minutes in airtime) is very small. A travelor would most likely have other, more important considerations when deciding which airport to fly from. The low p-value is due to the very large sample size, rather than the size of the difference. Note: there is also some question about whether it is appropriate to use a hypothesis test or confidence interval here at all. We have data on all flights in 2013, so one could argue that we have the entire population already. Perhaps, we could view this as a sample and generalize to flights in other years, though conditions change, so it is not clear that these flights from 2013 would be representative of flights in other years. 5.6.8 Smoking and Birthweight Example We consider data on the relationship between a pregnant mother’s smoking and the birthweight of the baby. Data come from a sample of 80 babies born in North Carolina in 2004. Thirty of the mothers were smokers, and fifty were nonsmokers. p1 &lt;- ggplot(data=NCBirths, aes(x=weight, fill=habit, color=habit)) + geom_density(alpha=0.2) + ggtitle(&quot;Birthweight and Smoking&quot;) p2 &lt;- ggplot(data=NCBirths, aes(x=weight, y=habit)) + geom_boxplot() + ggtitle(&quot;Birthweight and Smoking&quot;) grid.arrange(p1, p2, ncol=2) library(knitr) T &lt;- NCBirths %&gt;% group_by(habit) %&gt;% summarize(Mean_Weight = mean(weight), SD = sd(weight), n=n()) kable(T) habit Mean_Weight SD n nonsmoker 7.039200 1.709388 50 smoker 6.616333 1.106418 30 5.6.9 Model for Birthweight M_Birthwt &lt;- lm(data=NCBirths, weight~habit) summary(M_Birthwt) ## ## Call: ## lm(formula = weight ~ habit, data = NCBirths) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.0392 -0.6763 0.2372 0.8280 2.4437 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.0392 0.2140 32.89 &lt;0.0000000000000002 *** ## habitsmoker -0.4229 0.3495 -1.21 0.23 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.514 on 78 degrees of freedom ## Multiple R-squared: 0.01842, Adjusted R-squared: 0.005834 ## F-statistic: 1.464 on 1 and 78 DF, p-value: 0.23 5.6.10 Conclusions from Birthweight Data confint(M_Birthwt) ## 2.5 % 97.5 % ## (Intercept) 6.613070 7.4653303 ## habitsmoker -1.118735 0.2730012 The average birtweight of babies whose mothers are smokers is estimated to be about 0.42 lbs less than the average birthweight for babies whose mothers are nonsmokers. The large p-value of 0.23, tells us that there is not enough evidence to say that a mother’s smoking is associated with lower birthweights. It is plausible that this difference could have occurred by chance. We are 95% confident that the average birtweight of babies whose mothers are smokers is between 1.12 lbs less and 0.27 lbs more than the average birthweight for babies whose mothers are nonsmokers. Question: Many studies have shown that a mother’s smoking puts a baby at risk of low birthweight. Do our results contradict this research? Should we conclude that smoking has no impact on birthweights? 5.6.11 Impact of Small Sample Size Notice that we observed a difference of about 0.4 lbs. in mean birthweight, which is a considerable difference. The large p-value is mosty due to the relatively small sample size. Even though we observed a mean difference of 0.4 lbs, the sample is to small to allow us to say conclusively that smoking is associated with lower birthweights. This is very different from concluding that smoking does not impact birthweight. This is an example of why we should never “accept the null hypothesis” or say that our data “support the null hypothesis.” 5.6.12 Larger Dataset In fact, this sample of 80 babies is part of a larger dataset, consisting of 1,000 babies born in NC in 2004. When we consider the full dataset, notice that the difference between the groups is similar, but the p-value is much smaller, providing stronger evidence of a relationship between a mother’s smoking and lower birthweight. M_Birthwt_Full &lt;- lm(data=ncbirths, weight~habit) summary(M_Birthwt_Full) ## ## Call: ## lm(formula = weight ~ habit, data = ncbirths) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.1443 -0.7043 0.1657 0.9157 4.6057 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.14427 0.05086 140.472 &lt;0.0000000000000002 *** ## habitsmoker -0.31554 0.14321 -2.203 0.0278 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.503 on 997 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.004846, Adjusted R-squared: 0.003848 ## F-statistic: 4.855 on 1 and 997 DF, p-value: 0.02779 5.6.13 Cautions and Advice p-values are only (a small) part of a statistical analysis. For small samples, real differences might not be statistically significant. -Don’t accept null hypothesis. Gather more information. For large, even very small differences will be statistically significant. -Look at confidence interval. Is difference practically important? When many hypotheses are tested at once (such as many food items) some will produce a significant result just by change. -Use a multiple testing correction, such as Bonferroni Interpret p-values on a “sliding scale” 0.049 is practically the same as 0.051 Is sample representative of larger population? Were treatments randomly assigned (for experiments)? Are there other variables to consider? 5.7 The Regression Effect 5.7.1 The Regression Effect Exam 1 vs Exam 2 scores for intro stat students at another college What is the relationship between scores on the two exams? 5.7.2 The Regression Effect Exam 1 vs Exam 2 scores for intro stat students at another college How many of the 6 students who scored below 70 on Exam 1 improved their scores on Exam 2? How many of the 7 students who scored above 90 improved on Exam 2? 5.7.3 The Regression Effect A low score on an exam is often the result of both poor preparation and bad luck. A high score often results from both good preparation and good luck. While changes in study habits and preparation likely explain some improvement in low scores, we would also expect the lowest performers to improve simply because of better luck. Likewise, some of the highest performers may simply not be as lucky on exam 2, so a small dropoff should not be interpreted as weaker understanding of the exam material. 5.7.4 Simulating Regression Effect set.seed(110322018) Understanding &lt;-rnorm(25, 80, 10) Score1 &lt;- Understanding + rnorm(25, 0, 5) Score2 &lt;- Understanding + rnorm(25, 0, 5) Understanding &lt;- round(Understanding,0) TestSim &lt;- data.frame(Understanding, Score1, Score2) ggplot(data=TestSim, aes(y=Score2, x=Score1))+ geom_point() + stat_smooth(method=&quot;lm&quot;) + geom_abline(slope=1) + geom_text(aes(label=Understanding), vjust = 0, nudge_y = 0.5) This phenomon is called the regression effect. 5.7.5 Test Scores Simulation - Highest Scores kable(head(TestSim%&gt;%arrange(desc(Score1)))) Understanding Score1 Score2 97 98.86412 93.60285 89 98.57157 88.25851 94 97.23330 92.65175 91 93.92857 98.23312 85 93.66503 88.70963 93 92.06243 88.67015 These students’ success on test 1 is due to a strong understanding and good luck. We would expect the understanding to carry over to test 2 (provided the student continues to study in a similar way), but not necessarily the luck. 5.7.6 Test Scores Simulation - Lowest Scores kable(head(TestSim%&gt;%arrange(Score1))) Understanding Score1 Score2 58 54.44354 50.30597 69 59.86641 77.04696 61 61.35228 65.54305 66 65.22433 73.45304 75 65.87041 80.79416 72 69.53082 74.96092 These students’ lack of success on test 1 is due to a low understanding and poor luck. We would expect the understanding to carry over to test 2 (unless the student improves their preparation), but not necessarily the luck. 5.7.7 Another Example Wins by NFL teams in 2017 and 2018 5.7.8 Other Examples of Regression Effect A 1973 article by Kahneman, D. and Tversky, A., “On the Psychology of Prediction,” Pysch. Rev. 80:237-251 describes an instance of the regression effect in the training of Israeli air force pilots. Trainees were praised after performing well and criticized after performing badly. The flight instructors observed that “high praise for good execution of complex maneuvers typically results in a decrement of performance on the next try.” Kahneman and Tversky write that : “We normally reinforce others when their behavior is good and punish them when their behavior is bad. By regression alone, therefore, they [the trainees] are most likely to improve after being punished and most likely to deteriorate after being rewarded. Consequently, we are exposed to a lifetime schedule in which we are most often rewarded for punishing others, and punished for rewarding.” "],["building-models-for-interpretation.html", "Chapter 6 Building Models for Interpretation 6.1 Modeling SAT Score 6.2 Modeling Car Price", " Chapter 6 Building Models for Interpretation Learning Outcomes: Explain the differences in the ways we construct statistical models when we are focused primarily on interpretation. Describe the ways that multicollinearity influences the interpretability of regression models. Recognize situations where confounding and Simpson’s paradox might influence conclusions we draw from a model, and make appropriate interpretations in these situations. Evaluate the appropriateness of models using plots of residuals vs explanatory variables. Recognize when it is appropriate to use polynomials or other nonlinear functions in a statistical model, and interpret corresponding estimates of regression coefficients. Decide which variables to include in a statistical model, and justify your decision. 6.1 Modeling SAT Score 6.1.1 Overview of Model Building So far, we’ve dealt with models with 2 or fewer variables. Often, we want to use more complex models. We’ll need to decide how many variables to include in the model. This is not an obvious decision, and will be different, depending on the purpose of the model. We’ll also need to make other decisions, such as whether or not to use interaction terms, or transformations. In this chapter, we’ll focus on building models for situations when we want to make interpretations and draw conclusions about relationships between our explanatory and response variables. In Chapter 7, we focus on modeling for the purpose of prediction, when we are not interested in making interpretations or conclusions about relationships between variables. 6.1.2 Considerations in Modeling When building a model for the purpose of consideration, we’ll need to think about things like: which explanatory variables should we include in the model, and how many? should we include any interaction terms? should we use any nonlinear terms? should we use a transformation of the response variable? We’ll go through a couple example to see how we can address these questions in building a model. Keep in mind, there is no single correct model, but there are common characteristics of a good model. While two statisticians might use different models for a given set of data, they will hopefully lead to reasonably similar conclusions if constructed carefully. 6.1.3 SAT Scores Dataset We’ll now look at a dataset containing education data on all 50 states. It includes the following variables. state - a factor with names of each state expend - expenditure per pupil in average daily attendance in public elementary and secondary schools, 1994-95 (in thousands of US dollars) ratio - average pupil/teacher ratio in public elementary and secondary schools, Fall 1994 salary - estimated average annual salary of teachers in public elementary and secondary schools, 1994-95 (in thousands of US dollars) frac - percentage of all eligible students taking the SAT, 1994-95 sat - average total SAT score, 1994-95 glimpse(SAT) ## Rows: 50 ## Columns: 6 ## $ state &lt;fct&gt; Alabama, Alaska, Arizona, Arkansas, California, Colorado, Conne… ## $ expend &lt;dbl&gt; 4.405, 8.963, 4.778, 4.459, 4.992, 5.443, 8.817, 7.030, 5.718, … ## $ ratio &lt;dbl&gt; 17.2, 17.6, 19.3, 17.1, 24.0, 18.4, 14.4, 16.6, 19.1, 16.3, 17.… ## $ salary &lt;dbl&gt; 31.144, 47.951, 32.175, 28.934, 41.078, 34.571, 50.045, 39.076,… ## $ frac &lt;int&gt; 8, 47, 27, 6, 45, 29, 81, 68, 48, 65, 57, 15, 13, 58, 5, 9, 11,… ## $ sat &lt;int&gt; 1029, 934, 944, 1005, 902, 980, 908, 897, 889, 854, 889, 979, 1… 6.1.4 Teacher Salaries and SAT Scores The plot displays average SAT score against average teacher salary for all 50 US states. ggplot(data=SAT, aes(y=sat, x=salary)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) + ggtitle(&quot;Average SAT score vs Average Teacher Salary&quot;) + xlab(&quot;Average Teacher Salary in Thousands&quot;) What conclusion do you draw from the plot? Are these results surprising? 6.1.5 Simple Linear Regression Model SAT_M1 &lt;- lm(data=SAT, sat~salary) summary(SAT_M1) ## ## Call: ## lm(formula = sat ~ salary, data = SAT) ## ## Residuals: ## Min 1Q Median 3Q Max ## -147.125 -45.354 4.073 42.193 125.279 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1158.859 57.659 20.098 &lt; 0.0000000000000002 *** ## salary -5.540 1.632 -3.394 0.00139 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 67.89 on 48 degrees of freedom ## Multiple R-squared: 0.1935, Adjusted R-squared: 0.1767 ## F-statistic: 11.52 on 1 and 48 DF, p-value: 0.001391 6.1.6 A Closer Look Let’s break the data down by the percentage of students who take the SAT. Low = 0%-22% Medium = 22-49% High = 49-81% SAT &lt;- mutate(SAT, fracgrp = cut(frac, breaks=c(0, 22, 49, 81), labels=c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;))) 6.1.7 A Closer Look ggplot(data=SAT, aes( y=sat, x=salary )) +geom_point() + facet_wrap(facets = ~fracgrp) + stat_smooth(method=&quot;lm&quot;, se=FALSE) + xlab(&quot;Average Teacher Salary in Thousands&quot;) Now what conclusions do you draw from the plots? 6.1.8 Multiple Regression Model SAT_M2 &lt;- lm(data=SAT, sat~salary+frac) summary(SAT_M2) ## ## Call: ## lm(formula = sat ~ salary + frac, data = SAT) ## ## Residuals: ## Min 1Q Median 3Q Max ## -78.313 -26.731 3.168 18.951 75.590 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 987.9005 31.8775 30.991 &lt; 0.0000000000000002 *** ## salary 2.1804 1.0291 2.119 0.0394 * ## frac -2.7787 0.2285 -12.163 0.0000000000000004 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 33.69 on 47 degrees of freedom ## Multiple R-squared: 0.8056, Adjusted R-squared: 0.7973 ## F-statistic: 97.36 on 2 and 47 DF, p-value: &lt; 0.00000000000000022 For each one thousand dollar increase in average teacher salary, a state’s average SAT score is expected to increase by 2.18 points, assuming percentage of students taking the test is the same. For each one percent increase in percentage of students taking the SAT, a state’s average score is expected to decrease by 2.78 points, assuming average teacher salary is the same. 6.1.9 Add Other Variables? Let’s see what other possible explanatory variables we might want to add to the model. We’ll SAT_Num &lt;- select_if(SAT, is.numeric) C &lt;- cor(SAT_Num, use = &quot;pairwise.complete.obs&quot;) round(C,2) ## expend ratio salary frac sat ## expend 1.00 -0.37 0.87 0.59 -0.38 ## ratio -0.37 1.00 0.00 -0.21 0.08 ## salary 0.87 0.00 1.00 0.62 -0.44 ## frac 0.59 -0.21 0.62 1.00 -0.89 ## sat -0.38 0.08 -0.44 -0.89 1.00 library(corrplot) corrplot(C) 6.1.10 Add Student-to-Teacher Ratio SAT_M3 &lt;- lm(data=SAT, sat~salary+frac+ratio) summary(SAT_M3) ## ## Call: ## lm(formula = sat ~ salary + frac + ratio, data = SAT) ## ## Residuals: ## Min 1Q Median 3Q Max ## -89.244 -21.485 -0.798 17.685 68.262 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1057.8982 44.3287 23.865 &lt;0.0000000000000002 *** ## salary 2.5525 1.0045 2.541 0.0145 * ## frac -2.9134 0.2282 -12.764 &lt;0.0000000000000002 *** ## ratio -4.6394 2.1215 -2.187 0.0339 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 32.41 on 46 degrees of freedom ## Multiple R-squared: 0.8239, Adjusted R-squared: 0.8124 ## F-statistic: 71.72 on 3 and 46 DF, p-value: &lt; 0.00000000000000022 6.1.11 Add Expendatures SAT_M4 &lt;- lm(data=SAT, sat~salary+frac+ratio+expend) summary(SAT_M4) ## ## Call: ## lm(formula = sat ~ salary + frac + ratio + expend, data = SAT) ## ## Residuals: ## Min 1Q Median 3Q Max ## -90.531 -20.855 -1.746 15.979 66.571 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1045.9715 52.8698 19.784 &lt; 0.0000000000000002 *** ## salary 1.6379 2.3872 0.686 0.496 ## frac -2.9045 0.2313 -12.559 0.000000000000000261 *** ## ratio -3.6242 3.2154 -1.127 0.266 ## expend 4.4626 10.5465 0.423 0.674 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 32.7 on 45 degrees of freedom ## Multiple R-squared: 0.8246, Adjusted R-squared: 0.809 ## F-statistic: 52.88 on 4 and 45 DF, p-value: &lt; 0.00000000000000022 6.1.12 Confidence Intervals Confidence intervals for model involving teacher salary, percentage taking the test, and student-to-teacher ratio. confint(SAT_M3) ## 2.5 % 97.5 % ## (Intercept) 968.6691802 1147.1271438 ## salary 0.5304797 4.5744605 ## frac -3.3727807 -2.4539197 ## ratio -8.9098147 -0.3690414 Confidence intervals for model with above variables plus expendature. confint(SAT_M4) ## 2.5 % 97.5 % ## (Intercept) 939.486374 1152.456698 ## salary -3.170247 6.446081 ## frac -3.370262 -2.438699 ## ratio -10.100417 2.851952 ## expend -16.779204 25.704393 Question What happened to the confidence interval associated with teacher salary? How might we explain this? (Hint: think about how to interpret estimates/confidence intervals in multiple regression) 6.1.13 Residual Plots for SAT 3-variable Model Let’s return to the model with salary, ratio, and fraction taking test. We use residual plots to assess model assumptions. P1 &lt;- ggplot(data=data.frame(SAT_M3$residuals), aes(y=SAT_M3$residuals, x=SAT_M3$fitted.values)) + geom_point() + ggtitle(&quot;Residual Plot&quot;) + xlab(&quot;Predicted Values&quot;) + ylab(&quot;Residuals&quot;) P2 &lt;- ggplot(data=data.frame(SAT_M3$residuals), aes(x=SAT_M3$residuals)) + geom_histogram() + ggtitle(&quot;Histogram of Residuals&quot;) + xlab(&quot;Residual&quot;) P3 &lt;- ggplot(data=data.frame(SAT_M3$residuals), aes(sample = scale(SAT_M3$residuals))) + stat_qq() + stat_qq_line() + xlab(&quot;Normal Quantiles&quot;) + ylab(&quot;Residual Quantiles&quot;) + ggtitle(&quot;QQ Plot&quot;) grid.arrange(P1, P2, P3, ncol=3) There is some sign of a quadratic trend in the residual plot, creating concern about the linearity assumption. 6.1.14 Plots of Residuals Against Predictors We can plot our residuals against the explanatory variables to see whether the model is properly accounting for relationships involving each variable. If we see nonlinear trends, we should consider adding a nonlinear function of that explanatory variable. P1 &lt;- ggplot(data=data.frame(SAT_M3$residuals), aes(y=SAT_M3$residuals, x=SAT_M3$model$salary)) + geom_point() + ggtitle(&quot;Residual by Predicted Plot&quot;) + xlab(&quot;Salary&quot;) + ylab(&quot;Residuals&quot;) P2 &lt;- ggplot(data=data.frame(SAT_M3$residuals), aes(y=SAT_M3$residuals, x=SAT_M3$model$frac)) + geom_point() + ggtitle(&quot;Residual by Predicted Plot&quot;) + xlab(&quot;Fraction Taking Test&quot;) + ylab(&quot;Residuals&quot;) P3 &lt;- ggplot(data=data.frame(SAT_M3$residuals), aes(y=SAT_M3$residuals, x=SAT_M3$model$ratio)) + geom_point() + ggtitle(&quot;Residual by Predicted Plot&quot;) + xlab(&quot;Student to Teach Ratio&quot;) + ylab(&quot;Residuals&quot;) grid.arrange(P1, P2, P3, ncol=3) There is also a quadratic trend in the plot involving the fraction variable. 6.1.15 Model Using Frac^2 SAT_M5 &lt;- lm(data=SAT, sat~salary+frac+I(frac^2)+ratio) summary(SAT_M5) ## ## Call: ## lm(formula = sat ~ salary + frac + I(frac^2) + ratio, data = SAT) ## ## Residuals: ## Min 1Q Median 3Q Max ## -66.09 -15.20 -4.64 15.06 52.77 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1039.21242 36.28206 28.643 &lt; 0.0000000000000002 *** ## salary 1.80708 0.83150 2.173 0.0351 * ## frac -6.64001 0.77668 -8.549 0.0000000000555 *** ## I(frac^2) 0.05065 0.01025 4.942 0.0000111676728 *** ## ratio -0.04058 1.96174 -0.021 0.9836 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 26.38 on 45 degrees of freedom ## Multiple R-squared: 0.8858, Adjusted R-squared: 0.8757 ## F-statistic: 87.28 on 4 and 45 DF, p-value: &lt; 0.00000000000000022 6.1.16 Residual Plots for Quadratic SAT Model P1 &lt;- ggplot(data=data.frame(SAT_M4$residuals), aes(y=SAT_M4$residuals, x=SAT_M5$fitted.values)) + geom_point() + ggtitle(&quot;Residual Plot&quot;) + xlab(&quot;Predicted Values&quot;) + ylab(&quot;Residuals&quot;) P2 &lt;- ggplot(data=data.frame(SAT_M5$residuals), aes(x=SAT_M5$residuals)) + geom_histogram() + ggtitle(&quot;Histogram of Residuals&quot;) + xlab(&quot;Residual&quot;) P3 &lt;- ggplot(data=data.frame(SAT_M5$residuals), aes(sample = scale(SAT_M5$residuals))) + stat_qq() + stat_qq_line() + xlab(&quot;Normal Quantiles&quot;) + ylab(&quot;Residual Quantiles&quot;) + ggtitle(&quot;QQ Plot&quot;) grid.arrange(P1, P2, P3, ncol=3) 6.1.17 Model with Linear Term on Frac summary(SAT_M3) ## ## Call: ## lm(formula = sat ~ salary + frac + ratio, data = SAT) ## ## Residuals: ## Min 1Q Median 3Q Max ## -89.244 -21.485 -0.798 17.685 68.262 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1057.8982 44.3287 23.865 &lt;0.0000000000000002 *** ## salary 2.5525 1.0045 2.541 0.0145 * ## frac -2.9134 0.2282 -12.764 &lt;0.0000000000000002 *** ## ratio -4.6394 2.1215 -2.187 0.0339 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 32.41 on 46 degrees of freedom ## Multiple R-squared: 0.8239, Adjusted R-squared: 0.8124 ## F-statistic: 71.72 on 3 and 46 DF, p-value: &lt; 0.00000000000000022 6.1.18 Interpretations for Model with Linear Terms On average, a $1,000 dollar increase in average teacher salary is associated with a 2.5 point increase in average SAT score assuming fraction of students taking the SAT, and student to teacher ratio are held constant. On average, a 1% increase in percentage of students taking the SAT is associated with a 2.9 point decrease in average SAT score assuming average teacher salary, and student to teacher ratio are held constant. On average, a 1 student per teacher increase in student to teacher ratio is associated with a 4.6 point from in average SAT score, assuming average teacher salary, and percentage of students taking the SAT are held constant. 6.1.19 Model with Quadratic Term on Frac summary(SAT_M5) ## ## Call: ## lm(formula = sat ~ salary + frac + I(frac^2) + ratio, data = SAT) ## ## Residuals: ## Min 1Q Median 3Q Max ## -66.09 -15.20 -4.64 15.06 52.77 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1039.21242 36.28206 28.643 &lt; 0.0000000000000002 *** ## salary 1.80708 0.83150 2.173 0.0351 * ## frac -6.64001 0.77668 -8.549 0.0000000000555 *** ## I(frac^2) 0.05065 0.01025 4.942 0.0000111676728 *** ## ratio -0.04058 1.96174 -0.021 0.9836 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 26.38 on 45 degrees of freedom ## Multiple R-squared: 0.8858, Adjusted R-squared: 0.8757 ## F-statistic: 87.28 on 4 and 45 DF, p-value: &lt; 0.00000000000000022 6.1.20 Interpretations for Model with Quadratic Terms On average, a $1,000 dollar increase in average teacher salary is associated with a 1.8 point increase in average SAT score assuming fraction of students taking the SAT, and student to teacher ratio are held constant. On average, a 1 student per teacher increase in student to teacher ratio is associated with a 0.05 point from in average SAT score, assuming average teacher salary, and percentage of students taking the SAT are held constant. We cannot give a clear interpretation of the fraction variable, since it occurs in both linear and quadratic terms. In fact, the vertex of the parabola given by \\(y=-6.64x + 0.05x^2\\) occurs at \\(x=\\frac{6.64}{2(0.05)}\\approx 66\\). So the model estimates that SAT score decreases in a quadratic fashion with respect to fraction taking the test, until that fraction reaches 66 percent of student, then is expected to increase. 6.1.21 Plot of SAT and Frac ggplot(data=SAT, aes(x=frac, y=sat)) + geom_point() + stat_smooth(se=FALSE) We do see some possible quadratic trend, but we should be really careful about extrapolation. 6.1.22 SAT Model Summary Modeling SAT scores based on teacher salary alone led to misleading results, due to Simpson’s Paradox. This is corrected by adding percentage of students taking the test to the model. Modeling two highly correlated variables like average teacher salary and expenditure on education inflates the width of confidence intervals associated with both variables, preventing us from drawing meaningful conclusions about either variable. This issue is called multicollinearity. Including a quadratic term on the proportion taking the test improves the model fit, and validity of model assumptions, but also makes the model harder to interpret. We need to use judgement when deciding whether or not to include quadratic or higher power terms. There is no clear reason to expect an interaction between these variables, so we did not include an interaction effect in the model. 6.2 Modeling Car Price 6.2.1 Model for Price of 2015 Cars What factors contribute to the price of a car? We build a model for the price of a new 2015 car, in order to help us answer this question. data(Cars2015) glimpse(Cars2015) ## Rows: 110 ## Columns: 20 ## $ Make &lt;fct&gt; Chevrolet, Hyundai, Kia, Mitsubishi, Nissan, Dodge, Chevrole… ## $ Model &lt;fct&gt; Spark, Accent, Rio, Mirage, Versa Note, Dart, Cruze LS, 500L… ## $ Type &lt;fct&gt; Hatchback, Hatchback, Sedan, Hatchback, Hatchback, Sedan, Se… ## $ LowPrice &lt;dbl&gt; 12.270, 14.745, 13.990, 12.995, 14.180, 16.495, 16.170, 19.3… ## $ HighPrice &lt;dbl&gt; 25.560, 17.495, 18.290, 15.395, 17.960, 23.795, 25.660, 24.6… ## $ Drive &lt;fct&gt; FWD, FWD, FWD, FWD, FWD, FWD, FWD, FWD, FWD, FWD, FWD, AWD, … ## $ CityMPG &lt;int&gt; 30, 28, 28, 37, 31, 23, 24, 24, 28, 30, 27, 27, 25, 27, 30, … ## $ HwyMPG &lt;int&gt; 39, 37, 36, 44, 40, 35, 36, 33, 38, 35, 33, 36, 36, 37, 39, … ## $ FuelCap &lt;dbl&gt; 9.0, 11.4, 11.3, 9.2, 10.9, 14.2, 15.6, 13.1, 12.4, 11.1, 11… ## $ Length &lt;int&gt; 145, 172, 172, 149, 164, 184, 181, 167, 179, 154, 156, 180, … ## $ Width &lt;int&gt; 63, 67, 68, 66, 67, 72, 71, 70, 72, 67, 68, 69, 70, 68, 69, … ## $ Wheelbase &lt;int&gt; 94, 101, 101, 97, 102, 106, 106, 103, 104, 99, 98, 104, 104,… ## $ Height &lt;int&gt; 61, 57, 57, 59, 61, 58, 58, 66, 58, 59, 58, 58, 57, 58, 59, … ## $ UTurn &lt;int&gt; 34, 37, 37, 32, 37, 38, 38, 37, 39, 34, 35, 38, 37, 36, 37, … ## $ Weight &lt;int&gt; 2345, 2550, 2575, 2085, 2470, 3260, 3140, 3330, 2990, 2385, … ## $ Acc030 &lt;dbl&gt; 4.4, 3.7, 3.5, 4.4, 4.0, 3.4, 3.7, 3.9, 3.4, 3.9, 3.9, 3.7, … ## $ Acc060 &lt;dbl&gt; 12.8, 10.3, 9.5, 12.1, 10.9, 9.3, 9.8, 9.5, 9.2, 10.8, 11.1,… ## $ QtrMile &lt;dbl&gt; 19.4, 17.8, 17.3, 19.0, 18.2, 17.2, 17.6, 17.4, 17.1, 18.3, … ## $ PageNum &lt;int&gt; 123, 148, 163, 188, 196, 128, 119, 131, 136, 216, 179, 205, … ## $ Size &lt;fct&gt; Small, Small, Small, Small, Small, Small, Small, Small, Smal… Cars2015 &lt;- Cars2015 %&gt;% select(-HighPrice) 6.2.2 Categorical Variables Cars_Cat &lt;- select_if(Cars2015, is.factor) summary(Cars_Cat) ## Make Model Type Drive Size ## Chevrolet: 8 CTS : 2 7Pass :15 AWD:25 Large :29 ## Ford : 7 2 Touring : 1 Hatchback:11 FWD:63 Midsized:34 ## Hyundai : 7 200 : 1 Sedan :46 RWD:22 Small :47 ## Toyoto : 7 3 i Touring: 1 Sporty :11 ## Audi : 6 3 Series GT: 1 SUV :18 ## Nissan : 6 300 : 1 Wagon : 9 ## (Other) :69 (Other) :103 6.2.3 Correlation Matrix We examine the correlation matrix of quantitative variables. Cars_Num &lt;- select_if(Cars2015, is.numeric) C &lt;- cor(Cars_Num, use = &quot;pairwise.complete.obs&quot;) round(C,2) ## LowPrice CityMPG HwyMPG FuelCap Length Width Wheelbase Height UTurn ## LowPrice 1.00 -0.65 -0.59 0.57 0.47 0.48 0.46 0.02 0.40 ## CityMPG -0.65 1.00 0.93 -0.77 -0.72 -0.78 -0.69 -0.39 -0.73 ## HwyMPG -0.59 0.93 1.00 -0.75 -0.64 -0.75 -0.64 -0.54 -0.68 ## FuelCap 0.57 -0.77 -0.75 1.00 0.82 0.85 0.79 0.58 0.76 ## Length 0.47 -0.72 -0.64 0.82 1.00 0.81 0.92 0.46 0.84 ## Width 0.48 -0.78 -0.75 0.85 0.81 1.00 0.76 0.62 0.77 ## Wheelbase 0.46 -0.69 -0.64 0.79 0.92 0.76 1.00 0.49 0.81 ## Height 0.02 -0.39 -0.54 0.58 0.46 0.62 0.49 1.00 0.55 ## UTurn 0.40 -0.73 -0.68 0.76 0.84 0.77 0.81 0.55 1.00 ## Weight 0.55 -0.83 -0.84 0.91 0.82 0.91 0.81 0.71 0.80 ## Acc030 -0.76 0.64 0.51 -0.47 -0.38 -0.41 -0.31 0.21 -0.36 ## Acc060 -0.74 0.68 0.52 -0.49 -0.47 -0.46 -0.38 0.21 -0.41 ## QtrMile -0.76 0.65 0.49 -0.45 -0.42 -0.41 -0.35 0.25 -0.37 ## PageNum -0.23 0.28 0.15 -0.15 -0.23 -0.20 -0.24 0.06 -0.22 ## Weight Acc030 Acc060 QtrMile PageNum ## LowPrice 0.55 -0.76 -0.74 -0.76 -0.23 ## CityMPG -0.83 0.64 0.68 0.65 0.28 ## HwyMPG -0.84 0.51 0.52 0.49 0.15 ## FuelCap 0.91 -0.47 -0.49 -0.45 -0.15 ## Length 0.82 -0.38 -0.47 -0.42 -0.23 ## Width 0.91 -0.41 -0.46 -0.41 -0.20 ## Wheelbase 0.81 -0.31 -0.38 -0.35 -0.24 ## Height 0.71 0.21 0.21 0.25 0.06 ## UTurn 0.80 -0.36 -0.41 -0.37 -0.22 ## Weight 1.00 -0.41 -0.43 -0.39 -0.20 ## Acc030 -0.41 1.00 0.95 0.95 0.25 ## Acc060 -0.43 0.95 1.00 0.99 0.26 ## QtrMile -0.39 0.95 0.99 1.00 0.26 ## PageNum -0.20 0.25 0.26 0.26 1.00 library(corrplot) C &lt;- corrplot(C) We’ll examine what happens when we include two highy-correlated explanatory variables in the same model, for example: Acc060 - time it takes to accelerate from 0 to 60 mph and QtrMile - time it takes to drive a quarter mile cor(Cars2015$Acc060, Cars2015$QtrMile) ## [1] 0.9916625 6.2.4 Model Using Acceleration Time Cars_M1 &lt;- lm(data=Cars2015, log(LowPrice) ~ Acc060) summary(Cars_M1) ## ## Call: ## lm(formula = log(LowPrice) ~ Acc060, data = Cars2015) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.84587 -0.19396 0.00908 0.18615 0.53350 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.13682 0.13021 39.45 &lt;0.0000000000000002 *** ## Acc060 -0.22064 0.01607 -13.73 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.276 on 108 degrees of freedom ## Multiple R-squared: 0.6359, Adjusted R-squared: 0.6325 ## F-statistic: 188.6 on 1 and 108 DF, p-value: &lt; 0.00000000000000022 Confidence Interval for Effect of Acceleration Time: exp(confint(Cars_M1)) ## 2.5 % 97.5 % ## (Intercept) 131.4610408 220.284693 ## Acc060 0.7768669 0.827959 We are 95% confident that a 1-second increase in acceleration time is associated with an average price decrease betweeen 17% and 22.5%. 6.2.5 Model Using Quarter Mile Time Cars_M2 &lt;- lm(data=Cars2015, log(LowPrice) ~ QtrMile) summary(Cars_M2) ## ## Call: ## lm(formula = log(LowPrice) ~ QtrMile, data = Cars2015) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.91465 -0.19501 0.02039 0.17538 0.60073 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.8559 0.3248 24.19 &lt;0.0000000000000002 *** ## QtrMile -0.2776 0.0201 -13.81 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.275 on 108 degrees of freedom ## Multiple R-squared: 0.6385, Adjusted R-squared: 0.6351 ## F-statistic: 190.7 on 1 and 108 DF, p-value: &lt; 0.00000000000000022 Confidence Interval for Effect of Quarter Mile Time: exp(confint(Cars_M2)) ## 2.5 % 97.5 % ## (Intercept) 1355.8297704 4913.077313 ## QtrMile 0.7279941 0.788385 We are 95% confident that a 1-second increase in quarter mile time is associated with a price decrease between 21% and 27%, on average. 6.2.6 Model Using Quarter Mile Time and Acc. Time Cars_M3 &lt;- lm(data=Cars2015, log(LowPrice) ~ QtrMile + Acc060) summary(Cars_M3) ## ## Call: ## lm(formula = log(LowPrice) ~ QtrMile + Acc060, data = Cars2015) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.89124 -0.20030 0.01001 0.17576 0.57462 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.83974 1.54354 4.431 0.0000227 *** ## QtrMile -0.17316 0.15640 -1.107 0.271 ## Acc060 -0.08389 0.12455 -0.673 0.502 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2757 on 107 degrees of freedom ## Multiple R-squared: 0.64, Adjusted R-squared: 0.6332 ## F-statistic: 95.1 on 2 and 107 DF, p-value: &lt; 0.00000000000000022 Confidence Intervals from 2-variable Model exp(confint(Cars_M3)) ## 2.5 % 97.5 % ## (Intercept) 43.8095999 19922.799158 ## QtrMile 0.6168071 1.146686 ## Acc060 0.7183525 1.177065 It does not make sense to talk about holding QtrMile constant as Acc060 increases, or vice-versa. Trying to do so leads to nonsensical answers. We are 95% confident that a 1-second increase in quarter mile time is associated with an average price change between a 38% decrease and 15% increase, assuming acceleration time is held constant. We are 95% confident that a 1-second increase in acceleration time is associated with an average price change between a 28% decrease and 18% increase, assuming quarter mile time is held constant. 6.2.7 Problems with Multicollinearity in Modeling Because these variables are so highly correlated, it the model cannot separate the effect of one from the other, and thus is uncertain about both. Notice the very large standard errors associated with both regression coefficients, which lead to very wide confidence intervals. In fact, if two variables are perfectly correlated, it will be impossible to fit them both in a model, and you will get an error message. 6.2.8 Impact on Prediction Suppose we want to predict the price of a car that can accelerate from 0 to 60 mph in 9.5 seconds, and completes a quarter mile in 17.3 seconds. exp(predict(Cars_M1, newdata = data.frame(Acc060=9.5, QtrMile=17.3))) ## 1 ## 20.92084 exp(predict(Cars_M2, newdata = data.frame(Acc060=9.5, QtrMile=17.3))) ## 1 ## 21.18223 exp(predict(Cars_M3, newdata = data.frame(Acc060=9.5, QtrMile=17.3))) ## 1 ## 21.05489 The predicted values are similar. Multicollinearity does not hurt predictions, only interpretations. 6.2.9 Adding Weight to Model Cars_M4 &lt;- lm(data=Cars2015, log(LowPrice) ~ QtrMile + Weight) summary(Cars_M4) ## ## Call: ## lm(formula = log(LowPrice) ~ QtrMile + Weight, data = Cars2015) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.79365 -0.13931 -0.01368 0.15773 0.42234 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.21326823 0.33491778 18.552 &lt; 0.0000000000000002 *** ## QtrMile -0.22482146 0.01748563 -12.858 &lt; 0.0000000000000002 *** ## Weight 0.00020606 0.00002641 7.803 0.0000000000043 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2206 on 107 degrees of freedom ## Multiple R-squared: 0.7696, Adjusted R-squared: 0.7653 ## F-statistic: 178.7 on 2 and 107 DF, p-value: &lt; 0.00000000000000022 \\(R^2\\) went up from 0.64 to 0.76! 6.2.10 Add Interaction Term? Cars_M5 &lt;- lm(data=Cars2015, log(LowPrice) ~ QtrMile * Weight) summary(Cars_M5) ## ## Call: ## lm(formula = log(LowPrice) ~ QtrMile * Weight, data = Cars2015) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.82013 -0.12076 -0.01464 0.14717 0.41928 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.1114189 1.3270870 3.098 0.00249 ** ## QtrMile -0.0963226 0.0804413 -1.197 0.23381 ## Weight 0.0008110 0.0003707 2.188 0.03089 * ## QtrMile:Weight -0.0000373 0.0000228 -1.636 0.10482 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2188 on 106 degrees of freedom ## Multiple R-squared: 0.7752, Adjusted R-squared: 0.7689 ## F-statistic: 121.9 on 3 and 106 DF, p-value: &lt; 0.00000000000000022 p-value on interaction is not that small. \\(R^2\\) didn’t go up much. Let’s not use it. 6.2.11 Add HWY MPG? Cars_M6 &lt;- lm(data=Cars2015, log(LowPrice) ~ QtrMile + Weight + HwyMPG) summary(Cars_M6) ## ## Call: ## lm(formula = log(LowPrice) ~ QtrMile + Weight + HwyMPG, data = Cars2015) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.82308 -0.14513 -0.01922 0.16732 0.41390 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.54954436 0.42196132 15.522 &lt; 0.0000000000000002 *** ## QtrMile -0.21699008 0.01843615 -11.770 &lt; 0.0000000000000002 *** ## Weight 0.00015922 0.00004456 3.573 0.000532 *** ## HwyMPG -0.00961141 0.00737658 -1.303 0.195410 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2198 on 106 degrees of freedom ## Multiple R-squared: 0.7732, Adjusted R-squared: 0.7668 ## F-statistic: 120.5 on 3 and 106 DF, p-value: &lt; 0.00000000000000022 HwyMPG doesn’t make change \\(R^2\\) much, and has a high correlation with weight. Let’s not include it. 6.2.12 Categorical Variables to Consider Relationship between Price, Size, and Drive P1 &lt;- ggplot(data=Cars2015, aes(x=log(LowPrice), y=Size)) + geom_boxplot() + ggtitle(&quot;Price by Size&quot;) P2 &lt;- ggplot(data=Cars2015, aes(x=log(LowPrice), y=Drive)) + geom_boxplot() + ggtitle(&quot;Price by Drive&quot;) grid.arrange(P1, P2, ncol=2) Information about size is already included, through the weight variable. Let’s add drive type to the model. 6.2.13 Model with QtrMile, Weight, and Drive Cars_M7 &lt;- lm(data=Cars2015, log(LowPrice) ~ QtrMile + Weight + Drive) summary(Cars_M7) ## ## Call: ## lm(formula = log(LowPrice) ~ QtrMile + Weight + Drive, data = Cars2015) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.72386 -0.10882 0.01269 0.13306 0.45304 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.81406789 0.33961789 17.119 &lt; 0.0000000000000002 *** ## QtrMile -0.19007439 0.01959554 -9.700 0.000000000000000289 *** ## Weight 0.00020496 0.00002583 7.936 0.000000000002420675 *** ## DriveFWD -0.22403222 0.05704513 -3.927 0.000154 *** ## DriveRWD -0.13884399 0.06227709 -2.229 0.027913 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2077 on 105 degrees of freedom ## Multiple R-squared: 0.7995, Adjusted R-squared: 0.7919 ## F-statistic: 104.7 on 4 and 105 DF, p-value: &lt; 0.00000000000000022 6.2.14 Add Size Cars_M8 &lt;- lm(data=Cars2015, log(LowPrice) ~ QtrMile + Weight + Drive + Size) summary(Cars_M8) ## ## Call: ## lm(formula = log(LowPrice) ~ QtrMile + Weight + Drive + Size, ## data = Cars2015) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.71092 -0.12126 0.01355 0.11831 0.44439 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.66594310 0.37169625 15.243 &lt; 0.0000000000000002 *** ## QtrMile -0.19256547 0.02000711 -9.625 0.000000000000000505 *** ## Weight 0.00023978 0.00004101 5.847 0.000000059416930182 *** ## DriveFWD -0.21598794 0.05780323 -3.737 0.000306 *** ## DriveRWD -0.15259183 0.06410851 -2.380 0.019142 * ## SizeMidsized 0.04699095 0.06271499 0.749 0.455398 ## SizeSmall 0.08875861 0.08105810 1.095 0.276071 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2085 on 103 degrees of freedom ## Multiple R-squared: 0.8018, Adjusted R-squared: 0.7903 ## F-statistic: 69.46 on 6 and 103 DF, p-value: &lt; 0.00000000000000022 Adding size barely increased \\(R^2\\) at all. We find no evidence of differences in price between the three sizes, after accounting for the other variables. Note: Information about car size is already being taken into account through the Weight variable. We could keep looking at other variables to add, but at this point, we have a model that gives us a good sense of the factors related to price of a car, capturing 80% of total variability in car price, and is still easy to interpret. For our research purposes, this model is good enough. 6.2.15 Check of Model Assumptions P1 &lt;- ggplot(data=data.frame(Cars_M7$residuals), aes(y=Cars_M7$residuals, x=Cars_M7$fitted.values)) + geom_point() + ggtitle(&quot;Residual Plot&quot;) + xlab(&quot;Predicted Values&quot;) + ylab(&quot;Residuals&quot;) P2 &lt;- ggplot(data=data.frame(Cars_M7$residuals), aes(x=Cars_M7$residuals)) + geom_histogram() + ggtitle(&quot;Histogram of Residuals&quot;) + xlab(&quot;Residual&quot;) P3 &lt;- ggplot(data=data.frame(Cars_M7$residuals), aes(sample = scale(Cars_M7$residuals))) + stat_qq() + stat_qq_line() + xlab(&quot;Normal Quantiles&quot;) + ylab(&quot;Residual Quantiles&quot;) + ggtitle(&quot;QQ Plot&quot;) grid.arrange(P1, P2, P3, ncol=3) There is slight concern about constant variance, but otherwise, the model assumptions look good. 6.2.16 Residual by Predictor Plots P1 &lt;- ggplot(data=data.frame(Cars_M7$residuals), aes(y=Cars_M7$residuals, x=Cars_M7$model$QtrMile)) + geom_point() + ggtitle(&quot;Residual by Predicted Plot&quot;) + xlab(&quot;QtrMile&quot;) + ylab(&quot;Residuals&quot;) P2 &lt;- ggplot(data=data.frame(Cars_M7$residuals), aes(y=Cars_M7$residuals, x=Cars_M7$model$Weight)) + geom_point() + ggtitle(&quot;Residual by Predicted Plot&quot;) + xlab(&quot;Weight&quot;) + ylab(&quot;Residuals&quot;) P3 &lt;- ggplot(data=data.frame(Cars_M7$residuals), aes(y=Cars_M7$residuals, x=Cars_M7$model$Drive)) + geom_point() + ggtitle(&quot;Residual by Predicted Plot&quot;) + xlab(&quot;Drive&quot;) + ylab(&quot;Residuals&quot;) grid.arrange(P1, P2, P3, ncol=3) These plots don’t raise any concerns. 6.2.17 Coefficients and Exponentiation Cars_M7$coefficients ## (Intercept) QtrMile Weight DriveFWD DriveRWD ## 5.8140678915 -0.1900743859 0.0002049586 -0.2240322171 -0.1388439916 exp(Cars_M7$coefficients) ## (Intercept) QtrMile Weight DriveFWD DriveRWD ## 334.9790161 0.8268976 1.0002050 0.7992894 0.8703638 6.2.18 Interpretation of Coefficients exp(Cars_M7$coefficients) ## (Intercept) QtrMile Weight DriveFWD DriveRWD ## 334.9790161 0.8268976 1.0002050 0.7992894 0.8703638 The price of a car is expected to decrease by 17% for each additional second it takes to drive a quartermile, assuming weight, and drive type are held constant. The price of a car is expected to increase by 0.02% for each additional pound, assuming quarter mile time, and drive type are held constant. Thus, a 100 lb increase is assocated with an expected 2% increase in price, assuming quarter mile time, and drive type are held constant. FWD cars are expected to cost 20% less than AWD cars, assuming quarter mile time and weight are held constant. RWD cars are expected to cost 13% less than AWD cars, assuming quarter mile time and weight are held constant. 6.2.19 Adjusted \\(R^2\\), AIC, BIC When additional variables are added to a model, SSR never increases, hence \\(R^2\\) never decreases. Other diagnostics have been introduced to decrease when a term is added to a model and does little to help explain variability. These include: Adjusted \\(R^2\\) Akaike Information Criterion (AIC) Bayesian Information Criterion (BIC) These are intended to help guide us in deciding whether or not to include a variable in a model.They can decrease (or increase) when an additional variables is added if it doesn’t contain much useful information. These are mostly ad-hoc approaches designed for specific situations. Although they might work well in certain contexts, none are meanth for general use. Furthermore, they can and often do disagree on the best model. I do not advise using these to choose a model, unless you have good reason to in your specific context. 6.2.20 Model Building Summary Consider the following when building a model for the purpose of interpreting parameters and understanding and drawing conclusions about a population or process. Model driven by research question Include variables of interest Include potential confounders (like in SAT example) Avoid including highly correlated explanatory variables Avoid messy transformations and interactions where possible Use residual plots to assess appropriatness of model assumptions Aim for high \\(R^2\\) but not highest Only use AIC, BIC, Adjusted \\(R^2\\) in the other factors listed above (if you use them at all). Do not rely on these measures alone! Aim for model complex enough to capture nature of data, but simple enough to give clear interpretations "],["predictive-modeling.html", "Chapter 7 Predictive Modeling 7.1 Training and Test Error 7.2 Variance-Bias Tradeoff 7.3 Cross-Validation 7.4 Ridge Regression 7.5 Decision Trees 7.6 Regression Splines 7.7 Predicting House Prices Summary 7.8 Assumptions in Predictive Models", " Chapter 7 Predictive Modeling Learning Outcomes: Explain how prediction error changes, depending on model complexity, for both training data and test data. Explain the variance bias tradeoff, and identify situations when predictions might be impacted by either variance or bias. Describe how overfitting can impact prediction accuracy. Explain how and why we use cross-validation in predictive modeling. Compare and contrast Ridge regression and ordinary least squares regression. Explain the role of the parameter \\(\\lambda\\) is Ridge regression. Given possible sets of regression coefficients, determine which would be optimal, using either ordinary least squares, Ridge regression, or Lasso regression. Compare and contrast the assumptions made in decision trees, to those in the normal error regression model. Given possible splits for a node in a decision tree, determine which is optimal. Explain how the depth of a tree impacts prediction variance and bias, and overfitting. Describe how decision trees in a random forest differ from one another. Explain the process of fitting a polynomial spline to data, and the impact the the number of knots has on the process. Explain the assumptions made in predictive modeling, and identify situations where these assumptions might be inappropriate or problematic from an ethical standpoint. 7.1 Training and Test Error 7.1.1 Prediction Simulation Example Suppose we have a set of 100 observations of a single explanatory variable \\(x\\), and response variable \\(y\\). A scatterplot of the data is shown below. We want to find a model that captures the trend in the data and will be best able to predict new values of y, for given x. Constant Model to Sample Data Linear Model to Sample Data Quadratic Model Cubic Model Quartic Model Degree 8 Model 7.1.2 Model Complexity The complexity of the model increases as we add higher-order terms. This makes the model more flexible. The curve is allowed to have more twists and bends. For higher-order, more complex models, individual points have more influence on the shape of the curve. 7.1.3 New Data for Prediction Now, suppose we have a new dataset of 100, x-values, and want to predict \\(y\\). The first 5 rows of the new dataset are shown x Prediction 3.196237 ? 1.475586 ? 5.278882 ? 5.529299 ? 7.626731 ? 7.1.4 Fit Polynomial Models Sim_M0 &lt;-lm(data=Sampdf, y~1) Sim_M1 &lt;-lm(data=Sampdf, y~x) Sim_M2 &lt;- lm(data=Sampdf, y~x+I(x^2)) Sim_M3 &lt;- lm(data=Sampdf, y~x+I(x^2)+I(x^3)) Sim_M4 &lt;- lm(data=Sampdf, y~x+I(x^2)+I(x^3)+I(x^4)) Sim_M5 &lt;- lm(data=Sampdf, y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)) Sim_M6 &lt;- lm(data=Sampdf, y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6)) Sim_M7 &lt;- lm(data=Sampdf, y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6)+I(x^7)) Sim_M8 &lt;- lm(data=Sampdf, y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6)+I(x^7)+I(x^8)) 7.1.5 Predictions for New Data Newdf$Deg0Pred &lt;- predict(Sim_M0, newdata=Newdf) Newdf$Deg1Pred &lt;- predict(Sim_M1, newdata=Newdf) Newdf$Deg2Pred &lt;- predict(Sim_M2, newdata=Newdf) Newdf$Deg3Pred &lt;- predict(Sim_M3, newdata=Newdf) Newdf$Deg4Pred &lt;- predict(Sim_M4, newdata=Newdf) Newdf$Deg5Pred &lt;- predict(Sim_M5, newdata=Newdf) Newdf$Deg6Pred &lt;- predict(Sim_M6, newdata=Newdf) Newdf$Deg7Pred &lt;- predict(Sim_M7, newdata=Newdf) Newdf$Deg8Pred &lt;- predict(Sim_M8, newdata=Newdf) 7.1.6 Predicted Values and True y In fact, since these data were simulated, we know the true value of \\(y\\), so we can compare the predicted values to the true ones. kable(Newdf %&gt;% dplyr::select(-c(samp)) %&gt;% round(2) %&gt;% head(5)) x y Deg0Pred Deg1Pred Deg2Pred Deg3Pred Deg4Pred Deg5Pred Deg6Pred Deg7Pred Deg8Pred 108 5.53 5.49 1.17 1.05 0.41 -0.14 -0.30 0.10 0.40 0.25 0.31 4371 2.05 3.92 1.17 1.89 2.02 3.66 3.95 4.26 3.82 3.37 3.49 4839 3.16 1.46 1.17 1.63 1.29 3.24 3.35 2.78 2.24 2.15 2.07 6907 2.06 6.79 1.17 1.89 2.02 3.66 3.95 4.25 3.80 3.36 3.47 7334 2.92 -1.03 1.17 1.68 1.42 3.43 3.60 3.14 2.51 2.31 2.25 7.1.7 Evaluating Predictions - RMSE For quantitative response variables, we can evaluate the predictions by calculating the average of the squared differences between the true and predicted values. Often, we look at the square root of this quantity. This is called the Root Mean Square Prediction Error (RMSPE). \\[ \\text{RMSPE} = \\sqrt{\\displaystyle\\sum_{i=1}^{n&#39;}\\frac{(y_i-\\hat{y}_i)^2}{n&#39;}}, \\] where \\(n&#39;\\) represents the number of new cases being predicted. 7.1.8 Calculate Prediction Error RMSPE0 &lt;- sqrt(mean((Newdf$y-Newdf$Deg0Pred)^2)) RMSPE1 &lt;- sqrt(mean((Newdf$y-Newdf$Deg1Pred)^2)) RMSPE2 &lt;- sqrt(mean((Newdf$y-Newdf$Deg2Pred)^2)) RMSPE3 &lt;- sqrt(mean((Newdf$y-Newdf$Deg3Pred)^2)) RMSPE4 &lt;- sqrt(mean((Newdf$y-Newdf$Deg4Pred)^2)) RMSPE5 &lt;- sqrt(mean((Newdf$y-Newdf$Deg5Pred)^2)) RMSPE6 &lt;- sqrt(mean((Newdf$y-Newdf$Deg6Pred)^2)) RMSPE7 &lt;- sqrt(mean((Newdf$y-Newdf$Deg7Pred)^2)) RMSPE8 &lt;- sqrt(mean((Newdf$y-Newdf$Deg8Pred)^2)) 7.1.9 Prediction RMSPE by Model Degree RMSPE 0 4.051309 1 3.849624 2 3.726767 3 3.256592 4 3.283513 5 3.341336 6 3.346908 7 3.370821 8 3.350198 7.1.10 Training and Test Data The new data on which we make predictions is called test data. The data used to fit the model is called training data. In the training data, we know the values of the explanatory and response variables. In the test data, we know only the values of the explanatory variables and want to predict the values of the response variable. For quantitative response variables, if we find out the values of \\(y\\), we can evaluate predictions, using RMSPE. This is referred to as test error. We can also calculate root mean square error on the training data from the known y-values. This is called training error. 7.1.11 Training Data Error RMSE0 &lt;- sqrt(mean(Sim_M0$residuals^2)) RMSE1 &lt;- sqrt(mean(Sim_M1$residuals^2)) RMSE2 &lt;- sqrt(mean(Sim_M2$residuals^2)) RMSE3 &lt;- sqrt(mean(Sim_M3$residuals^2)) RMSE4 &lt;- sqrt(mean(Sim_M4$residuals^2)) RMSE5 &lt;- sqrt(mean(Sim_M5$residuals^2)) RMSE6 &lt;- sqrt(mean(Sim_M6$residuals^2)) RMSE7 &lt;- sqrt(mean(Sim_M7$residuals^2)) RMSE8 &lt;- sqrt(mean(Sim_M8$residuals^2)) 7.1.12 Test and Training Error Degree &lt;- 0:8 Test &lt;- c(RMSPE0, RMSPE1, RMSPE2, RMSPE3, RMSPE4, RMSPE5, RMSPE6, RMSPE7, RMSPE8) Train &lt;- c(RMSE0, RMSE1, RMSE2, RMSE3, RMSE4, RMSE5, RMSE6, RMSE7, RMSE8) RMSPEdf &lt;- data.frame(Degree, Test, Train) RMSPEdf ## Degree Test Train ## 1 0 4.051309 3.431842 ## 2 1 3.849624 3.366650 ## 3 2 3.726767 3.296821 ## 4 3 3.256592 2.913233 ## 5 4 3.283513 2.906845 ## 6 5 3.341336 2.838738 ## 7 6 3.346908 2.809928 ## 8 7 3.370821 2.800280 ## 9 8 3.350198 2.799066 7.1.13 Graph of RMSPE Training error decreases as model becomes more complex Testing error is lowest for the 3rd degree model, then starts to increase again 7.1.14 Third Degree Model Summary summary(Sim_M3) ## ## Call: ## lm(formula = y ~ x + I(x^2) + I(x^3), data = Sampdf) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.9451 -1.7976 0.1685 1.3988 6.8064 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.54165 1.26803 -0.427 0.670221 ## x 4.16638 1.09405 3.808 0.000247 *** ## I(x^2) -1.20601 0.25186 -4.788 0.00000610 *** ## I(x^3) 0.08419 0.01622 5.191 0.00000117 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.973 on 96 degrees of freedom ## Multiple R-squared: 0.2794, Adjusted R-squared: 0.2569 ## F-statistic: 12.41 on 3 and 96 DF, p-value: 0.0000006309 7.1.15 True Data Model In fact, the data were generated from the model \\(y_i = 4.5x - 1.4x^2 + 0.1x^3 + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0,3)\\) 7.1.16 True Data Model In fact, the data were generated from the model \\(y_i = 4.5x - 1.4x^2 + 0.1x^3 + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0,3)\\) The yellow curve represents the true expected response curve. 7.1.17 Linear Model on Larger Population The yellow curve represents the true expected response curve. 7.1.18 Cubic Model on Larger Population The yellow curve represents the true expected response curve. 7.1.19 Eighth-Degree Model on Larger Population The yellow curve represents the true expected response curve. The 8th degree model performs worse than the cubic. The extra terms cause the model to be “too flexible,” and it starts to model random fluctuations (noise) in the training data, that do not capture the true trend for the population. This is called overfitting. 7.1.20 Model Complexity, Training Error, and Test Error 7.2 Variance-Bias Tradeoff 7.2.1 What Contributes to Prediction Error? Suppose \\(Y_i = f(x_i) + \\epsilon_i\\), where \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\). Let \\(\\hat{f}\\) represent the function of our explanatory variable(s) \\(x^*\\) used to predict the value of response variable \\(y^*\\). Thus \\(\\hat{y}^* = f(x^*)\\). There are three factors that contribute to the expected value of \\(\\left(y^* - \\hat{y}\\right)^2 = \\left(y^* - \\hat{f}(x^*)\\right)^2\\). Bias associated with fitting model: Model bias pertains to the difference between the true response function value \\(f(x^*)\\), and the average value of \\(\\hat{f}(x^*)\\) that would be obtained in the long run over many samples. - for example, if the true response function \\(f\\) is cubic, then using a constant, linear, or quadratic model would result in biased predictions for most values of \\(x^*\\). Variance associated with fitting model: Individual observations in the training data are subject to random sampling variability. The more flexible a model is, the more weight is put on each individual observation increasing the variance associated with the model. Variability associated with prediction: Even if we knew the true value \\(f(x^*)\\), which represents the expected value of \\(y^*\\) given \\(x=x^*\\), the actual value of \\(y^*\\) will vary due to random noise (i.e. the \\(\\epsilon_i\\sim\\mathcal{N}(0,\\sigma)\\) term). 7.2.2 Variance and Bias The third source of variability cannot be controlled or eliminated. The first two, however are things we can control. If we could figure out how to minimize bias while also minimizing variance associated with a prediction, that would be great! But… The constant model suffers from high bias. Since it does not include a linear, quadratic, or cubic term, it cannot accurately approximate the true regression function. The Eighth degree model suffers from high variance. Although it could, in theory, approximate the true regression function correctly, it is too flexible, and is thrown off because of the influence of individual points with high degrees of variability. 7.2.3 Variance-Bias Tradeoff As model complexity (flexibility) increases, bias decreases. Variance, however, increases. In fact, it can be shown that: \\(\\text{Expected RMSPE} = \\text{Variance} + \\text{Bias}^2\\) Our goal is the find the “sweetspot” where expected RMSPE is minimized. 7.2.4 Modeling for Prediction When our purpose is purely prediction, we don’t need to worry about keeping the model simple enough to interpret. Goal is to fit data well enough to make good predictions on new data without modeling random noise in the training (overfitting) A model that is too simple suffers from high bias A model that is too complex suffers from high variance and is prone to overfitting The right balance is different for every dataset Measuring error on data used to fit the model (training data) does not accurately predict how well model will be able to predict new data (test data) 7.3 Cross-Validation 7.3.1 Cross-Validation (CV) We’ve seen that training error is not an accurate approximation of test error. Instead, we’ll approximate test error, by setting aside a set of the training data, and using it as if it were a test set. This process is called cross-validation, and the set we put aside is called the validation set. Partition data into disjoint sets (folds). Approximately 5 folds recommended. Build a model using 4 of the 5 folds. Use model to predict responses for remaining fold. Calculate root mean square error \\(RMSPE=\\displaystyle\\sqrt{\\frac{\\sum((\\hat{y}_i-y_i)^2)}{n&#39;}}\\). Repeat for each of 5 folds. Average RMSPE values across folds. If computational resources permit, it is often beneficial to perform CV multiple times, using different sets of folds. 7.3.2 Cross-Validation Illustration Order &lt;- sample(1:nrow(Cars2015)) fold1 &lt;- Order[1:22] fold2 &lt;- Order[23:44] fold3 &lt;- Order[45:66] fold4 &lt;- Order[67:88] fold5 &lt;- Order[89:110] Fold &lt;- data.frame(fold1, fold2, fold3, fold4, fold5) 7.3.3 Models for Price of New Cars Consider the following five models for predicting the price of a new car: M1 &lt;- lm(data=Cars2015, LowPrice~Acc060) M2 &lt;- lm(data=Cars2015, LowPrice~Acc060 + I(Acc060^2)) M3 &lt;- lm(data=Cars2015, LowPrice~Acc060 + I(Acc060^2) + Size) M4 &lt;- lm(data=Cars2015, LowPrice~Acc060 + I(Acc060^2) + Size + PageNum) M5 &lt;- lm(data=Cars2015, LowPrice~Acc060 * I(Acc060^2) * Size) M6 &lt;- lm(data=Cars2015, LowPrice~Acc060 * I(Acc060^2) * Size * PageNum) RMSPE &lt;- array(NA, dim=c(5,6)) #5 folds, 5 models for (i in 1:5){ M1 &lt;- lm(data=Cars2015[-Fold[,i], ], LowPrice~Acc060) M2 &lt;- lm(data=Cars2015[-Fold[,i], ], LowPrice~Acc060 + I(Acc060^2)) M3 &lt;- lm(data=Cars2015[-Fold[,i], ], LowPrice~Acc060 + I(Acc060^2) + Size) M4 &lt;- lm(data=Cars2015[-Fold[,i], ], LowPrice~Acc060 + I(Acc060^2) + Size + PageNum) M5 &lt;- lm(data=Cars2015[-Fold[,i], ], LowPrice~Acc060 * I(Acc060^2) * Size) M6 &lt;- lm(data=Cars2015[-Fold[,i], ], LowPrice~Acc060 * I(Acc060^2) * Size * PageNum) Yhat1 &lt;- predict(M1, newdata=Cars2015[Fold[,i], ]) Yhat2 &lt;- predict(M2, newdata=Cars2015[Fold[,i], ]) Yhat3 &lt;- predict(M3, newdata=Cars2015[Fold[,i], ]) Yhat4 &lt;- predict(M4, newdata=Cars2015[Fold[,i], ]) Yhat5 &lt;- predict(M5, newdata=Cars2015[Fold[,i], ]) Yhat6 &lt;- predict(M6, newdata=Cars2015[Fold[,i], ]) Y &lt;- Cars2015[Fold[,i], ]$LowPrice RMSPE[i, 1] &lt;- sqrt(mean((Y-Yhat1)^2)) RMSPE[i, 2] &lt;- sqrt(mean((Y-Yhat2)^2)) RMSPE[i, 3] &lt;- sqrt(mean((Y-Yhat3)^2)) RMSPE[i, 4] &lt;- sqrt(mean((Y-Yhat4)^2)) RMSPE[i, 5] &lt;- sqrt(mean((Y-Yhat5)^2)) RMSPE[i, 6] &lt;- sqrt(mean((Y-Yhat6)^2)) } 7.3.4 Cross Validation in Cars Dataset To Use cross validation on the cars dataset, we’ll fit each model to 4 folds (88 cars), while withholding the other 1 fold (22 cars). We then make predictions on the last fold, and see which model yields the lowest MSPE. t(Fold) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] ## fold1 76 82 73 14 30 15 109 5 102 92 25 24 6 ## fold2 65 80 107 62 8 4 91 75 2 41 99 81 37 ## fold3 68 101 108 31 23 59 105 42 35 21 1 84 106 ## fold4 50 104 13 52 29 98 66 54 49 110 38 48 63 ## fold5 55 22 17 3 10 28 39 51 20 47 69 83 88 ## [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] ## fold1 36 16 57 44 72 53 86 94 34 ## fold2 74 56 27 90 19 40 61 78 71 ## fold3 60 43 77 64 67 100 7 58 87 ## fold4 12 103 97 85 95 96 32 45 89 ## fold5 9 70 46 11 33 79 26 93 18 7.3.5 CV Results Rows show RMSPE, columns show model. kable(RMSPE) 11.329817 11.460672 9.989957 10.130743 10.808705 10.66698 11.904377 12.406512 10.998581 10.993984 13.447907 17.27614 10.044277 9.399102 8.482429 9.167676 7.906023 13.84186 9.868489 9.081304 10.658656 10.661173 9.435986 12.09450 10.630225 9.538196 9.178224 9.681109 8.266503 18.80454 Mean RMSPE apply(RMSPE,2,mean) ## [1] 10.755437 10.377157 9.861570 10.126937 9.973025 14.536803 7.3.6 CV Conclusions Model 3 achieves the best performance using cross-validation. This is the model that includes a quadratic function of acceleration time and size. Including page number, or interactions harms the performance of the model. We should use Model 3 if we want to make predictions on new data. 7.4 Ridge Regression 7.4.1 Complexity in Model Coefficients We’ve thought about complexity in terms of the number of terms we include in a model, as well as whether we include quadratic terms and higher order terms and interactions We can also think about model complexity in terms of the coefficients \\(b_1, \\ldots, b_p\\). Larger values of \\(b_1, \\ldots, b_p\\) are associated with more complex models. Smaller values of \\(b_1, \\ldots, b_p\\) are associated with less complex models. When \\(b_j=0\\), this mean variable \\(j\\) is not used in the model. 7.4.2 Regression on Housing Dataset set.seed(10302021) samp &lt;- sample(1:nrow(ames_raw), 1000) Train_Data &lt;- ames_raw[samp,] M_OLS &lt;- lm(data=Train_Data, SalePrice ~ .) summary(M_OLS) ## ## Call: ## lm(formula = SalePrice ~ ., data = Train_Data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -158158 -8892 213 9496 158158 ## ## Coefficients: ## Estimate Std. Error t value ## (Intercept) -14136063.8945 19081301.5981 -0.741 ## `Overall Qual` 6975.9250 1187.4137 5.875 ## `Year Built` 494.5820 107.1664 4.615 ## `Mas Vnr Area` 33.3817 6.4707 5.159 ## `Central Air`Y -3745.3629 4857.8749 -0.771 ## `Gr Liv Area` 37.2290 18.5297 2.009 ## `Lot Frontage` -14.9594 60.3992 -0.248 ## `1st Flr SF` 13.3690 19.2561 0.694 ## `Bedroom AbvGr` -2353.0379 1675.4799 -1.404 ## `TotRms AbvGrd` 914.5595 1112.3776 0.822 ## Order 9.0730 15.1005 0.601 ## PID 0.7939 4.3378 0.183 ## `MS SubClass`030 1747.3878 6199.3466 0.282 ## `MS SubClass`040 5867.2885 18051.4432 0.325 ## `MS SubClass`045 7182.7066 10431.2913 0.689 ## `MS SubClass`050 -1330.0097 5044.6915 -0.264 ## `MS SubClass`060 -7400.8589 5261.2795 -1.407 ## `MS SubClass`070 -953.7000 7122.4627 -0.134 ## `MS SubClass`075 -10585.7403 12375.9825 -0.855 ## `MS SubClass`080 -6516.1315 4515.3606 -1.443 ## `MS SubClass`085 -6527.2338 6376.7201 -1.024 ## `MS SubClass`090 -21891.2260 9761.2969 -2.243 ## `MS SubClass`120 -20296.4700 4678.1426 -4.339 ## `MS SubClass`160 -34837.6664 7079.2626 -4.921 ## `MS SubClass`180 -18822.1874 12977.3945 -1.450 ## `MS SubClass`190 -12421.4283 7938.1494 -1.565 ## `MS Zoning`C (all) -38785.6865 33604.3855 -1.154 ## `MS Zoning`FV -23114.7183 29478.1257 -0.784 ## `MS Zoning`I (all) -17213.3332 41603.5670 -0.414 ## `MS Zoning`RH -13306.2220 29844.8659 -0.446 ## `MS Zoning`RL -17643.5479 28756.2363 -0.614 ## `MS Zoning`RM -24783.4465 29226.7175 -0.848 ## `Lot Area` 0.7408 0.1658 4.468 ## StreetPave 37262.6394 17051.5092 2.185 ## AlleyPave 2527.1840 7354.5945 0.344 ## AlleyNA 29.0268 5198.4814 0.006 ## `Lot Shape`IR2 8575.5874 5485.4669 1.563 ## `Lot Shape`IR3 10397.8002 12598.2284 0.825 ## `Lot Shape`Reg 2676.2691 2031.2908 1.318 ## `Land Contour`HLS 11318.1617 6089.7012 1.859 ## `Land Contour`Low -22452.1871 11524.1454 -1.948 ## `Land Contour`Lvl 12230.7110 4699.3902 2.603 ## `Lot Config`CulDSac 10427.6522 5333.0392 1.955 ## `Lot Config`FR2 -12317.5424 5422.2301 -2.272 ## `Lot Config`FR3 -14557.2814 11621.4721 -1.253 ## `Lot Config`Inside -1168.2308 2362.0973 -0.495 ## `Land Slope`Mod 10550.3767 4611.9960 2.288 ## `Land Slope`Sev -24213.7594 18971.9679 -1.276 ## NeighborhoodBlueste 19001.1402 15359.2081 1.237 ## NeighborhoodBrDale 20558.0541 14183.6972 1.449 ## NeighborhoodBrkSide 14314.5790 12163.1858 1.177 ## NeighborhoodClearCr 7379.3466 15007.2726 0.492 ## NeighborhoodCollgCr -46.6308 10100.6076 -0.005 ## NeighborhoodCrawfor 28775.4385 11758.9005 2.447 ## NeighborhoodEdwards -7491.2919 10588.5901 -0.707 ## NeighborhoodGilbert 1951.6205 9311.2407 0.210 ## NeighborhoodGreens 4667.7593 24819.3716 0.188 ## NeighborhoodIDOTRR 12473.1493 13394.5390 0.931 ## NeighborhoodMeadowV 18891.1265 14852.3899 1.272 ## NeighborhoodMitchel -9274.3206 11674.4738 -0.794 ## NeighborhoodNAmes 1781.9099 9557.5827 0.186 ## NeighborhoodNoRidge 32109.4882 10208.1864 3.145 ## NeighborhoodNPkVill 19601.4138 12568.2771 1.560 ## NeighborhoodNridgHt 34407.2004 9268.6339 3.712 ## NeighborhoodNWAmes -3367.8476 9915.0330 -0.340 ## NeighborhoodOldTown 9857.0795 11660.0353 0.845 ## NeighborhoodSawyer 6761.6510 10233.7204 0.661 ## NeighborhoodSawyerW 3945.8509 9472.8485 0.417 ## NeighborhoodSomerst 20453.1723 9940.5448 2.058 ## NeighborhoodStoneBr 44556.0799 10407.7868 4.281 ## NeighborhoodSWISU 8518.9311 12621.7640 0.675 ## NeighborhoodTimber 1455.9164 11778.9617 0.124 ## NeighborhoodVeenker 5759.5517 11938.6907 0.482 ## `Condition 1`Feedr -315.5373 6556.3195 -0.048 ## `Condition 1`Norm 10041.9750 5463.8941 1.838 ## `Condition 1`PosA 49498.8746 23721.3107 2.087 ## `Condition 1`PosN 15873.9314 9869.5060 1.608 ## `Condition 1`RRAe -11599.3285 9309.0850 -1.246 ## `Condition 1`RRAn 7894.0461 8747.5548 0.902 ## `Condition 1`RRNn 6045.1254 14793.6446 0.409 ## `Condition 2`Feedr 7639.7933 31157.3993 0.245 ## `Condition 2`Norm 6804.6579 27562.1927 0.247 ## `Condition 2`PosA 1496.1297 39927.8279 0.037 ## `Condition 2`PosN -224695.9130 33982.6831 -6.612 ## `Condition 2`RRNn 20826.2553 37187.7025 0.560 ## `Overall Cond` 5652.4500 1110.9628 5.088 ## `Year Remod/Add` 115.1030 67.4315 1.707 ## `Roof Style`Gable -983.4966 21461.8852 -0.046 ## `Roof Style`Gambrel -3775.4699 23821.0554 -0.158 ## `Roof Style`Hip -1020.9768 21560.9811 -0.047 ## `Roof Style`Mansard 18711.8071 32901.5367 0.569 ## `Roof Style`Shed -9552.0309 40823.6097 -0.234 ## `Roof Matl`CompShg 671041.0671 49405.0627 13.582 ## `Roof Matl`Membran 738250.2085 60710.0732 12.160 ## `Roof Matl`Tar&amp;Grv 653237.7171 51651.9656 12.647 ## `Roof Matl`WdShngl 757954.9463 51602.0280 14.688 ## `Mas Vnr Type`BrkFace -14297.1434 10170.9970 -1.406 ## `Mas Vnr Type`None -5178.2754 10178.9123 -0.509 ## `Mas Vnr Type`Stone -11764.3736 10522.7000 -1.118 ## `Exter Qual`Fa -21903.6214 14605.4913 -1.500 ## `Exter Qual`Gd -37435.0319 5548.9279 -6.746 ## `Exter Qual`TA -40396.2769 6354.0424 -6.358 ## `Exter Cond`Fa -2499.1726 12458.9295 -0.201 ## `Exter Cond`Gd 9625.5182 10138.4484 0.949 ## `Exter Cond`Po -9090.3356 27937.9324 -0.325 ## `Exter Cond`TA 10976.5040 9966.7706 1.101 ## FoundationCBlock 2136.6597 3877.0143 0.551 ## FoundationPConc 4341.3771 4362.7304 0.995 ## FoundationSlab -8011.2101 13611.4882 -0.589 ## FoundationStone 5209.4079 17623.6756 0.296 ## FoundationWood -21499.8677 15486.8933 -1.388 ## `Bsmt Qual`Fa -19118.1142 7414.2132 -2.579 ## `Bsmt Qual`Gd -13338.1127 3835.2102 -3.478 ## `Bsmt Qual`Po 58476.8071 51564.9674 1.134 ## `Bsmt Qual`TA -14904.6825 5075.8325 -2.936 ## `Bsmt Qual`NA 29237.6695 26500.2573 1.103 ## `Bsmt Exposure`Gd 8445.8558 3569.0548 2.366 ## `Bsmt Exposure`Mn -5636.1528 3589.6788 -1.570 ## `Bsmt Exposure`No -5726.8656 2630.3304 -2.177 ## `Bsmt Exposure`NA -28286.6797 22803.9493 -1.240 ## `BsmtFin SF 1` 33.8128 5.9187 5.713 ## `BsmtFin SF 2` 24.9525 7.4967 3.328 ## `Bsmt Unf SF` 13.2018 5.7763 2.286 ## HeatingGasW 3554.5996 9352.0654 0.380 ## HeatingWall 16477.8270 25523.1969 0.646 ## `Heating QC`Fa -7277.0668 5802.5586 -1.254 ## `Heating QC`Gd -760.5325 2379.1850 -0.320 ## `Heating QC`Po -17982.8274 20110.2913 -0.894 ## `Heating QC`TA -5710.2836 2478.7220 -2.304 ## ElectricalFuseF 728.4352 8127.3798 0.090 ## ElectricalFuseP 25345.9011 20610.3725 1.230 ## ElectricalMix 51715.0816 52375.4236 0.987 ## ElectricalSBrkr -2872.3538 3735.6385 -0.769 ## `2nd Flr SF` 12.4887 18.3682 0.680 ## `Bsmt Full Bath` -498.4666 2188.6038 -0.228 ## `Bsmt Half Bath` 3026.8853 3865.0541 0.783 ## `Full Bath` 3781.8172 2857.0874 1.324 ## `Half Bath` 3333.8342 2530.9936 1.317 ## `Kitchen AbvGr` -12956.8476 9020.3081 -1.436 ## `Kitchen Qual`Fa -7789.8981 8251.1512 -0.944 ## `Kitchen Qual`Gd -13554.1557 4309.1256 -3.145 ## `Kitchen Qual`TA -13298.5376 4825.6842 -2.756 ## FunctionalMaj2 -24570.4189 29046.5460 -0.846 ## FunctionalMin1 -8664.3164 13350.1102 -0.649 ## FunctionalMin2 -12709.0599 13033.9665 -0.975 ## FunctionalMod -14394.7177 14722.7551 -0.978 ## FunctionalTyp 4206.1036 12139.0145 0.346 ## Fireplaces 11467.2013 3016.6825 3.801 ## `Fireplace Qu`Fa -12766.1233 8228.9993 -1.551 ## `Fireplace Qu`Gd -14204.2309 6274.3681 -2.264 ## `Fireplace Qu`Po -23140.9430 9267.6637 -2.497 ## `Fireplace Qu`TA -17684.2618 6494.6717 -2.723 ## `Fireplace Qu`NA -5555.0116 7184.2516 -0.773 ## `Garage Type`Attchd -1098.5464 10410.7828 -0.106 ## `Garage Type`Basment -698.4291 12956.4102 -0.054 ## `Garage Type`BuiltIn -4596.8780 11028.1180 -0.417 ## `Garage Type`CarPort -10478.5628 14668.5208 -0.714 ## `Garage Type`Detchd -138.9775 10352.7490 -0.013 ## `Garage Yr Blt` -61.6634 71.0809 -0.868 ## `Garage Finish`RFn -4343.2684 2324.3495 -1.869 ## `Garage Finish`Unf -1482.5096 2761.9889 -0.537 ## `Garage Cars` 2990.4178 2575.0518 1.161 ## `Garage Area` 24.3014 8.4310 2.882 ## `Garage Qual`Fa -70420.9028 34399.0827 -2.047 ## `Garage Qual`Gd -51114.4969 34997.4461 -1.461 ## `Garage Qual`Po -134052.9765 53225.9619 -2.519 ## `Garage Qual`TA -67768.2608 34050.0303 -1.990 ## `Garage Cond`Fa 73986.1047 38135.5692 1.940 ## `Garage Cond`Gd 64104.4269 44282.0927 1.448 ## `Garage Cond`Po 112712.4021 42502.3108 2.652 ## `Garage Cond`TA 71259.2489 37706.9633 1.890 ## `Paved Drive`P 5257.0318 6578.8847 0.799 ## `Paved Drive`Y 2077.1864 4637.9411 0.448 ## `Wood Deck SF` 7.6415 7.1551 1.068 ## `Open Porch SF` -16.2882 14.7710 -1.103 ## `Enclosed Porch` -11.3738 15.5858 -0.730 ## `3Ssn Porch` 16.1217 25.0628 0.643 ## `Screen Porch` 38.2573 13.2698 2.883 ## `Pool Area` -106.8683 172.5566 -0.619 ## `Pool QC`TA 24551.0372 50543.0049 0.486 ## `Pool QC`NA -100661.2222 115845.2657 -0.869 ## FenceGdWo 3.7226 6004.9653 0.001 ## FenceMnPrv 1567.2382 5154.5417 0.304 ## FenceMnWw -880.4036 11445.7271 -0.077 ## FenceNA 1297.2149 4799.1348 0.270 ## `Misc Feature`Gar2 545161.5428 41606.6518 13.103 ## `Misc Feature`Othr 569599.2024 49966.9781 11.400 ## `Misc Feature`Shed 551469.2570 55133.8721 10.002 ## `Misc Feature`NA 547892.2455 57027.5502 9.608 ## `Misc Val` 1.6361 2.9165 0.561 ## `Mo Sold` -262.1973 301.1296 -0.871 ## `Yr Sold` 5937.2903 9495.7730 0.625 ## `Sale Type`Con 7985.9754 14377.6833 0.555 ## `Sale Type`ConLD 23972.1097 12356.7468 1.940 ## `Sale Type`ConLI 9525.1931 14763.1370 0.645 ## `Sale Type`ConLw 11056.3865 17164.1972 0.644 ## `Sale Type`CWD -9551.0936 15096.9006 -0.633 ## `Sale Type`New 23118.1311 16568.0832 1.395 ## `Sale Type`Oth 39733.0338 17369.1737 2.288 ## `Sale Type`VWD -11729.6976 24093.7246 -0.487 ## `Sale Type`WD 4819.9398 4886.0362 0.986 ## `Sale Condition`AdjLand 41276.9513 18154.2390 2.274 ## `Sale Condition`Alloca 24461.6278 10762.9224 2.273 ## `Sale Condition`Family 404.8499 6776.3642 0.060 ## `Sale Condition`Normal 1592.2626 3745.5754 0.425 ## `Sale Condition`Partial -4641.7581 15793.5170 -0.294 ## Pr(&gt;|t|) ## (Intercept) 0.459014 ## `Overall Qual` 0.0000000062218 *** ## `Year Built` 0.0000045805578 *** ## `Mas Vnr Area` 0.0000003140313 *** ## `Central Air`Y 0.440943 ## `Gr Liv Area` 0.044859 * ## `Lot Frontage` 0.804449 ## `1st Flr SF` 0.487714 ## `Bedroom AbvGr` 0.160592 ## `TotRms AbvGrd` 0.411229 ## Order 0.548117 ## PID 0.854835 ## `MS SubClass`030 0.778119 ## `MS SubClass`040 0.745243 ## `MS SubClass`045 0.491293 ## `MS SubClass`050 0.792122 ## `MS SubClass`060 0.159918 ## `MS SubClass`070 0.893515 ## `MS SubClass`075 0.392618 ## `MS SubClass`080 0.149386 ## `MS SubClass`085 0.306334 ## `MS SubClass`090 0.025194 * ## `MS SubClass`120 0.0000161908290 *** ## `MS SubClass`160 0.0000010467637 *** ## `MS SubClass`180 0.147347 ## `MS SubClass`190 0.118034 ## `MS Zoning`C (all) 0.248771 ## `MS Zoning`FV 0.433197 ## `MS Zoning`I (all) 0.679171 ## `MS Zoning`RH 0.655830 ## `MS Zoning`RL 0.539685 ## `MS Zoning`RM 0.396709 ## `Lot Area` 0.0000090257776 *** ## StreetPave 0.029159 * ## AlleyPave 0.731223 ## AlleyNA 0.995546 ## `Lot Shape`IR2 0.118374 ## `Lot Shape`IR3 0.409427 ## `Lot Shape`Reg 0.188044 ## `Land Contour`HLS 0.063457 . ## `Land Contour`Low 0.051734 . ## `Land Contour`Lvl 0.009424 ** ## `Lot Config`CulDSac 0.050899 . ## `Lot Config`FR2 0.023373 * ## `Lot Config`FR3 0.210713 ## `Lot Config`Inside 0.621038 ## `Land Slope`Mod 0.022423 * ## `Land Slope`Sev 0.202226 ## NeighborhoodBlueste 0.216409 ## NeighborhoodBrDale 0.147617 ## NeighborhoodBrkSide 0.239597 ## NeighborhoodClearCr 0.623055 ## NeighborhoodCollgCr 0.996318 ## NeighborhoodCrawfor 0.014616 * ## NeighborhoodEdwards 0.479471 ## NeighborhoodGilbert 0.834035 ## NeighborhoodGreens 0.850870 ## NeighborhoodIDOTRR 0.352027 ## NeighborhoodMeadowV 0.203772 ## NeighborhoodMitchel 0.427194 ## NeighborhoodNAmes 0.852148 ## NeighborhoodNoRidge 0.001720 ** ## NeighborhoodNPkVill 0.119254 ## NeighborhoodNridgHt 0.000220 *** ## NeighborhoodNWAmes 0.734194 ## NeighborhoodOldTown 0.398157 ## NeighborhoodSawyer 0.508982 ## NeighborhoodSawyerW 0.677125 ## NeighborhoodSomerst 0.039959 * ## NeighborhoodStoneBr 0.0000208772751 *** ## NeighborhoodSWISU 0.499910 ## NeighborhoodTimber 0.901661 ## NeighborhoodVeenker 0.629635 ## `Condition 1`Feedr 0.961627 ## `Condition 1`Norm 0.066454 . ## `Condition 1`PosA 0.037235 * ## `Condition 1`PosN 0.108149 ## `Condition 1`RRAe 0.213124 ## `Condition 1`RRAn 0.367103 ## `Condition 1`RRNn 0.682921 ## `Condition 2`Feedr 0.806365 ## `Condition 2`Norm 0.805062 ## `Condition 2`PosA 0.970119 ## `Condition 2`PosN 0.0000000000695 *** ## `Condition 2`RRNn 0.575616 ## `Overall Cond` 0.0000004522273 *** ## `Year Remod/Add` 0.088220 . ## `Roof Style`Gable 0.963461 ## `Roof Style`Gambrel 0.874109 ## `Roof Style`Hip 0.962244 ## `Roof Style`Mansard 0.569706 ## `Roof Style`Shed 0.815058 ## `Roof Matl`CompShg &lt; 0.0000000000000002 *** ## `Roof Matl`Membran &lt; 0.0000000000000002 *** ## `Roof Matl`Tar&amp;Grv &lt; 0.0000000000000002 *** ## `Roof Matl`WdShngl &lt; 0.0000000000000002 *** ## `Mas Vnr Type`BrkFace 0.160211 ## `Mas Vnr Type`None 0.611086 ## `Mas Vnr Type`Stone 0.263905 ## `Exter Qual`Fa 0.134094 ## `Exter Qual`Gd 0.0000000000292 *** ## `Exter Qual`TA 0.0000000003455 *** ## `Exter Cond`Fa 0.841068 ## `Exter Cond`Gd 0.342702 ## `Exter Cond`Po 0.744982 ## `Exter Cond`TA 0.271096 ## FoundationCBlock 0.581714 ## FoundationPConc 0.319988 ## FoundationSlab 0.556322 ## FoundationStone 0.767619 ## FoundationWood 0.165447 ## `Bsmt Qual`Fa 0.010100 * ## `Bsmt Qual`Gd 0.000533 *** ## `Bsmt Qual`Po 0.257119 ## `Bsmt Qual`TA 0.003416 ** ## `Bsmt Qual`NA 0.270232 ## `Bsmt Exposure`Gd 0.018201 * ## `Bsmt Exposure`Mn 0.116790 ## `Bsmt Exposure`No 0.029756 * ## `Bsmt Exposure`NA 0.215183 ## `BsmtFin SF 1` 0.0000000157066 *** ## `BsmtFin SF 2` 0.000913 *** ## `Bsmt Unf SF` 0.022545 * ## HeatingGasW 0.703982 ## HeatingWall 0.518724 ## `Heating QC`Fa 0.210170 ## `Heating QC`Gd 0.749309 ## `Heating QC`Po 0.371480 ## `Heating QC`TA 0.021496 * ## ElectricalFuseF 0.928606 ## ElectricalFuseP 0.219150 ## ElectricalMix 0.323751 ## ElectricalSBrkr 0.442178 ## `2nd Flr SF` 0.496762 ## `Bsmt Full Bath` 0.819895 ## `Bsmt Half Bath` 0.433777 ## `Full Bath` 0.185996 ## `Half Bath` 0.188150 ## `Kitchen AbvGr` 0.151280 ## `Kitchen Qual`Fa 0.345407 ## `Kitchen Qual`Gd 0.001720 ** ## `Kitchen Qual`TA 0.005989 ** ## FunctionalMaj2 0.397864 ## FunctionalMin1 0.516521 ## FunctionalMin2 0.329821 ## FunctionalMod 0.328511 ## FunctionalTyp 0.729063 ## Fireplaces 0.000155 *** ## `Fireplace Qu`Fa 0.121214 ## `Fireplace Qu`Gd 0.023852 * ## `Fireplace Qu`Po 0.012728 * ## `Fireplace Qu`TA 0.006613 ** ## `Fireplace Qu`NA 0.439622 ## `Garage Type`Attchd 0.915990 ## `Garage Type`Basment 0.957024 ## `Garage Type`BuiltIn 0.676914 ## `Garage Type`CarPort 0.475216 ## `Garage Type`Detchd 0.989293 ## `Garage Yr Blt` 0.385925 ## `Garage Finish`RFn 0.062047 . ## `Garage Finish`Unf 0.591588 ## `Garage Cars` 0.245867 ## `Garage Area` 0.004053 ** ## `Garage Qual`Fa 0.040970 * ## `Garage Qual`Gd 0.144543 ## `Garage Qual`Po 0.011979 * ## `Garage Qual`TA 0.046905 * ## `Garage Cond`Fa 0.052724 . ## `Garage Cond`Gd 0.148113 ## `Garage Cond`Po 0.008163 ** ## `Garage Cond`TA 0.059147 . ## `Paved Drive`P 0.424485 ## `Paved Drive`Y 0.654370 ## `Wood Deck SF` 0.285852 ## `Open Porch SF` 0.270485 ## `Enclosed Porch` 0.465753 ## `3Ssn Porch` 0.520245 ## `Screen Porch` 0.004045 ** ## `Pool Area` 0.535881 ## `Pool QC`TA 0.627282 ## `Pool QC`NA 0.385149 ## FenceGdWo 0.999506 ## FenceMnPrv 0.761169 ## FenceMnWw 0.938707 ## FenceNA 0.786998 ## `Misc Feature`Gar2 &lt; 0.0000000000000002 *** ## `Misc Feature`Othr &lt; 0.0000000000000002 *** ## `Misc Feature`Shed &lt; 0.0000000000000002 *** ## `Misc Feature`NA &lt; 0.0000000000000002 *** ## `Misc Val` 0.574961 ## `Mo Sold` 0.384174 ## `Yr Sold` 0.531983 ## `Sale Type`Con 0.578749 ## `Sale Type`ConLD 0.052733 . ## `Sale Type`ConLI 0.518983 ## `Sale Type`ConLw 0.519662 ## `Sale Type`CWD 0.527143 ## `Sale Type`New 0.163303 ## `Sale Type`Oth 0.022425 * ## `Sale Type`VWD 0.626509 ## `Sale Type`WD 0.324202 ## `Sale Condition`AdjLand 0.023251 * ## `Sale Condition`Alloca 0.023307 * ## `Sale Condition`Family 0.952374 ## `Sale Condition`Normal 0.670875 ## `Sale Condition`Partial 0.768909 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 21710 on 794 degrees of freedom ## Multiple R-squared: 0.943, Adjusted R-squared: 0.9283 ## F-statistic: 64.08 on 205 and 794 DF, p-value: &lt; 0.00000000000000022 7.4.3 Housing Model- Some Exp. Vars. Let’s focus on the first 10 variables on the list. head(coef(M_OLS),10) %&gt;% round(3) ## (Intercept) `Overall Qual` `Year Built` `Mas Vnr Area` `Central Air`Y ## -14136063.894 6975.925 494.582 33.382 -3745.363 ## `Gr Liv Area` `Lot Frontage` `1st Flr SF` `Bedroom AbvGr` `TotRms AbvGrd` ## 37.229 -14.959 13.369 -2353.038 914.559 7.4.4 Ridge Regression Penalty We’ve seen that in ordinary least-squares regression, \\(b_0, b_1, \\ldots, b_p\\) are chosen in a way that to minimizes \\[ \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2 = \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2{x_i2} + \\ldots +b_px_{ip}))^2 \\] When \\(p\\) is large, and we want to be careful of overfitting, a common approach is to add a “penalty term” to this function, to incentivize choosing low values of \\(b_1, \\ldots, b_p\\). Specifically, we minimize: \\[ \\begin{aligned} &amp; \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2 + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\\\\ = &amp; \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2 \\end{aligned} \\] where \\(\\lambda\\) is a pre-determined positive constant. Larger values of \\(b_j\\) typically help the model better fit the training data, thereby making the first term smaller, but also make the second term larger. The idea is the find optimal values of \\(b_0, b_1, \\ldots, b_p\\) that are large enough to allow the model to fit the data well, thus keeping the first term (SSR) small, while also keeping the penalty term small as well. 7.4.5 Choosing \\(\\lambda\\) The value of \\(\\lambda\\) is predetermined by the user. The larger the value of \\(\\lambda\\), the more heavily large \\(b_j&#39;s\\) are penalized. A value of \\(\\lambda=0\\) corresponds to ordinary least-squares. \\[ \\begin{aligned} Q=&amp; \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2 + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\\\\ = &amp; \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2 \\end{aligned} \\] 7.4.6 Optimal value of \\(\\lambda\\) Small values of \\(\\lambda\\) lead to more complex models, with larger \\(|b_j|\\)’s. As \\(\\lambda\\) increases, \\(|b_j|\\)’s shrink toward 0. The model becomes less complex, thus bias increases, but variance decreases. We can use cross validation to determine the optimal value of \\(\\lambda\\) 7.4.7 Standardizing Train_sc &lt;- Train_Data %&gt;% mutate_if(is.numeric, scale) It is important to standardize each explanatory variable (i.e. subtract the mean and divide by the standard deviation). This ensures each variable has mean 0 and standard deviation 1. Without standardizing the optimal choice of \\(b_j\\)’s would depend on scale, with variables with larger absolute measurements having more influence. We’ll standardize the response variable too. Though this is not strictly necessary, it doesn’t hurt. We can always transform back if necessary. Standardization is performed using the scale command in R. 7.4.8 Ridge Regression on Housing Dataset control = trainControl(&quot;repeatedcv&quot;, number = 10, repeats=10) l_vals = 10^seq(-3, 3, length = 100) set.seed(11162020) Housing_ridge &lt;- train(SalePrice ~ ., data = Train_sc, method = &quot;glmnet&quot;, trControl=control , tuneGrid=expand.grid(alpha=0, lambda=l_vals)) Housing_ridge$bestTune$lambda ## [1] 0.6135907 7.4.9 Ridge Regression RMSPE by \\(\\lambda\\) 7.4.10 Ridge Regression Coefficients for Optimal Model M_OLS_sc &lt;- lm(data=Train_sc, SalePrice ~ .) OLS_coef &lt;- M_OLS_sc$coefficients Ridge_coef &lt;- coef(Housing_ridge$finalModel, Housing_ridge$bestTune$lambda)[,1] df &lt;- data.frame(OLS_coef[2:10], Ridge_coef[2:10]) names(df) &lt;-c(&quot;OLS Coeff&quot;, &quot;Ridge Coeff&quot;) df ## OLS Coeff Ridge Coeff ## `Overall Qual` 0.121728754 0.10435284 ## `Year Built` 0.187102422 0.03451303 ## `Mas Vnr Area` 0.080212607 0.06202880 ## `Central Air`Y -0.046191694 0.04289126 ## `Gr Liv Area` 0.237623291 0.00000000 ## `Lot Frontage` -0.004290945 0.07967743 ## `1st Flr SF` 0.069910650 0.01020597 ## `Bedroom AbvGr` -0.022457937 0.07194208 ## `TotRms AbvGrd` 0.017574153 0.01342224 7.4.11 OLS and Ridge Predictions library(glmnet) MAT &lt;- model.matrix(SalePrice~., data=Train_sc) ridge_mod &lt;- glmnet(x=MAT, y=Train_sc$SalePrice, alpha = 0, lambda=Housing_ridge$bestTune$lambda ) y &lt;- Train_sc$SalePrice Pred_OLS &lt;- predict(M_OLS_sc) Pred_Ridge &lt;- predict(ridge_mod, newx=MAT) OLS_Resid &lt;- y - Pred_OLS Ridge_Resid &lt;- y - Pred_Ridge Resdf &lt;- data.frame(y, Pred_OLS, Pred_Ridge, OLS_Resid, Ridge_Resid) names(Resdf) &lt;- c(&quot;y&quot;, &quot;OLS Pred&quot;, &quot;Ridge Pred&quot;, &quot;OLS Resid&quot;, &quot;Ridge Resid&quot;) kable(head(Resdf)) y OLS Pred Ridge Pred OLS Resid Ridge Resid 859 -0.6210832 -0.4637429 -0.4651589 -0.1573403 -0.1559243 1850 0.6800520 1.1897467 1.0528536 -0.5096947 -0.3728016 1301 -0.4545873 -0.4527781 -0.4958630 -0.0018092 0.0412758 981 -0.6408161 -0.6626212 -0.7711186 0.0218051 0.1303025 2694 -0.7937457 -0.8679455 -0.7543093 0.0741997 -0.0394365 2209 -0.7906625 -0.6955254 -0.6449779 -0.0951370 -0.1456845 7.4.12 Choosing \\(b_j\\) in OLS In OLS, we choose \\(b_0, b_1, \\ldots, b_p\\) are chosen in a way that minimizes \\[ \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2 = \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 \\] OLS: \\(\\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2\\) sum((y-Pred_OLS)^2) ## [1] 56.94383 Ridge: \\(\\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2\\) sum((y-Pred_Ridge)^2) ## [1] 127.1331 Not surprisingly the OLS model achieves smaller \\(\\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2\\). This has to be true, since the OLS coefficients are chosen to minimize this quantity. 7.4.13 Choosing \\(b_j\\) in Ridge Regression In ridge regression, \\(b_0, b_1, \\ldots, b_p\\) are chosen in a way that minimizes \\[ \\begin{aligned} Q=&amp; \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2 + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\\\\ = &amp; \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2 \\end{aligned} \\] OLS: \\(\\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2 + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\\) sum((y-Pred_OLS)^2) + 0.6136*sum(coef(M_OLS_sc)[-1]^2) ## [1] 373.1205 Ridge: \\(\\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2 + \\lambda\\displaystyle\\sum_{j=1}^pb_j^2\\) sum((y-Pred_Ridge)^2) + 0.6136*sum((Ridge_coef)[-1]^2) ## [1] 130.3375 We see that the ridge coefficients achieve a lower value of Q than the OLS ones. 7.4.14 Lasso Regression Lasso regression is very similar to ridge regression. Coefficients \\(b_0, b_1, \\ldots, b_p\\) are chosen in a way that to minimizes \\[ \\begin{aligned} &amp; \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2 + \\lambda\\displaystyle\\sum_{j=1}^p|b_j|\\\\ = &amp; \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 + \\lambda\\displaystyle\\sum_{j=1}^p|b_j| \\end{aligned} \\] 7.5 Decision Trees 7.5.1 Basics of Decision Trees A decision tree is a flexible alternative to a regression model. It is said to be nonparametric because it does not involve parameters like \\(\\beta_0, \\beta_1, \\ldots \\beta_p\\). A tree makes no assumption about the nature of the relationship between the response and explanatory variables, and instead allows us to learn this relationship from the data. A tree makes prediction by repeatedly grouping together like observations in the training data. We can make predictions for a new case, by tracing it through the tree, and averaging responses of training cases in the same terminal node. 7.5.2 Decision Tree Example library(rpart) library(rpart.plot) tree &lt;- rpart(SalePrice~., data=Train_Data, cp=0.04) rpart.plot(tree, box.palette=&quot;RdBu&quot;, shadow.col=&quot;gray&quot;, nn=TRUE, cex=1, extra=1) The predicted price of a House with overall quality 7, and was built in 1995 is $200,000. The predicted price of a House overall quality 8 and 1,750 sq. ft. on the first floor is $370,000. 7.5.3 Partitioning in A Decision Tree For a quantitative response variable, data are split into two nodes so that responses in the same node are as similar as possible, while responses in the different nodes are as different as possible. Let L and R represent the left and right nodes from a possible split. Let \\(n_L\\) and \\(n_R\\) represent the number of observations in each node, and \\(\\bar{y}_L\\) and \\(\\bar{y}_R\\) represent the mean of the training data responses in each node. For each possible split, involving an explanatory variable, we calculate: \\[ \\displaystyle\\sum_{i=1}^{n_L} (y_i -\\bar{y}_L)^2 + \\displaystyle\\sum_{i=1}^{n_R} (y_i -\\bar{y}_R)^2 \\] We choose the split that minimizes this quantity. 7.5.4 Partitioning Example ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] ## x1 8 2 8 1 8 6 2 5 1 8 4 10 9 8 ## x2 5 3 1 1 4 3 8 1 10 8 6 5 0 2 ## y 253 64 258 21 257 203 246 114 331 256 213 406 326 273 ## [,15] ## x1 6 ## x2 1 ## y 155 7.5.5 One Possible Split (\\(x_1 &lt; 5.5\\)) We could split the data into 2 groups depending on whether \\(x_1 &lt; 5.5\\). 7.5.6 One Possible Split (\\(x_1 &lt; 5.5\\)) (cont.) \\(\\bar{y}_L = (331+246+213+21+64+114)/6 \\approx 164.84\\) \\(\\bar{y}_R = (203+155+256+253+257+273+258+326+406)/9 \\approx 265.22\\) \\[ \\begin{aligned} &amp; \\displaystyle\\sum_{i=1}^{n_L} (y_i -\\bar{y}_L)^2 \\\\ &amp; =(331-164.83)^2+(246-164.33)^2 + \\ldots+(114-164.33)^2 \\\\ &amp; =69958.83 \\end{aligned} \\] \\[ \\begin{aligned} \\displaystyle\\sum_{i=1}^{n_R} (y_i -\\bar{y}_R)^2 \\\\ &amp; =(203-265.22)^2+(155-265.22)^2 + \\ldots+(406-265.22)^2 \\\\ &amp; =39947.56 \\end{aligned} \\] 69958.83 + 39947.56 = 109906.4 7.5.7 Second Possible Split (\\(x_1 &lt; 6.5\\)) We could split the data into 2 groups depending on whether \\(x_1 &lt; 6.5\\). 7.5.8 Second Possible Split (\\(x_1 &lt; 6.5\\)) (cont.) \\(\\bar{y}_L = (331+246+213+21+64+114 + 203+155)/8 \\approx 168.375\\) \\(\\bar{y}_R = (256+253+257+273+258+326+406)/7 \\approx 289.857\\) \\[ \\begin{aligned} &amp; \\displaystyle\\sum_{i=1}^{n_L} (y_i -\\bar{y}_L)^2 \\\\ &amp; =(331-168.375)^2+(246-168.375)^2 + \\ldots+(203-168.375)^2 \\\\ &amp; =71411.88 \\end{aligned} \\] \\[ \\begin{aligned} \\displaystyle\\sum_{i=1}^{n_R} (y_i -\\bar{y}_R)^2 \\\\ &amp; =(203-289.857)^2+(155-289.857)^2 + \\ldots+(406-289.857)^2 \\\\ &amp; =19678.86 \\end{aligned} \\] 71411.88 + 19678.86 = 91090.74 The split at \\(x1 &lt; 6.5\\) is better than \\(x_1&lt;5.5\\) 7.5.9 Third Possible Split (\\(x_2 &lt; 5.5\\)) We could split the data into 2 groups depending on whether \\(x_2 &lt; 5.5\\). 7.5.10 Third Possible Split (\\(x_2 &lt; 5.5\\)) (cont.) \\(\\bar{y}_L = (331+246+213+256)/4 \\approx 261.5\\) \\(\\bar{y}_R = (21 + 64 + \\ldots + 406)/11 \\approx 211.82\\) \\[ \\begin{aligned} &amp; \\displaystyle\\sum_{i=1}^{n_L} (y_i -\\bar{y}_L)^2 \\\\ &amp; =(331-261.5)^2+(246-261.5)^2 + (213-261.5)^2+(256-261.5)^2 \\\\ &amp; =7453 \\end{aligned} \\] \\[ \\begin{aligned} \\displaystyle\\sum_{i=1}^{n_R} (y_i -\\bar{y}_R)^2 \\\\ &amp; =(21-211.82)^2+(64-211.82)^2 + \\ldots+(406-211.82)^2 \\\\ &amp; =131493.6 \\end{aligned} \\] 7453 + 131493.6 = 138946.6 7.5.11 Comparison of Splits Of the three split’s we’ve calculated, \\(\\displaystyle\\sum_{i=1}^{n_L} (y_i -\\bar{y}_L)^2 + \\displaystyle\\sum_{i=1}^{n_R} (y_i -\\bar{y}_R)^2\\) is minimized using \\(x_1 &lt; 6.5\\). In fact, if we calculate all possible splits over \\(x_1\\) and \\(x_2\\), \\(\\displaystyle\\sum_{i=1}^{n_L} (y_i -\\bar{y}_L)^2 + \\displaystyle\\sum_{i=1}^{n_R} (y_i -\\bar{y}_R)^2\\) is minimized by splitting on \\(x_1 &lt; 6.5\\) 7.5.12 First Split 7.5.13 First Split in the Tree 7.5.14 Next Splits Next, we find the best splits on the resulting two nodes. It turns out that the left node is best split on \\(x_2 &lt; 4.5\\), and the right node is best split on \\(x_1 &lt; 8.5\\). 7.5.15 Next Splits in Tree 7.5.16 Recursive Partitioning Splitting continues until nodes reach a certain predetermined minimal size, or until change improvement in model fit drops below a predetermined value 7.5.17 Tree on Housing Data 7.5.18 Model Complexity in Trees The more we partition data into smaller nodes, the more complex the model becomes. As we continue to partition, bias decreases, as cases are grouped with those that are more similar to themselves. On the other hand, variance increases, as there are fewer cases in each node to be averaged, putting more weight on each individual observation. Splitting into too small of nodes can lead to drastic overfitting. In the extreme case, if we split all the way to nodes of size 1, we would get RMSE of 0 on the training data, but should certainly not expect RMSPE of 0 on the test data. The optimal depth of the tree, or minimal size for terminal nodes can be determined using cross-validation. the rpart package uses a complexity parameter cp, which determines how much a split must improve model fit in order to be made. Smaller values of cp are associated with more complex tree models. 7.5.19 Cross-Validation on Housing Data cp_vals = 10^seq(-8, 1, length = 100) #cp_vals = c(0.00000001, 0.00001) colnames(Train_sc) &lt;- make.names(colnames(Train_sc)) #colnames(Train_Data) &lt;- make.names(colnames(Train_Data)) set.seed(11162020) Housing_Tree &lt;- train(data=Train_sc, SalePrice ~ ., method=&quot;rpart&quot;, trControl=control, tuneGrid=expand.grid(cp=cp_vals)) Housing_Tree$bestTune ## cp ## 52 0.0004328761 cp &lt;- Housing_Tree$results$cp RMSPE &lt;- Housing_Tree$results$RMSE ggplot(data=data.frame(cp, RMSPE), aes(x=cp, y=RMSPE))+geom_line() + xlim(c(0,0.001)) + ylim(c(0.475,0.485)) + ggtitle(&quot;Regression Tree Cross Validation Results&quot;) 7.5.20 Comparing OLS, Lasso, Ridge, and Tree set.seed(11162020) Housing_OLS &lt;- train(data=Train_sc, SalePrice ~ ., method=&quot;lm&quot;, trControl=control) set.seed(11162020) Housing_lasso &lt;- train(SalePrice ~., data = Train_sc, method = &quot;glmnet&quot;, trControl=control, tuneGrid=expand.grid(alpha=1, lambda=l_vals)) min(Housing_OLS $results$RMSE) ## [1] 0.5634392 min(Housing_ridge$results$RMSE) ## [1] 0.4570054 min(Housing_lasso$results$RMSE) ## [1] 0.4730849 min(Housing_Tree$results$RMSE) ## [1] 0.4783224 In this situation, the tree outperforms OLS, but does not do as well as lasso or ridge. The best model will vary depending on the nature of the data. We can use cross-validation to determine which model is likely to perform best in prediction. 7.5.21 Random Forest A popular extension of a decision tree is a random forest. A random forest consists of many (often ~10,000) trees. Predictions are made by averaging predictions from individual trees. In order to ensure the trees are different from each other: each tree is grown from a different bootstrap sample of the training data. when deciding on a split, only a random subset of explanatory variables are considered. Growing deep trees ensures low bias. In a random forest, averaging across many deep trees decreases variance, while maintaining low bias. 7.6 Regression Splines 7.6.1 Regression Splines We’ve seen that we can use polynomial regression to capture nonlinear trends in data. A regression spline is a piecewise function of polynomials. Here we’ll keep thing simple by focusing on a spline with a single explanatory variable. Splines can also be used for multivariate data. 7.6.2 Price of 2015 New Cars 7.6.3 Two Models with High Bias 7.6.4 Cubic Model 7.6.5 Cubic Splines 7.6.6 Cubic Splines region boundaries are called knots 7.6.7 Cubic Spline with 5 Knots 7.6.8 Cubic Spline with 10 Knots 7.6.9 Cubic Spline with 20 Knots 7.6.10 Model Evaluation predicted price for 35 new 2015 cars not in original dataset calculated mean square prediction error (MSPE) 7.6.11 Implementation of Splines Important Considerations: how many knots where to place knots degree of polynomial The best choices for all of these will vary between datasets and can be assessed through cross-validation. 7.7 Predicting House Prices Summary In the previous sections, we’ve applied various predictive modeling techniques to predict house prices in Ames, IA. In each section, we’ve focused on an individual predictive technique (OLS, ridge/lasso regression, trees, splines), but in practice, we often test out these techniques together to find which is likely to perform best on a set of data. Here, we’ll go through the steps to test out and evaluate these techiques on the Ames Housing dataset. There are no new statistical ideas presented in this section, just a synthesis of the preceeding material. We leave out splines, since we did not discuss using splines in a multivariate setting, but we compare OLS, ridge and decision trees. We use a subset of variables for illustrative purposes. set.seed(10302021) samp &lt;- sample(1:nrow(ames_raw), 1000) Houses &lt;- ames_raw[samp,] New_Houses &lt;- ames_raw &lt;- ames_raw[-samp,] New_Houses &lt;- New_Houses[1:5, ] We’ll begin by doing some data preparation. We standardize all explanatory variables in the training and new data. We do not standardize the response variable, price, so we can interpret predicted values more easily. Houses_Combined &lt;- rbind(Houses, New_Houses) Houses_sc &lt;- Houses_Combined %&gt;% mutate_if(is.numeric, scale) Houses_sc$SalePrice &lt;- as.numeric(Houses_Combined$SalePrice) Houses_sc_Train &lt;- Houses_sc[1:1000, ] Houses_sc_New &lt;- Houses_sc[1001:1005, ] The Houses_sc_Train dataset contains standardized values for the 1000 houses in the training data. The first six rows are shown below. head(Houses_sc_Train) ## Overall.Qual Year.Built Central.Air Gr.Liv.Area X1st.Flr.SF ## 2771 -0.07619084 0.79212207 Y -0.003620472 -0.9340706 ## 2909 -0.77229808 0.18658251 Y -0.392399656 -1.3891530 ## 2368 -0.07619084 0.01837707 Y -0.938684253 -1.6993209 ## 2604 -2.16451257 -1.89916487 Y -0.571836202 -1.1908489 ## 669 -0.07619084 -0.14982836 Y -0.918746859 -0.3111924 ## 1427 2.01213088 1.22945620 Y 0.528707949 1.5345608 ## Bedroom.AbvGr TotRms.AbvGrd Lot.Area Lot.Shape Land.Contour ## 2771 0.1632018 -0.2910546 0.28570669 IR1 Lvl ## 2909 0.1632018 -0.9160758 -0.88703104 Reg Lvl ## 2368 0.1632018 -0.2910546 -0.98002927 Reg Lvl ## 2604 0.1632018 -0.2910546 0.04235132 Reg Lvl ## 669 0.1632018 -0.2910546 0.17314883 IR1 Lvl ## 1427 0.1632018 0.9589877 0.20650820 Reg Lvl ## Overall.Cond Exter.Qual Heating.QC Paved.Drive SalePrice ## 2771 -0.4680319 Gd Ex Y 187000 ## 2909 0.4279149 TA TA Y 104500 ## 2368 1.3238618 TA Ex Y 116000 ## 2604 -1.3639788 TA TA Y 105000 ## 669 -1.3639788 Fa Gd Y 163000 ## 1427 -0.4680319 Ex Ex Y 395039 The Houses_sc_New displays standardized values for the new houses that we’re trying to predict. head(Houses_sc_New) ## Overall.Qual Year.Built Central.Air Gr.Liv.Area X1st.Flr.SF Bedroom.AbvGr ## 2 -0.77229808 -0.3516749 Y -1.2058453 -0.6772922 -1.0608118 ## 4 0.61991640 -0.1161873 Y 1.2145543 2.4091326 0.1632018 ## 6 -0.07619084 0.8930453 Y 0.2057222 -0.6010214 0.1632018 ## 7 1.31602364 0.9939686 Y -0.3246125 0.4464308 -1.0608118 ## 8 1.31602364 0.6911988 Y -0.4402494 0.2989739 -1.0608118 ## TotRms.AbvGrd Lot.Area Lot.Shape Land.Contour Overall.Cond Exter.Qual ## 2 -0.9160758 0.1877886 Reg Lvl 0.4279149 TA ## 4 0.9589877 0.1323496 Reg Lvl -0.4680319 Gd ## 6 0.3339665 -0.0094877 IR1 Lvl 0.4279149 TA ## 7 -0.2910546 -0.6164362 Reg Lvl -0.4680319 Gd ## 8 -0.9160758 -0.6062364 IR1 HLS -0.4680319 Gd ## Heating.QC Paved.Drive SalePrice ## 2 TA Y 105000 ## 4 Ex Y 244000 ## 6 Ex Y 195500 ## 7 Ex Y 213500 ## 8 Ex Y 191500 Since the glmnet command requires training data to be entered as a matrix, we create versions of the datasets in matrix form. Houses_sc$SalePrice[is.na(Houses$SalePrice)] &lt;- 0 #can&#39;t take NA&#39;s when fitting model matrix, doesn&#39;t matter since only need x-coeffs Houses_sc_Combined_MAT &lt;- model.matrix(SalePrice~., data=rbind(Houses_sc)) Houses_sc_Train_MAT &lt;- Houses_sc_Combined_MAT[1:1000, ] Houses_sc_New_MAT &lt;- Houses_sc_Combined_MAT[1001:1005, ] 7.7.1 Modeling with OLS We first fit an ordinary least squares regression model to the data. Housing_OLS &lt;- lm(data=Houses_sc_Train, SalePrice~ .) coef(Housing_OLS) ## (Intercept) Overall.Qual Year.Built Central.AirY Gr.Liv.Area ## 238908.0614 23368.4152 14036.2549 -4497.1153 29640.4606 ## X1st.Flr.SF Bedroom.AbvGr TotRms.AbvGrd Lot.Area Lot.ShapeIR2 ## 8320.1472 -5011.0485 1554.7785 7566.9377 1570.1676 ## Lot.ShapeIR3 Lot.ShapeReg Land.ContourHLS Land.ContourLow Land.ContourLvl ## 19082.7508 -4566.5111 44704.8906 22406.5959 19096.4163 ## Overall.Cond Exter.QualFa Exter.QualGd Exter.QualTA Heating.QCFa ## 7965.6704 -68750.2773 -62800.5804 -74028.3841 -3972.4036 ## Heating.QCGd Heating.QCPo Heating.QCTA Paved.DriveP Paved.DriveY ## -4478.6876 -23000.4394 -5272.7136 -1901.9235 709.1532 7.7.2 Ridge Regression with Housing Data Now, we’ll use ridge regression to predict insurance costs. We use cross validation to determine the optimal value of lamba. We perform 10 repeats of 10-fold cross-validation. We test 100 lambda-values ranging from \\(10^-5\\) to \\(10^5\\). control = trainControl(&quot;repeatedcv&quot;, number = 10, repeats=10) l_vals = 10^seq(-5, 5, length = 100) set.seed(2022) Housing_ridge &lt;- train( SalePrice ~ ., data = Houses_sc_Train, method = &quot;glmnet&quot;, trControl=control , tuneGrid=expand.grid(alpha=0, lambda=l_vals)) Housing_ridge$bestTune$lambda ## [1] 6135.907 We fit a model to the full training dataset using the optimal value of \\(lambda\\) . ridge_mod &lt;- glmnet(x=Houses_sc_Train_MAT, y=Houses_sc_Train$SalePrice, alpha = 0, lambda=Housing_ridge$bestTune$lambda ) coef(ridge_mod) ## 26 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s0 ## (Intercept) 206079.5442 ## (Intercept) . ## Overall.Qual 24716.6502 ## Year.Built 12909.8224 ## Central.AirY -1693.4832 ## Gr.Liv.Area 24137.8185 ## X1st.Flr.SF 10707.5001 ## Bedroom.AbvGr -5034.2266 ## TotRms.AbvGrd 5394.5170 ## Lot.Area 7086.7613 ## Lot.ShapeIR2 3322.3720 ## Lot.ShapeIR3 18987.3176 ## Lot.ShapeReg -5345.7478 ## Land.ContourHLS 41239.7682 ## Land.ContourLow 15011.2269 ## Land.ContourLvl 12784.0351 ## Overall.Cond 6560.4987 ## Exter.QualFa -29042.5581 ## Exter.QualGd -24942.6445 ## Exter.QualTA -35102.6069 ## Heating.QCFa -8118.6371 ## Heating.QCGd -6380.1279 ## Heating.QCPo -19693.6611 ## Heating.QCTA -7645.0855 ## Paved.DriveP -613.3798 ## Paved.DriveY 1324.2704 The regression coefficients are displayed together with the OLS coefficients in a data.frame. Ridge_coef &lt;- as.vector(ridge_mod$beta)[-1] #leave off intercept using [-1] OLS_coef &lt;- coef(Housing_OLS)[-1] data.frame(OLS_coef, Ridge_coef) ## OLS_coef Ridge_coef ## Overall.Qual 23368.4152 24716.6502 ## Year.Built 14036.2549 12909.8224 ## Central.AirY -4497.1153 -1693.4832 ## Gr.Liv.Area 29640.4606 24137.8185 ## X1st.Flr.SF 8320.1472 10707.5001 ## Bedroom.AbvGr -5011.0485 -5034.2266 ## TotRms.AbvGrd 1554.7785 5394.5170 ## Lot.Area 7566.9377 7086.7613 ## Lot.ShapeIR2 1570.1676 3322.3720 ## Lot.ShapeIR3 19082.7508 18987.3176 ## Lot.ShapeReg -4566.5111 -5345.7478 ## Land.ContourHLS 44704.8906 41239.7682 ## Land.ContourLow 22406.5959 15011.2269 ## Land.ContourLvl 19096.4163 12784.0351 ## Overall.Cond 7965.6704 6560.4987 ## Exter.QualFa -68750.2773 -29042.5581 ## Exter.QualGd -62800.5804 -24942.6445 ## Exter.QualTA -74028.3841 -35102.6069 ## Heating.QCFa -3972.4036 -8118.6371 ## Heating.QCGd -4478.6876 -6380.1279 ## Heating.QCPo -23000.4394 -19693.6611 ## Heating.QCTA -5272.7136 -7645.0855 ## Paved.DriveP -1901.9235 -613.3798 ## Paved.DriveY 709.1532 1324.2704 7.7.3 Decision Tree Now, we’ll predict house prices using using a decision tree. First, we grow and display a small decision tree, by setting the cp parameter equal to 0.05. tree &lt;- rpart(SalePrice~., data=Houses_sc_Train, cp=0.05) rpart.plot(tree, box.palette=&quot;RdBu&quot;, shadow.col=&quot;gray&quot;, nn=TRUE, cex=1, extra=1) Now we use cross-validation to determine the optimal value of the cp parameter. We use 10 repeats of 10-fold cross-validation. We test 1000 cp-values ranging from \\(10^-5\\) to \\(10^5\\). cp_vals = 10^seq(-5, 5, length = 100) colnames(Houses_sc_Train) &lt;- make.names(colnames(Houses_sc_Train)) set.seed(2022) Housing_Tree &lt;- train(data=Houses_sc_Train, SalePrice ~ ., method=&quot;rpart&quot;, trControl=control,tuneGrid=expand.grid(cp=cp_vals)) Housing_Tree$bestTune ## cp ## 4 0.00002009233 We grow a full tree using the optimal cp value. Housing_Best_Tree &lt;- rpart(SalePrice~., data=Houses_sc_Train, cp=Housing_Tree$bestTune) 7.7.4 Comparing Performance We use cross-validation to compare the performance of the linear model, ridge regression model, and decision tree. set.seed(2022) Housing_OLS &lt;- train(data=Houses_sc_Train, SalePrice ~ ., method=&quot;lm&quot;, trControl=control) min(Housing_OLS$results$RMSE) ## [1] 35313.3 min(Housing_ridge$results$RMSE) ## [1] 35747.22 min(Housing_Tree$results$RMSE) ## [1] 33675.53 The tree predictions give slightly lower RMSPE. 7.7.5 Predictions on New Data We now predict the sale price of the five new houses using each technique. Ordinary Least-Squares model: OLS_pred &lt;- predict(Housing_OLS, newdata=Houses_sc_New) head(OLS_pred) ## 2 4 6 7 8 ## 114709.4 253695.8 195078.1 222117.6 242493.9 Ridge regression model: We use the Customers_sc_New_MAT dataset, since the glmnet package requires inputs in matrix form. ridge_pred &lt;- predict(ridge_mod, newx=Houses_sc_New_MAT) head(ridge_pred) ## s0 ## 2 114950.4 ## 4 259359.8 ## 6 195288.0 ## 7 226841.6 ## 8 249064.8 Decision tree: tree_pred &lt;- predict(Housing_Best_Tree, newdata=Houses_sc_New) head(tree_pred) ## 2 4 6 7 8 ## 132926.5 287045.1 183978.6 207079.4 207079.4 7.8 Assumptions in Predictive Models 7.8.1 Assumptions in Predictive Models Like any other statistical technique, predictive inference (sometimes done through machine learning algorithms) depends on the validity of assumptions. The response variable observed in the data is actually the thing we want to predict Training/Test data representative of population of interest Prediction accuracy is appropriate metric Below are some examples of real uses of predictive inference in which some of these assumptions were violated, leading to inappropriate and unethical conclusions. 7.8.2 Amazon Hiring Algorithm In 2014, Amazon began working on an algorithm to predict whether a job applicant would be suitable for hire for software developer positions, based on characteristics of their job application. response variable: rating of candidate’s strength (1-5) explanatory variables: many variables based on information included on the resume (e.g. highest degree, major, GPA, college/university, prior job experiences, internships, frequency of certain words on resume, etc.) The algorithm was trained using data from past applications, rated by humans, over the past 10 years. It could then be used to predict ratings of future job applicants. According to [Reuters])(https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G), “In effect, Amazon’s system taught itself that male candidates were preferable. It penalized resumes that included the word “women’s,” as in “women’s chess club captain.” And it downgraded graduates of two all-women’s colleges, according to people familiar with the matter.\" While the algorithm was intended to predict candidate quality, the response variable on the training data actually reflected biases in past hiring decisions, leading the algorithm to do the same. 7.8.3 Facial Recognition Facial recognition technology is used by law enforcement surveillance, airport passenger screening, and employment and housing decisions. It has, however, been banned for use by police in some cities, including San Francisco and Boston, due to concerns about inequity and privacy. Research has shown that although certain facial recognition algorithms achieve over 90% accuracy overall, accuracy rate is lower among subjects who are female, Black, or 18-30 years old. This is likely due, at least in part, to the algorithms being trained primarily on data an images of people who are not members of these groups. Although the algorithms might attain strong accuracy overall, it is inappropriate to evaluate them on this basis, without accounting for performance on subgroups in the population. 7.8.4 Comments The biases and assumptions noted above are not reasons to abandon predictive modeling, but rather flaws to be aware of and work to correct. Predictive algorithms, are only as good as the data on which they are trained and the societies in which they are developed, and will reflect inherent biases. Thus, they should be used cautiously and with with human judgment, just like any other statistical technique. Beware of statements like: “The data say this!” “The algorithm is objective.” “The numbers don’t lie.” Any data-driven analysis depends on assumptions, and sound judgment and awareness of context are required when assessing the validaty of conclusions drawn. 7.8.5 Modeling for Prediction Goal is to make the most accurate predictions possible. Not concerned with understanding relationships between variables. Not worried model being to complicated to interpret, as long as it yields good predictions. Aim for a model that best captures the signal in the data, without being thrown off by noise. - Large number of predictors is ok - Don’t make model so complicated that it overfits the data. Be sure that model is predicting what you intend it to Reflective of biases inherent in the data on which it was trained "],["logistic-regression-and-classification.html", "Chapter 8 Logistic Regression and Classification 8.1 Logistic Regression 8.2 Interpretations in a Logistic Regression Model 8.3 Multiple Logistic Regression 8.4 Assessing a Classifier’s Performance 8.5 Receiver Operating Characteristic Curve", " Chapter 8 Logistic Regression and Classification Learning Outcomes: Define and distinguish between probability, odds, and odds ratio. Identify situations where it is appropriate to use logistic regression. Estimate probabilities, odds, and odds ratios using logistic regression. Interpret coefficients in a logistic regression model. Explain how probability estimates are obtained from decision trees and random forests. Construct and interpret a confusion matrix, given probability estimates and true results. Define specificity and sensitivity, and calculate them for given data. Explain the information contained in a receiver operating characteristic (ROC) curve. Construct receiver operating curves for small sets of data. Compare classifiers using misclassification rate, and AUC. 8.1 Logistic Regression 8.1.1 Modeling Binary Response So far, we have modeled only quantitative response variables. The normal error regression model makes the assumption that the response variable is normally distributed, given the value(s) of the explanatory variables. Now, we’ll look at how to model a categorical response variable. We’ll consider only situations where the response is binary (i.e. has 2 categories) Problems with categorical response variables are sometimes called classification problems, while problems with numeric response variables are sometimes called regression problems. 8.1.2 Credit Card Dataset We’ll consider a dataset pertaining to 10,000 credit cards. The goal is to predict whether or not the user will default on the payment, using information on the credit card balance, user’s annual income, and whether or not the user is a student. Data come from Introduction to Statistical Learning by James, Witten, Hastie, Tibshirani. library(ISLR) data(Default) summary(Default) ## default student balance income ## No :9667 No :7056 Min. : 0.0 Min. : 772 ## Yes: 333 Yes:2944 1st Qu.: 481.7 1st Qu.:21340 ## Median : 823.6 Median :34553 ## Mean : 835.4 Mean :33517 ## 3rd Qu.:1166.3 3rd Qu.:43808 ## Max. :2654.3 Max. :73554 8.1.3 Default and Balance ggplot(data=Default, aes(y=default, x=balance)) + geom_point(alpha=0.2) 8.1.4 Linear Regression Model for Default #convert default from yes/no to 0/1 Default$default &lt;- as.numeric(Default$default==&quot;Yes&quot;) ggplot(data=Default, aes(y=default, x= balance)) + geom_point(alpha=0.2) + stat_smooth(method=&quot;lm&quot;, se=FALSE) There are a lot of problems with this model! 8.1.5 Transforming into interval (0,1) Starting with our linear model \\(E(Y_i) = \\beta_0+\\beta_1x_{i1}\\), we need to transform \\(\\beta_0+\\beta_1x_{i1}\\) into (0,1). Let \\(\\pi_i = \\frac{e^{\\beta_0+\\beta_1x_{i1} }}{1+e^{\\beta_0+\\beta_1x_{i1}}}\\). Then \\(0 \\leq \\pi_i \\leq 1\\), and \\(\\pi_i\\) represents an estimate of \\(P(Y_i=1)\\). This function maps the values of \\(\\beta_0+\\beta_1x_{i1}\\) into the interval (0,1). The logistic regression model assumes that: \\(Y_i \\in \\{0,1\\}\\) \\(E(Y_i) = P(Y_i=1) = \\pi_i=\\frac{e^{\\beta_0+\\beta_1x_{i1} + \\ldots \\beta_px_{ip}}}{1+e^{\\beta_0+\\beta_1x_{i1} + \\ldots \\beta_px_{ip}}}\\) i.e. \\(\\beta_0+\\beta_1x_{i1} + \\ldots \\beta_px_{ip}= \\text{log}\\left(\\frac{\\pi_i}{1-\\pi_i}\\right).\\) (This is called the logit function and can be written \\(\\text{logit}(\\pi_i)\\). Instead of assuming that the expected response is a linear function of the explanatory variables, we are assuming that it is a function of a linear function of the explanatory variables. 8.1.6 Logistic Regression Model for Default ggplot(data=Default, aes(y=default, x= balance)) + geom_point(alpha=0.2) + stat_smooth(method=&quot;glm&quot;, se=FALSE, method.args = list(family=binomial)) 8.1.7 Fitting the Logistic Regression Model in R CCDefault_M &lt;- glm(data=Default, default ~ balance, family = binomial(link = &quot;logit&quot;)) summary(CCDefault_M) ## ## Call: ## glm(formula = default ~ balance, family = binomial(link = &quot;logit&quot;), ## data = Default) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2697 -0.1465 -0.0589 -0.0221 3.7589 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -10.6513306 0.3611574 -29.49 &lt;0.0000000000000002 *** ## balance 0.0054989 0.0002204 24.95 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2920.6 on 9999 degrees of freedom ## Residual deviance: 1596.5 on 9998 degrees of freedom ## AIC: 1600.5 ## ## Number of Fisher Scoring iterations: 8 8.1.8 The Logistic Regression Equation The regression equation is: \\[ P(\\text{Default}) = \\hat{\\pi}_i = \\frac{e^{-10.65+0.0055\\times\\text{balance}}}{1+e^{-10.65+0.0055\\times\\text{balance}}} \\] For a $1,000 balance, the estimated default probability is \\(\\frac{e^{-10.65+0.0055(1000) }}{1+e^{-10.65+0.0055(1000)}} \\approx 0.006\\) For a $1,500 balance, the estimated default probability is \\(\\frac{e^{-10.65+0.0055(1500) }}{1+e^{-10.65+0.0055(1500)}} \\approx 0.08\\) For a $2,000 balance, the estimated default probability is \\(\\frac{e^{-10.65+0.0055(2000) }}{1+e^{-10.65+0.0055(2000)}} \\approx 0.59\\) 8.1.9 Predict in R predict(CCDefault_M, newdata=data.frame((balance=1000)), type=&quot;response&quot;) ## 1 ## 0.005752145 predict(CCDefault_M, newdata=data.frame((balance=1500)), type=&quot;response&quot;) ## 1 ## 0.08294762 predict(CCDefault_M, newdata=data.frame((balance=2000)), type=&quot;response&quot;) ## 1 ## 0.5857694 8.1.10 Where do the b’s come from? Recall that for a quantitative response variable, the values of \\(b_1, b_2, \\ldots, b_p\\) are chosen in a way that minimizes \\(\\displaystyle\\sum_{i=1}^n \\left(y_i-(\\beta_0+\\beta_1x_{i1}+\\ldots+\\beta_px_{ip})^2\\right)\\). Least squares does not work well in this generalized setting. Instead, the b’s are calculated using a more advanced technique, known as maximum likelihood estimation. 8.2 Interpretations in a Logistic Regression Model 8.2.1 Recall Logistic Regression Curve for Credit Card Data ggplot(data=Default, aes(y=default, x= balance)) + geom_point(alpha=0.2) + stat_smooth(method=&quot;glm&quot;, se=FALSE, method.args = list(family=binomial)) 8.2.2 Recall Credit Card Model Output M &lt;- glm(data=Default, default ~ balance, family = binomial(link = &quot;logit&quot;)) summary(M) ## ## Call: ## glm(formula = default ~ balance, family = binomial(link = &quot;logit&quot;), ## data = Default) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2697 -0.1465 -0.0589 -0.0221 3.7589 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -10.6513306 0.3611574 -29.49 &lt;0.0000000000000002 *** ## balance 0.0054989 0.0002204 24.95 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2920.6 on 9999 degrees of freedom ## Residual deviance: 1596.5 on 9998 degrees of freedom ## AIC: 1600.5 ## ## Number of Fisher Scoring iterations: 8 8.2.3 Balance Logistic Model Equation The regression equation is: \\[ P(\\text{Default}) = \\hat{\\pi}_i = \\frac{e^{-10.65+0.0055\\times\\text{balance}}}{1+e^{-10.65+0.0055\\times\\text{balance}}} \\] For a $1,000 balance, the estimated default probability is \\(\\hat{\\pi}_i=\\frac{e^{10.65+0.0055(1000) }}{1+e^{10.65+0.0055(1000)}} \\approx 0.005752145\\) For a $1,500 balance, the estimated default probability is \\(\\hat{\\pi}_i=\\frac{e^{10.65+0.0055(1500) }}{1+e^{10.65+0.0055(1500)}} \\approx 0.08294762\\) For a $2,000 balance, the estimated default probability is \\(\\hat{\\pi}_i=\\frac{e^{10.65+0.0055(2000) }}{1+e^{10.65+0.0055(2000)}} \\approx 0.5857694\\) 8.2.4 Odds and Odds Ratio For an event with probability \\(p\\), the odds of the event occurring are \\(\\frac{p}{1-p}\\). Examples: 1. The odds of a fair coin landing heads are \\(\\frac{0.5}{1-0.5}=1\\), sometimes written 1:1. The odds of a fair 6-sided die landing on a 1 are \\(\\frac{1/6}{1-1/6}=\\frac{1}{5}\\), sometimes written 1:5. The odds of a randomly selected month having 31 days are \\(\\frac{17/12}{1-7/12}=\\frac{7}{5}\\), sometimes written 7:5, or 1.4:1. We are often interested in studying the odds ratio between two different events. This is the ratio of the odds of one event occurring, relative to another. For example, if Patient A has a 0.05 probability of having a certain disease, and Patient B has a 0.01 probability, we compute the odds ratio of disease between the two patients as follows: Odds for patient A: \\(\\frac{0.05}{1-0.05} = \\frac{1}{19}\\). Odds for patient B: \\(\\frac{0.01}{1-0.01} = \\frac{1}{99}\\). Odds ratio: \\(\\frac{\\frac{1}{19}}{\\frac{1}{99}}=\\frac{99}{19}\\approx5.21\\), or \\(5.21:1\\). The odds of Patient A having the disease are 5.21 times as great as the odds of Patient B having it. 8.2.5 Odds in Logistic Regression The odds of default are given by \\(\\frac{\\pi_i}{1-\\pi_i}\\). Examples: The estimated odds of default for a $1,000 balance are \\(\\frac{0.005752145}{1-0.005752145} \\approx 1:173.\\) The estimated odds of default for a $1,500 balance are \\(\\frac{0.08294762 }{1-0.08294762 } \\approx 1:11.\\) The estimated odds of default for a $2,000 balance are \\(\\frac{0.5857694}{1-0.5857694} \\approx 1.414:1.\\) 8.2.6 Odds Ratio in Regression The quantity \\(\\frac{\\frac{\\pi_i}{1-\\pi_i}}{\\frac{\\pi_j}{1-\\pi_j}}\\) represents the odds ratio of a default for user \\(i\\), compared to user \\(j\\). This quantity is called the odds ratio. Example: The default odds ratio for a $1,000 payment, compared to a $2,000 payment is The odds ratio is \\(\\frac{\\frac{1}{173}}{\\frac{1.414}{1}}\\approx 1:244.\\) The odds of a default are about 244 times larger for a $2,000 payment than a $1,000 payment. 8.2.7 Interpretation of \\(\\beta_1\\) Consider the odds ratio for a case \\(j\\) with explanatory variable \\(x + 1\\), compared to case \\(i\\) with explanatory variable \\(x\\). That is \\(\\text{log}\\left(\\frac{\\pi_i}{1-\\pi_i}\\right) = \\beta_0+\\beta_1x\\), and \\(\\text{log}\\left(\\frac{\\pi_j}{1-\\pi_j}\\right) = \\beta_0+\\beta_1(x+1)\\). \\(\\text{log}\\left(\\frac{\\frac{\\pi_j}{1-\\pi_j}}{\\frac{\\pi_i}{1-\\pi_i}}\\right)=\\text{log}\\left(\\frac{\\pi_j}{1-\\pi_j}\\right)-\\text{log}\\left(\\frac{\\pi_i}{1-\\pi_j}\\right)=\\beta_0+\\beta_1(x+1)-(\\beta_0+\\beta_1(x))=\\beta_1.\\) For every 1-unit increase in \\(x\\) we expect the log odds of “success” to multiply by a factor of \\(\\beta_1\\). For every 1-unit increase in \\(x\\) we expect the odds of “success” to multiply by a factor of \\(e^{\\beta_1}\\). 8.2.8 Intrepretation in Credit Card Example \\(b_1=0.0055\\) For each 1-dollar increase in balance on the credit card., the odds of default are estimated to multiply by \\(e^{0.0055}\\approx1.0055\\). That is, for each additional dollar on the card balance, the odds of default are estimated to increase by 0.55% For each increase of \\(d\\) dollars in credit card balance, odds of default are estimated to multiply by a factor of \\(e^{0.0055d}\\). For every $1,000 increase in balance, the odds of default are expected to multiply by a factor of \\(e^{0.0055\\times 1000}\\approx 244\\). Thus, the odds of default for a balance of $2,000 are estimated to be \\(e^{0.0055\\times 1000}\\approx 244\\) times as great as the odds of default for a $1,000 balance. 8.2.9 Hypothesis Tests in Logistic Regression The p-value on the “balance” line of the regression output is associated with the null hypothesis \\(\\beta_1=0\\), that is that there is no relationship between balance and the odds of defaulting on the payment. The fact that the p-value is so small tells us that there is strong evidence of a relationship between balance and odds of default. 8.2.10 Confidence Intervals for \\(\\beta_1\\) confint(M, level = 0.95) ## 2.5 % 97.5 % ## (Intercept) -11.383288936 -9.966565064 ## balance 0.005078926 0.005943365 We are 95% confident that for each 1 dollar increase in credit card balance, the odds of default are expected to multiply by a factor between \\(e^{0.00508}\\approx 1.0051\\) and \\(e^{0.00594}\\approx 1.0060\\). This is a profile-likelihood interval, which you can read more about here. 8.3 Multiple Logistic Regression 8.3.1 Logistic Regression Models with Multiple Explanatory Variables We can also perform logistic regression in situations where there are multiple explanatory variables. 8.3.2 Logistic Model with Multiple Predictors CCDefault_M2 &lt;- glm(data=Default, default ~ balance + student, family = binomial(link = &quot;logit&quot;)) summary(CCDefault_M2) ## ## Call: ## glm(formula = default ~ balance + student, family = binomial(link = &quot;logit&quot;), ## data = Default) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4578 -0.1422 -0.0559 -0.0203 3.7435 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -10.7494959 0.3691914 -29.116 &lt; 0.0000000000000002 *** ## balance 0.0057381 0.0002318 24.750 &lt; 0.0000000000000002 *** ## studentYes -0.7148776 0.1475190 -4.846 0.00000126 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2920.6 on 9999 degrees of freedom ## Residual deviance: 1571.7 on 9997 degrees of freedom ## AIC: 1577.7 ## ## Number of Fisher Scoring iterations: 8 8.3.3 Multiple Logistic Model Illustration ggplot(data=Default, aes(y=default, x= balance, color=student)) + geom_point(alpha=0.2) + stat_smooth(method=&quot;glm&quot;, se=FALSE, method.args = list(family=binomial)) 8.3.4 Multiple Logistic Regression Interpretation The regression equation is: \\[ P(\\text{Default}) = \\hat{\\pi}_i = \\frac{e^{-10.75+0.005738\\times\\text{balance}-0.7149\\times\\text{I}_{\\text{student}}}}{1+e^{-10.75+0.005738\\times\\text{balance}-0.7149\\times\\text{I}_{\\text{student}}}} \\] For each 1 dollar increase in balance, the odds of default are estimated to multiply by a factor \\(e^{0.005738}\\approx 1.00575\\), whether the user is a student or nonstudent. Thus, the estimated odds of default increase by about 0.5%, for each 1-dollar increase in balance.. For every $100 increase in balance, the odds of default are estimated to multiply by \\(e^{0.005738\\times100}\\approx 1.775\\) for students as well as nonstudents. Thus, the estimated odds of default increase by about 77.5%. The odds of default for students are estimated to be \\(e^{-0.7149} \\approx 0.49\\) as high for students as non-students, assuming balance amount is held constant. 8.3.5 Hypothesis Tests in Multiple Logistic Regression Model There is strong evidence of a relationship between balance and odds of default, provided we are comparing students to students, or nonstudents to nonstudents. There is evidence that students are less likely to default than nonstudents, provided the balance on the card is the same. 8.3.6 Multiple Logistic Regression Model with Interaction CCDefault_M_Int &lt;- glm(data=Default, default ~ balance * student, family = binomial(link = &quot;logit&quot;)) summary(CCDefault_M_Int) ## ## Call: ## glm(formula = default ~ balance * student, family = binomial(link = &quot;logit&quot;), ## data = Default) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4839 -0.1415 -0.0553 -0.0202 3.7628 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -10.8746818 0.4639679 -23.438 &lt;0.0000000000000002 *** ## balance 0.0058188 0.0002937 19.812 &lt;0.0000000000000002 *** ## studentYes -0.3512310 0.8037333 -0.437 0.662 ## balance:studentYes -0.0002196 0.0004781 -0.459 0.646 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2920.6 on 9999 degrees of freedom ## Residual deviance: 1571.5 on 9996 degrees of freedom ## AIC: 1579.5 ## ## Number of Fisher Scoring iterations: 8 8.3.7 Interpretations for Logistic Model with Interaction The regression equation is: \\[ P(\\text{Default}) = \\hat{\\pi}_i = \\frac{e^{-10.87+0.0058\\times\\text{balance}-0.35\\times\\text{I}_{\\text{student}}-0.0002\\times\\text{balance}\\times{\\text{I}_{\\text{student}}}}}{1+e^{-10.87+0.0058\\times\\text{balance}-0.35\\times\\text{I}_{\\text{student}}-0.0002\\times\\text{balance}\\times{\\text{I}_{\\text{student}}}}} \\] Equation for Students \\[ P(\\text{Default}) = \\hat{\\pi}_i = \\frac{e^{-10.52+0.0056\\times\\text{balance}}}{1+e^{-10.52+0.0056\\times\\text{balance}}} \\] Assuming a person is a student, for every $100 increase in balance, the odds of default are expected to multiply by a factor of \\(e^{0.0056\\times 100}=1.75\\), a 75% increase. Equation for Non-Students \\[ P(\\text{Default}) = \\hat{\\pi}_i = \\frac{e^{-10.87+0.0058\\times\\text{balance}}}{1+e^{-10.87+0.0058\\times\\text{balance}}} \\] Assuming a person is a student, for every $100 increase in balance, the odds of default are expected to multiply by a factor of \\(e^{0.0058\\times 100}=1.786\\), a 78.6% increase. Since estimate of the interaction effect is so small and the p-value on this estimate is large, it is plausible that there is no interaction at all. Thus, the simpler non-interaction model is preferable. 8.3.8 Logistic Regression Key Points \\(Y\\) is a binary response variable. \\(\\pi_i\\) is a function of explanatory variables \\(x_{i1}, \\ldots x_{ip}\\). \\(E(Y_i) = \\pi_i = \\frac{e^{\\beta_0+\\beta_1x_i + \\ldots\\beta_px_{ip}}}{1+e^{\\beta_0+\\beta_1x_i + \\ldots\\beta_px_{ip}}}\\) \\(\\beta_0+\\beta_1x_i + \\ldots\\beta_px_{ip} = \\text{log}\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)\\) For quantitative \\(x_j\\), when all other explanatory variables are held constant, the odds of “success” multiply be a factor of \\(e^{\\beta_j}\\) for each 1 unit increase in \\(x_j\\) For categorical \\(x_j\\), when all other explanatory variables are held constant, the odds of “success” are \\(e^\\{beta_j}\\) times higher for category \\(j\\) than for the “baseline category.” For models with interaction, we can only interpret \\(\\beta_j\\) when the values of all other explanatory variables are given (since the effect of \\(x_j\\) depends on the other variables.) 8.4 Assessing a Classifier’s Performance 8.4.1 Measuring Prediction Accuracy We’ve seen \\(\\text{RMSPE} = \\sqrt{\\displaystyle\\sum_{i=1}^{n}{(\\hat{y}_i-y_i)^2}}\\) used as a measure of predictive accuracy in a regression problem. Why might this not be the best measure of prediction accuracy in a classification problem (i.e. one with a binary response)? 8.4.2 Classification Accuracy A common way to measure the accuracy of a classifier is to calculate the percentage of cases for which it correctly predicts the class, or category of the response variable. Let’s calculate classification accuracy for the logistic regression model. We’ll use cross-validation, by randomly witholding 1,000 of the 10,000 of the cases, on which we’ll make predictions. set.seed(08172022) samp &lt;- sample(1:nrow(Default), 1000) Default_Test &lt;- Default[samp, ] Default_Train &lt;- Default[-samp, ] We fit the model with interaction to the training data: LR_Default_M_Int &lt;- glm(data=Default_Train, default ~ balance * student, family = binomial(link = &quot;logit&quot;)) summary(LR_Default_M_Int) ## ## Call: ## glm(formula = default ~ balance * student, family = binomial(link = &quot;logit&quot;), ## data = Default_Train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.5610 -0.1373 -0.0515 -0.0180 3.8242 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -11.2714061 0.5188284 -21.725 &lt;0.0000000000000002 *** ## balance 0.0060696 0.0003273 18.547 &lt;0.0000000000000002 *** ## studentYes 0.0924588 0.8606304 0.107 0.914 ## balance:studentYes -0.0004749 0.0005142 -0.924 0.356 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2617.1 on 8999 degrees of freedom ## Residual deviance: 1385.5 on 8996 degrees of freedom ## AIC: 1393.5 ## ## Number of Fisher Scoring iterations: 8 8.4.3 First 50 Cases Let’s look at the predicted probability of default for the first 50 test cases, along with whether or not the person actually defaulted. LR_Prob &lt;- predict(LR_Default_M_Int, newdata=Default_Test, type=&quot;response&quot;) LR_Predict_Default &lt;- factor(ifelse(LR_Prob &gt; 0.5, &quot;Yes&quot;, &quot;No&quot;)) Actual_Default &lt;- factor(ifelse(Default_Test$default==1, &quot;Yes&quot;, &quot;No&quot;)) LR_Res_df &lt;- data.frame(LR_Prob, LR_Predict_Default, Actual_Default) kable(head(LR_Res_df, 50)%&gt;% arrange(desc(LR_Prob))) LR_Prob LR_Predict_Default Actual_Default 2465 0.5400853 Yes No 1228 0.2570897 No No 6656 0.1350159 No No 1185 0.1286163 No No 9963 0.1160190 No No 6635 0.0713025 No Yes 5921 0.0623171 No No 9691 0.0558990 No No 9755 0.0222295 No No 7569 0.0191176 No No 9435 0.0129212 No No 1290 0.0129187 No No 7023 0.0112015 No No 2222 0.0102643 No No 6591 0.0097522 No No 3674 0.0082435 No No 6764 0.0059578 No No 7848 0.0058386 No No 7491 0.0055477 No No 9386 0.0053516 No No 3022 0.0038748 No No 8081 0.0033171 No No 4598 0.0030966 No No 8204 0.0025254 No No 4023 0.0024937 No No 2962 0.0020771 No No 1792 0.0020008 No No 9722 0.0019758 No No 6579 0.0014153 No No 5295 0.0009563 No No 576 0.0008414 No No 27 0.0006303 No No 6702 0.0006275 No No 2047 0.0005230 No No 742 0.0004324 No No 1253 0.0003789 No No 2226 0.0003423 No No 5409 0.0002650 No No 4282 0.0002322 No No 6298 0.0002287 No No 2547 0.0002225 No No 5206 0.0002186 No No 2213 0.0001745 No No 1215 0.0001467 No No 8807 0.0001466 No No 9790 0.0001379 No No 6578 0.0000895 No No 9977 0.0000479 No No 9808 0.0000192 No No 63 0.0000127 No No What percentage of these 50 cases did the logistic regression model classify correctly? 8.4.4 Confusion Matrix A confusion matrix is a two-by-two table displaying the number of cases predicted in each category as columns, and the number of cases actually in each category as rows Actually Negative Actually Positive Predicted Negative True Negative False Negative Predicted Positive False Positive True Positive Create a confusion matrix for the first 50 cases. 8.4.5 Full Test Data Confusion Matrix Now, let’s look at the confusion matrix for all 1,000 test cases. LR_Predict_Default &lt;- factor(LR_Predict_Default) confusionMatrix( data =LR_Predict_Default, reference=factor(Actual_Default) , positive=&quot;Yes&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction No Yes ## No 957 20 ## Yes 8 15 ## ## Accuracy : 0.972 ## 95% CI : (0.9598, 0.9813) ## No Information Rate : 0.965 ## P-Value [Acc &gt; NIR] : 0.12988 ## ## Kappa : 0.5035 ## ## Mcnemar&#39;s Test P-Value : 0.03764 ## ## Sensitivity : 0.4286 ## Specificity : 0.9917 ## Pos Pred Value : 0.6522 ## Neg Pred Value : 0.9795 ## Prevalence : 0.0350 ## Detection Rate : 0.0150 ## Detection Prevalence : 0.0230 ## Balanced Accuracy : 0.7101 ## ## &#39;Positive&#39; Class : Yes ## 8.4.6 Decision Tree Classifier For comparison, let’s use a decision tree to predict whether a person will default. In a binary classification problem, we can treat a default as \\(y=1\\) and non-default as \\(y=0\\), and grow the tree as we would in regression. The mean response in a node \\(\\bar{Y}\\) can be interpreted as the probability of default. The first few splits of the tree are shown. library(rpart) library(rpart.plot) tree &lt;- rpart(data=Default_Train, default~balance + student, cp=0.005) rpart.plot(tree, box.palette=&quot;RdBu&quot;, shadow.col=&quot;gray&quot;, nn=TRUE, cex=1, extra=1) tree &lt;- rpart(data=Default_Train, default~balance + student) Tree_Prob &lt;- predict(tree, newdata = Default_Test) Tree_Pred_Default &lt;- factor(ifelse(Tree_Prob &gt; 0.5, &quot;Yes&quot;, &quot;No&quot;)) LR_Res_df &lt;- data.frame(LR_Prob, LR_Predict_Default, Tree_Prob, Tree_Pred_Default, Actual_Default) kable(head(LR_Res_df, 50)%&gt;% arrange(desc(LR_Prob))) LR_Prob LR_Predict_Default Tree_Prob Tree_Pred_Default Actual_Default 2465 0.5400853 Yes 0.7745098 Yes No 1228 0.2570897 No 0.1585557 No No 6656 0.1350159 No 0.1585557 No No 1185 0.1286163 No 0.1585557 No No 9963 0.1160190 No 0.1585557 No No 6635 0.0713025 No 0.0065351 No Yes 5921 0.0623171 No 0.1585557 No No 9691 0.0558990 No 0.0065351 No No 9755 0.0222295 No 0.0065351 No No 7569 0.0191176 No 0.0065351 No No 9435 0.0129212 No 0.0065351 No No 1290 0.0129187 No 0.0065351 No No 7023 0.0112015 No 0.0065351 No No 2222 0.0102643 No 0.0065351 No No 6591 0.0097522 No 0.0065351 No No 3674 0.0082435 No 0.0065351 No No 6764 0.0059578 No 0.0065351 No No 7848 0.0058386 No 0.0065351 No No 7491 0.0055477 No 0.0065351 No No 9386 0.0053516 No 0.0065351 No No 3022 0.0038748 No 0.0065351 No No 8081 0.0033171 No 0.0065351 No No 4598 0.0030966 No 0.0065351 No No 8204 0.0025254 No 0.0065351 No No 4023 0.0024937 No 0.0065351 No No 2962 0.0020771 No 0.0065351 No No 1792 0.0020008 No 0.0065351 No No 9722 0.0019758 No 0.0065351 No No 6579 0.0014153 No 0.0065351 No No 5295 0.0009563 No 0.0065351 No No 576 0.0008414 No 0.0065351 No No 27 0.0006303 No 0.0065351 No No 6702 0.0006275 No 0.0065351 No No 2047 0.0005230 No 0.0065351 No No 742 0.0004324 No 0.0065351 No No 1253 0.0003789 No 0.0065351 No No 2226 0.0003423 No 0.0065351 No No 5409 0.0002650 No 0.0065351 No No 4282 0.0002322 No 0.0065351 No No 6298 0.0002287 No 0.0065351 No No 2547 0.0002225 No 0.0065351 No No 5206 0.0002186 No 0.0065351 No No 2213 0.0001745 No 0.0065351 No No 1215 0.0001467 No 0.0065351 No No 8807 0.0001466 No 0.0065351 No No 9790 0.0001379 No 0.0065351 No No 6578 0.0000895 No 0.0065351 No No 9977 0.0000479 No 0.0065351 No No 9808 0.0000192 No 0.0065351 No No 63 0.0000127 No 0.0065351 No No 8.4.7 Tree Confusion Matrix # data is predicted class # reference is actual class confusionMatrix( data = Tree_Pred_Default , reference= Actual_Default, &quot;Yes&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction No Yes ## No 960 24 ## Yes 5 11 ## ## Accuracy : 0.971 ## 95% CI : (0.9586, 0.9805) ## No Information Rate : 0.965 ## P-Value [Acc &gt; NIR] : 0.1724819 ## ## Kappa : 0.4186 ## ## Mcnemar&#39;s Test P-Value : 0.0008302 ## ## Sensitivity : 0.3143 ## Specificity : 0.9948 ## Pos Pred Value : 0.6875 ## Neg Pred Value : 0.9756 ## Prevalence : 0.0350 ## Detection Rate : 0.0110 ## Detection Prevalence : 0.0160 ## Balanced Accuracy : 0.6546 ## ## &#39;Positive&#39; Class : Yes ## Question: Which classifier, the logistic regression model or decision tree, appears to do better at predicting default? Question: Why might a prediction accuracy rate of over 97% on these data not be as impressive as it sounds? 8.4.8 More on Accuracy Rate Although a ~97% accuracy rate seems impressive recall the percentage of defaults in the dataset. mean(Default$default) ## [1] 0.0333 Thus, if we simply predicted that everyone would not default, we would achieve 96.67% accuracy, without providing any useful information. Classification accuracy rate can be misleading, especially in cases of unbalanced data, where one class is far more prevelant than the other. 8.4.9 Sensitivity and Specificity The sensitivity of a classifier is the proportion of all positive cases that the model correctly identifies as positive. (i.e. probability model says “positive” given actually is positive.) \\[ \\text{Sensitivity} = \\frac{\\text{True Positive}}{\\text{True Positive} + \\text{False Negative}} = \\frac{\\text{Predicted Positives}}{\\text{Actual Positives}} \\] LR Sensitivity \\[ \\frac{15}{15+20} \\approx 0.4286 \\] Tree Sensitivity \\[ \\frac{11}{11+24} \\approx 0.3143 \\] The specificity of a classifier is the proportion of all negative cases that the model correctly identifies as negative (i.e probabiltiy model says “negative” given truly is negative.) \\[ \\text{Specificity} = \\frac{\\text{True Negative}}{\\text{True Negative} + \\text{False Positive}}= \\frac{\\text{Predicted Negatives}}{\\text{Actual Negatives}} \\] LR Specificity \\[ \\frac{957}{957+8} \\approx 0.9917 \\] Tree Specificity \\[ \\frac{960}{960+5} \\approx 0.9948 \\] In a given situation, we should think about the cost of a false negative vs a false positive when determining whether to place more weight on sensitivity or specificity. For example, “is it worse to tell a patient they tested positive for a disease when they really don’t have it, or to not tell them they tested positive when they really do have it?” 8.5 Receiver Operating Characteristic Curve 8.5.1 ROC Curve A receiver operating characteristic curve tells us how well a predictor is able to separate positive cases from negative cases. The blog (Toward Data Science) [https://towardsdatascience.com/applications-of-different-parts-of-an-roc-curve-b534b1aafb68] writes “Receiver Operating Characteristic (ROC) curve is one of the most common graphical tools to diagnose the ability of a binary classifier, independent of the inherent classification algorithm. The ROC analysis has been used in many fields including medicine, radiology, biometrics, natural hazards forecasting, meteorology, model performance assessment, and other areas for many decades and is increasingly used in machine learning and data mining research [1]. If you are a Data Scientist, you might be using it on a daily basis.” The ROC curve plots the true positive (or hit) rate against the false positive rate (false alarm) rate, as the cutoff for a positive classification varies. The higher the curve, the better the predictor is able to separate positive cases from negative ones. Predictions made totally at random would be expected to yield a diagonal ROC curve. 8.5.2 LR and Tree ROC Curves library(pROC) library(verification) roc.plot(x=Default_Test$default, pred = LR_Prob) auc(response=Default_Test$default, predictor = LR_Prob) ## Area under the curve: 0.918 roc.plot(x=Default_Test$default, pred = Tree_Prob) auc(response=Default_Test$default, predictor = Tree_Prob) ## Area under the curve: 0.8176 RandProb &lt;- runif(1000, 0, 1) roc.plot(x=Default_Test$default, pred = RandProb) auc(response=Default_Test$default, predictor = RandProb) ## Area under the curve: 0.563 Even though a model that assigns predictions randomly, with 97% predicted as negatives will have a high accuracy rate, it will yield a poor ROC curve indicating an inability to separate positive cases from negative ones. 8.5.3 Constructing ROC Curve Order the probabilities from highest to lowest. Assume only the case with the highest probability is predicted as a positive. Calculate the true positive rate (hit rate) \\(\\frac{\\text{# True Positives}}{\\text{# Actual Positives}}\\) and false positive (false alarm) \\(\\frac{\\text{# False Positives}}{\\text{# Actual Negatives}}\\)rate. Plot the point \\(\\left( \\frac{\\text{# False Positives}}{\\text{# Actual Negatives}}, \\frac{\\text{# True Positives}}{\\text{# Actual Positives}} \\right)\\) in the coordinate plane. Now assume the cases with the two highest probabilities are predicted as positives, and repeat steps 3-4. Continue, by classifiying one more case as positive in each step. 8.5.4 Construct ROC Example Let’s practice constructing an ROC curve for a small set of probability estimates. prob &lt;- c(0.9, 0.8, 0.7, 0.65, 0.45, 0.3, 0.2, 0.15, 0.1, 0.05) Actual &lt;- c(&quot;+&quot;, &quot;-&quot;, &quot;+&quot;, &quot;+&quot;, &quot;-&quot;, &quot;-&quot;, &quot;-&quot;, &quot;-&quot;, &quot;+&quot;, &quot;-&quot;) Hit_Rate &lt;- c(&quot;1/4&quot;, &quot;1/4&quot;, &quot;2/4&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;) FA_Rate &lt;- c(&quot;0/6&quot;, &quot;1/6&quot;, &quot;1/6&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;) kable(data.frame(prob, Actual, Hit_Rate, FA_Rate)) prob Actual Hit_Rate FA_Rate 0.90 + 1/4 0/6 0.80 - 1/4 1/6 0.70 + 2/4 1/6 0.65 + 0.45 - 0.30 - 0.20 - 0.15 - 0.10 + 0.05 - Finish filling in the table and sketch a graph of the resulting ROC curve. Question: If the probability estimate of 0.45 were instead 0.5 or 0.55, would this change the ROC curve? Why or why not? "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
