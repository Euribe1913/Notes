[["index.html", "Stat 255: Statistics for Data Science Notes Preface", " Stat 255: Statistics for Data Science Notes Andrew Sage - Lawrence University 2022-09-25 Preface These notes serve as the primary textual resource for Stat 255: Statistics for Data Science at Lawrence University. What is this course about? Stat 255 provides an introduction to essential statistical tasks including modeling, inference, prediction, and computation. The course employs a modern approach, intended to equip students with skills needed for working with today’s complex data. Traditional concepts, like interval estimation, and hypothesis testing, are introduced through the lens of multivariate models and simulation. Data computation in R plays a central role throughout the course. The course’s overarching learning outcomes are: Visualize and wrangle data using statistical software R. Build and assess multivariate models to predict future outcomes. Quantify uncertainty associated with estimates and predictions. Explain the assumptions associated with statistical models, and evalate whether these assumptions are reasonably satisfied in context. Write reproducible analyses, using statistical software. Work with data in an ethical and responsible manner. More specific learning tasks, related to these outcomes are provided in each chapter. Who is this course intended for? This course is intended for students who are interested in learning statistical modeling and data computation skills that might prove useful in further courses, research, or career. Stat 255 can serve as either: a first course in statistics for students with a strong quantitative background, typically including calculus. a second course in statistics, building on introductory topics taught in courses like Lawrence’s Stat 107: Principles of Statistics, or AP Statistics. At Lawrence, this course is required for the Statistics Track of the Mathematics Major, the Economics and Mathematics-Economics Majors, and the Statistics and Data Science Minor. The prerequisite for the course is either 1) a prior college-level course in statistics (i.e. STAT 107, BIOL 170 or 280, ANTH 207, AP Stats) OR 2) Calculus. (Math 140, AP Calculus, or equivalent). The course does not assume any prior knowledge of statistics, but does move more rapidly than a typical introductory statistics course. Students engage rigorously in statistical thinking and computation, intended to equip them with essential skills for further study in statistics and data science. "],["exploratory-data-analysis.html", "Chapter 1 Exploratory Data Analysis 1.1 Exploring Data Visualizations 1.2 Exploratory Analysis in R", " Chapter 1 Exploratory Data Analysis Learning Outcomes: Interpret graphical summaries of data, including boxplots, histograms, violin plots, density plots, scatterplots, and correlation plots. Read data from a .csv file into R. Preview data in R. Create graphical summaries of data using R. Calculate summary statistics for entire datasets and grouped summaries. Create reproducible documents using R Markdown. 1.1 Exploring Data Visualizations 1.1.1 COVID-19 Data The following data were contained in a UK technical report on the Delta Variant on August 2, 2021 Vaccination Status Deaths Total Cases Fatality Rate Fully Vax 402 47,008 0.86% Unvax 253 151,054 0.17 % ggplot(data=Covid_Data, aes(x=Vac_Status, fill=Outcome)) + geom_bar(position=&quot;fill&quot;) One explanation would be that vaccines don’t work, and even cause harm. Can you think of another explanation? 1.1.2 Breakdown by Age &lt; 50 We break down the cases based on whether the patient was older or younger than 50. Under 50 Vaccination Status Deaths Total Cases Fatality Rate Fully Vax 13 25,536 0.05% Unvax 48 147, 612 0.03 % 50 or Older Vaccination Status Deaths Total Cases Fatality Rate Fully Vax 389 21,472 1.81% Unvax 205 3,440 5.96 % ggplot(data=Covid_Data, aes(x=Vac_Status, fill=Outcome)) + geom_bar(position=&quot;fill&quot;) + facet_wrap(~Age) Before accounting for age, vaccinated people appear to be 5 times MORE likely to die from covid than unvaccinated people. After accounting for age, vaccinated young people appear to be about 1.5 times more likely to die from covid (though death rates are very low) overall, and vaccinated older people appear to be more than 3 times LESS likely. 1.1.3 Vaccination by Age Breakdown ggplot(data=Covid_Data, aes(x=Vac_Status, fill=Age)) + geom_bar(position=&quot;fill&quot;) Since almost all of the unvaccinated people were from the lower risk category, the death rate among unvaccinated people appears lower. We should account for age when comparing survival rates. 1.1.4 Simpson’s Paradox Simpson’s Paradox refers to a situation where an apparent trend either disappears or reverses when one or more additional variables are accounted for. These additional variables are called confounding variables. In this situation, age is a counfouding variable. Older people are more likely to be vaccinated, and also more likely to die of covid (regardless of vaccination status), thus it appears that vaccinated people are more likely to die from covid, unless we account for age. When we build statistical models, it will be important to account for potential confounding variables in our data, otherwise a model will give misleading results. Final note: Subsequent data has shown that among people under 50, vaccinated people also had lower death rates (though death rates remained very low for both vaccinated and unvaccinated people). The slightly higher death rate for vaccinated people under 50, seen in these early data, is likely due to young people with other risk conditions being vaccinated first. Data Source 1.1.5 Exploring Diamond Prices The following data come from a dataset with information on over 53,940 diamonds. We’ll explore the relationship between price (in $ US) and quality of the cut of the diamond (ideal, premium, very good, good, fair.\" Consider the following plots: ggplot(data=diamonds, aes(x=price, y=cut, fill=cut)) + geom_boxplot(outlier.size=0.01, outlier.alpha = 0.1) + stat_summary(fun=mean, geom=&quot;point&quot;, shape=4, color=&quot;red&quot;, size=3) + ggtitle(&quot;Price by Quality of Cut&quot;) Diamonds come in different carat sizes, so we should consider information about that as well. We examine a histogram, displaying the number of diamonds with each cut, and carat size. ggplot(data=diamonds, aes(x=carat, fill=cut)) + geom_histogram() + ggtitle(&quot;Diamonds by Carat Size and Cut Quality&quot;) The table shows the number of diamonds of each cut, as well as the average carat size and price of each diamond. diamonds %&gt;% group_by(cut) %&gt;% summarize(N=n(), Avg_carat=mean(carat), Avg_price=mean(price) ) ## # A tibble: 5 × 4 ## cut N Avg_carat Avg_price ## &lt;ord&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Fair 1610 1.05 4359. ## 2 Good 4906 0.849 3929. ## 3 Very Good 12082 0.806 3982. ## 4 Premium 13791 0.892 4584. ## 5 Ideal 21551 0.703 3458. Finally, we use a scatterplot to visualize cut, price, and carat size. ggplot(data=diamonds, aes(x=carat, y=price, color=cut)) + geom_point() + ggtitle(&quot;Price by Carat Size and Cut&quot;) Question: How is Simpson’s paradox present in the diamonds data? What is the confounding variable? What conclusion should we draw about the relationship between the price of a diamond and the quality of the cut? 1.2 Exploratory Analysis in R This section provides examples of how to read data into R, create graphics, like those in the previous section, and calculate summary statistics. We’ll work with data on movies released in Hollywood between 2012 and 2018. 1.2.1 Loading the Data We’ll begin by loading the tidyverse package, which can be used to create professional graphics, and wrangle (or manipulate) data into forms that are informative and easy to work with. library(tidyverse) Next, we read in the data itself, from the website where it is stored. HollywoodMovies &lt;- read_csv(&quot;https://www.lock5stat.com/datasets3e/HollywoodMovies.csv&quot;) 1.2.2 Previewing the Data head() The head() function displays the first 5 rows of the dataset. head(HollywoodMovies) ## # A tibble: 6 × 15 ## Movie LeadStudio RottenTomatoes AudienceScore Genre TheatersOpenWeek ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2016: Obama&#39;s … Rocky Mou… 26 73 Docu… 1 ## 2 21 Jump Street Sony Pict… 85 82 Come… 3121 ## 3 A Late Quartet Entertain… 76 71 Drama 9 ## 4 A Royal Affair Magnolia … 90 82 Drama 7 ## 5 Abraham Lincol… Twentieth… 35 51 Horr… 3108 ## 6 Act of Valor Relativit… 27 72 Acti… 3039 ## # … with 9 more variables: OpeningWeekend &lt;dbl&gt;, BOAvgOpenWeekend &lt;dbl&gt;, ## # Budget &lt;dbl&gt;, DomesticGross &lt;dbl&gt;, WorldGross &lt;dbl&gt;, ForeignGross &lt;dbl&gt;, ## # Profitability &lt;dbl&gt;, OpenProfit &lt;dbl&gt;, Year &lt;dbl&gt; The rows of the dataset are called observations. In this case, the observations are the movies. The columns of the dataset, which contain information about the movies, are called variables. glimpse The glimpse() command shows the number of observations (rows), and the number of variables, (columns). We also see the name of each variable and its type. Variable types include Categorical variables, which take on groups or categories, rather than numeric values. In R, these might be coded as logical &lt;logi&gt;, character &lt;chr&gt;, factor &lt;fct&gt; and ordered factor &lt;ord&gt;. Quantitative variables, which take on meaningful numeric values. These include numeric &lt;num&gt;, integer &lt;int&gt;, and double &lt;dbl&gt;. glimpse(HollywoodMovies) ## Rows: 1,295 ## Columns: 15 ## $ Movie &lt;chr&gt; &quot;2016: Obama&#39;s America&quot;, &quot;21 Jump Street&quot;, &quot;A Late Qu… ## $ LeadStudio &lt;chr&gt; &quot;Rocky Mountain Pictures&quot;, &quot;Sony Pictures Releasing&quot;,… ## $ RottenTomatoes &lt;dbl&gt; 26, 85, 76, 90, 35, 27, 91, 56, 11, 44, 93, 63, 87, 9… ## $ AudienceScore &lt;dbl&gt; 73, 82, 71, 82, 51, 72, 62, 47, 47, 63, 82, 51, 63, 9… ## $ Genre &lt;chr&gt; &quot;Documentary&quot;, &quot;Comedy&quot;, &quot;Drama&quot;, &quot;Drama&quot;, &quot;Horror&quot;, … ## $ TheatersOpenWeek &lt;dbl&gt; 1, 3121, 9, 7, 3108, 3039, 132, 245, 2539, 3192, 3, 1… ## $ OpeningWeekend &lt;dbl&gt; 0.03, 36.30, 0.08, 0.04, 16.31, 24.48, 1.14, 0.70, 11… ## $ BOAvgOpenWeekend &lt;dbl&gt; 30000, 11631, 8889, 5714, 5248, 8055, 8636, 2857, 449… ## $ Budget &lt;dbl&gt; 3.0, 42.0, NA, NA, 68.0, 12.0, NA, 7.5, 35.0, 50.0, 1… ## $ DomesticGross &lt;dbl&gt; 33.35, 138.45, 1.56, 1.55, 37.52, 70.01, 1.99, 3.01, … ## $ WorldGross &lt;dbl&gt; 33.35, 202.81, 6.30, 7.60, 137.49, 82.50, 3.59, 8.54,… ## $ ForeignGross &lt;dbl&gt; 0.00, 64.36, 4.74, 6.05, 99.97, 12.49, 1.60, 5.53, 9.… ## $ Profitability &lt;dbl&gt; 1334.00, 482.88, NA, NA, 202.19, 687.50, NA, 113.87, … ## $ OpenProfit &lt;dbl&gt; 1.20, 86.43, NA, NA, 23.99, 204.00, NA, 9.33, 32.57, … ## $ Year &lt;dbl&gt; 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012,… There are 1,295 movies in the dataset, and 15 variables for each film. summary summary displays the mean, minimum, first quartile, median, third quartile, and maximum for each numeric variable. summary(HollywoodMovies) ## Movie LeadStudio RottenTomatoes AudienceScore ## Length:1295 Length:1295 Min. : 0.00 Min. :10.00 ## Class :character Class :character 1st Qu.:33.00 1st Qu.:49.00 ## Mode :character Mode :character Median :61.00 Median :64.00 ## Mean :57.58 Mean :62.18 ## 3rd Qu.:84.00 3rd Qu.:77.00 ## Max. :99.00 Max. :99.00 ## NA&#39;s :6 ## Genre TheatersOpenWeek OpeningWeekend BOAvgOpenWeekend ## Length:1295 Min. : 1.0 Min. : 0.020 Min. : 204 ## Class :character 1st Qu.: 152.5 1st Qu.: 0.845 1st Qu.: 3482 ## Mode :character Median :2459.0 Median : 7.600 Median : 6586 ## Mean :2008.0 Mean : 17.541 Mean : 13400 ## 3rd Qu.:3213.5 3rd Qu.: 20.810 3rd Qu.: 14534 ## Max. :4529.0 Max. :257.700 Max. :240000 ## ## Budget DomesticGross WorldGross ForeignGross ## Min. : 0.90 Min. : 1.02 Min. : 0.74 Min. : -0.76 ## 1st Qu.: 12.00 1st Qu.: 6.40 1st Qu.: 13.09 1st Qu.: 3.91 ## Median : 30.00 Median : 26.46 Median : 50.37 Median : 21.58 ## Mean : 51.38 Mean : 58.16 Mean : 147.01 Mean : 88.84 ## 3rd Qu.: 65.00 3rd Qu.: 66.44 3rd Qu.: 160.38 3rd Qu.: 89.75 ## Max. :365.00 Max. :936.66 Max. :2068.22 Max. :1369.54 ## NA&#39;s :239 ## Profitability OpenProfit Year ## Min. : 2.3 Min. : 0.05 Min. :2012 ## 1st Qu.: 139.1 1st Qu.: 12.87 1st Qu.:2013 ## Median : 268.9 Median : 31.77 Median :2015 ## Mean : 435.7 Mean : 64.50 Mean :2015 ## 3rd Qu.: 483.0 3rd Qu.: 62.59 3rd Qu.:2017 ## Max. :10176.0 Max. :3373.00 Max. :2018 ## NA&#39;s :239 NA&#39;s :239 Notice that 239 films have missing information on some of the variables, recorded as NA’s. 1.2.3 Adding a New Variable We can use the mutate() function to create a new variable based on variables already in the dataset. In the data description, the variable Profitability is defined as WorldGross as a percentage of Budget. Thus, films for which Profitability exceeds 100 were profitable. We create a variable to tell whether or not a film was profitable. Note that in R, a variable defined as a condition, such as Profitability&gt;100 will return values of either TRUE or FALSE. HollywoodMovies &lt;- HollywoodMovies %&gt;% mutate(Profitable = Profitability &gt; 100) summary(HollywoodMovies$Profitable) ## Mode FALSE TRUE NA&#39;s ## logical 170 886 239 1.2.4 Selecting Columns If the dataset contains a large number of variables, narrow down to the ones you are interested in working with. This can be done with the select() command. If there are not very many variables to begin with, or you are interested in all of them, then you may skip this step. Let’s narrow the dataset down to the variables Movie, RottenTomatoes, AudienceScore, Genre, WorldGross, Budget, “Profitable”, and Year. MoviesSubset &lt;- HollywoodMovies %&gt;% select(Movie, RottenTomatoes, AudienceScore, Genre, WorldGross, Budget, Profitable, Year) 1.2.5 Filtering by Row The filter() command narrows a dataset down to rows that meet specified conditions. Filtering by a Categorical Variable Let’s filter the data to only include action movies, comedies, dramas, and horror movies. We’ll also keep only those films whose budget was listed, excluding the 239 NA’s. The command !is.na() returns only values that are not NA’s. MoviesSubset1 &lt;- MoviesSubset %&gt;% filter(Genre %in% c(&quot;Action&quot;, &quot;Comedy&quot;, &quot;Drama&quot;, &quot;Horror&quot;)) %&gt;% filter(!is.na(Budget)) In R, the ! operator means “not”. glimpse(MoviesSubset1) ## Rows: 679 ## Columns: 8 ## $ Movie &lt;chr&gt; &quot;21 Jump Street&quot;, &quot;Abraham Lincoln: Vampire Hunter&quot;, &quot;A… ## $ RottenTomatoes &lt;dbl&gt; 85, 35, 27, 56, 44, 93, 63, 86, 34, 86, 74, 41, 71, 32,… ## $ AudienceScore &lt;dbl&gt; 82, 51, 72, 47, 63, 82, 51, 86, 55, 76, 64, 35, 62, 70,… ## $ Genre &lt;chr&gt; &quot;Comedy&quot;, &quot;Horror&quot;, &quot;Action&quot;, &quot;Drama&quot;, &quot;Comedy&quot;, &quot;Drama… ## $ WorldGross &lt;dbl&gt; 202.81, 137.49, 82.50, 8.54, 236.80, 36.79, 71.00, 36.7… ## $ Budget &lt;dbl&gt; 42.0, 68.0, 12.0, 7.5, 50.0, 10.0, 49.0, 4.6, 220.0, 1.… ## $ Profitable &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T… ## $ Year &lt;dbl&gt; 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2… Filtering by a Quantitative Variable Let’s filter the data to only include films whose world gross exceeds 100 million dollars. MoviesSubset2 &lt;- MoviesSubset %&gt;% filter(WorldGross &gt;100) Now, let’s preview the data again. glimpse(MoviesSubset2) ## Rows: 444 ## Columns: 8 ## $ Movie &lt;chr&gt; &quot;21 Jump Street&quot;, &quot;Abraham Lincoln: Vampire Hunter&quot;, &quot;A… ## $ RottenTomatoes &lt;dbl&gt; 85, 35, 44, 96, 34, 78, 85, 66, 38, 88, 78, 17, 74, 45,… ## $ AudienceScore &lt;dbl&gt; 82, 51, 63, 90, 55, 76, 71, 67, 46, 92, 75, 32, 56, 72,… ## $ Genre &lt;chr&gt; &quot;Comedy&quot;, &quot;Horror&quot;, &quot;Comedy&quot;, &quot;Thriller&quot;, &quot;Action&quot;, &quot;Ad… ## $ WorldGross &lt;dbl&gt; 202.81, 137.49, 236.80, 227.14, 313.48, 554.61, 123.68,… ## $ Budget &lt;dbl&gt; 42.0, 68.0, 50.0, 45.0, 220.0, 185.0, 12.0, 102.0, 150.… ## $ Profitable &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T… ## $ Year &lt;dbl&gt; 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2… We’ll use MoviesSubset1 from this point forward. 1.2.6 Histogram Next, we’ll create graphics to help us visualize the distributions and relationships between variables. We’ll use the ggplot() function, which is part of the tidyverse package. Histograms are useful for displaying the distribution of a single quantitative variable General Template for Histogram ggplot(data=DatasetName, aes(x=VariableName)) + geom_histogram(fill=&quot;colorchoice&quot;, color=&quot;colorchoice&quot;) + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;x-axis label&quot;) + ylab(&quot;y-axis label&quot;) Histogram of Audience Scores ggplot(data=MoviesSubset1, aes(x=AudienceScore)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + ggtitle(&quot;Distribution of Audience Scores&quot;) + xlab(&quot;Audience Score&quot;) + ylab(&quot;Frequency&quot;) 1.2.7 Density Plots Density plots show the distribution for a quantitative variable like audience score. Scores can be compared across categories, like genre. General Template for Density Plot ggplot(data=DatasetName, aes(x=QuantitativeVariable, color=CategoricalVariable, fill=CategoricalVariable)) + geom_density(alpha=0.2) + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;Axis Label&quot;) + ylab(&quot;Frequency&quot;) alpha, ranging from 0 to 1 dictates transparency. Density Plot of Audience Scores ggplot(data=MoviesSubset1, aes(x=AudienceScore, color=Genre, fill=Genre)) + geom_density(alpha=0.2) + ggtitle(&quot;Distribution of Audience Scores&quot;) + xlab(&quot;Audience Score&quot;) + ylab(&quot;Frequency&quot;) 1.2.8 Boxplot Boxplots can be used to compare a quantitative variable with a categorical variable General Template for Boxplot ggplot(data=DatasetName, aes(x=CategoricalVariable, y=QuantitativeVariable)) + geom_boxplot() + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;Variable Name&quot;) + ylab(&quot;Variable Name&quot;) You can make the plot horizontal by adding + coordflip(). You can turn the axis text vertical by adding theme(axis.text.x = element_text(angle = 90)). Boxplot Comparing Scores for Genres ggplot(data=MoviesSubset1, aes(x=Genre, y=AudienceScore)) + geom_boxplot() + ggtitle(&quot;Audience Score by Genre&quot;) + xlab(&quot;Genre&quot;) + ylab(&quot;Audience Score&quot;) + theme(axis.text.x = element_text(angle = 90)) 1.2.9 Violin Plot Violin plots are an alternative to boxplots. The width of the violin tells us the density of observations in a given range. General Template for Violin Plot ggplot(data=DatasetName, aes(x=CategoricalVariable, y=QuantitativeVariable, fill=CategoricalVariable)) + geom_violin() + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;Variable Name&quot;) + ylab(&quot;Variable Name&quot;) Violin Plot Comparing Scores for Genres ggplot(data=MoviesSubset1, aes(x=Genre, y=AudienceScore, fill=Genre)) + geom_violin() + ggtitle(&quot;Audience Score by Genre&quot;) + xlab(&quot;Genre&quot;) + ylab(&quot;Audience Score&quot;) + theme(axis.text.x = element_text(angle = 90)) We can view the boxplot and violin plot together. 1.2.10 Scatterplots Scatterplots are used to visualize the relationship between two quantitative variables. Scatterplot Template ggplot(data=DatasetName, aes(x=CategoricalVariable, y=QuantitativeVariable)) + geom_point() + ggtitle(&quot;Plot Title&quot;) + ylab(&quot;Axis Label&quot;) + xlab(&quot;Axis Label&quot;) Scatterplot Comparing Audience Score and Rotten Tomatoes Score ggplot(data=MoviesSubset1, aes(x=RottenTomatoes, y=AudienceScore)) + geom_point() + ggtitle(&quot;Audience and Critics Ratings&quot;) + ylab(&quot;Audience Rating&quot;) + xlab(&quot;Critics&#39; Rating&quot;) We see that there is an upward trend, indicating a positive association between critics scores (RottenTomatoes), and audience scores. However, there is a lot of variability, and the relationship is moderately strong at best. We can also add color, size, and shape to the scatterplot to display information about other variables. ggplot(data=MoviesSubset1, aes(x=RottenTomatoes, y=AudienceScore, color=Genre, size=WorldGross)) + geom_point() + ggtitle(&quot;Audience and Critics Ratings&quot;) + ylab(&quot;Audience Rating&quot;) + xlab(&quot;Critics&#39; Rating&quot;) We can add labels for points meeting certain conditions, using geom_text(). This should be done carefully, to avoid overlap. ggplot(data=MoviesSubset1, aes(x=RottenTomatoes, y=AudienceScore, color=Genre, size=WorldGross)) + geom_point() + ggtitle(&quot;Audience and Critics Ratings&quot;) + ylab(&quot;Audience Rating&quot;) + xlab(&quot;Critics&#39; Rating&quot;) + geom_text(data = MoviesSubset1 %&gt;% filter(WorldGross &gt;800), aes(label = Movie), color=&quot;black&quot;, check_overlap = TRUE) 1.2.11 Bar Graphs Bar graphs can be used to visualize one or more categorical variables Bar Graph Template ggplot(data=DatasetName, aes(x=CategoricalVariable)) + geom_bar(fill=&quot;colorchoice&quot;,color=&quot;colorchoice&quot;) + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;Variable Name&quot;) + ylab(&quot;Frequency&quot;) Bar Graph by Genre ggplot(data=MoviesSubset1, aes(x=Genre)) + geom_bar(fill=&quot;lightblue&quot;,color=&quot;white&quot;) + ggtitle(&quot;Number of Films by Genre&quot;) + xlab(&quot;Genre&quot;) + ylab(&quot;Number of Films&quot;) + theme(axis.text.x = element_text(angle = 90)) 1.2.12 Stacked and Side-by-Side Bar Graphs Stacked Bar Graph Template ggplot(data = DatasetName, mapping = aes(x = CategoricalVariable1, fill = CategoricalVariable2)) + stat_count(position=&quot;fill&quot;) + theme_bw() + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;Variable 1&quot;) + ylab(&quot;Proportion of Variable 2&quot;) + theme(axis.text.x = element_text(angle = 90)) Stacked Bar Graph Example The stat_count(position=\"fill\") command creates a stacked bar graph, comparing two categorical variables. Let’s explore whether certain genres are more profitable than others, using the profitability variable. ggplot(data = MoviesSubset1, mapping = aes(x = Genre, fill = Profitable)) + stat_count(position=&quot;fill&quot;) + theme_bw() + ggtitle(&quot;Profitability by Genre&quot;) + xlab(&quot;Genre&quot;) + ylab(&quot;Proportion Profitable&quot;) + theme(axis.text.x = element_text(angle = 90)) Side-by-side Bar Graph Template We can create a side-by-side bar graph, using position=dodge. ggplot(data = DatasetName, mapping = aes(x = CategoricalVariable1, fill = CategoricalVariable2)) + geom_bar(position = &quot;dodge&quot;) + ggtitle(&quot;Plot Title&quot;) + xlab(&quot;Genre&quot;) + ylab(&quot;Frequency&quot;) Side-by-side Bar Graph Example ggplot(data = MoviesSubset1, mapping = aes(x = Genre, fill = Profitable)) + geom_bar(position = &quot;dodge&quot;) + ggtitle(&quot;Number of Films by Genre&quot;) + xlab(&quot;Genre&quot;) + ylab(&quot;Number of Films&quot;) + theme(axis.text.x = element_text(angle = 90)) 1.2.13 Correlation Plot Correlation plots can be used to visualize relationships between quantitative variables. These can be helpful when we proceed to modeling. Explanatory variables that are highly correlated with the response are often strong predictors that should be included in a model. However, including two explanatory variables that are highly correlated with one another can create interpretation problems. The cor() function calculates correlations between quantitative variables. We’ll use select_if to select only numeric variables. The `use=“complete.obs” command tells R to ignore observations with missing data. cor(select_if(HollywoodMovies, is.numeric), use=&quot;complete.obs&quot;) %&gt;% round(2) ## RottenTomatoes AudienceScore TheatersOpenWeek OpeningWeekend ## RottenTomatoes 1.00 0.71 -0.27 0.14 ## AudienceScore 0.71 1.00 -0.21 0.20 ## TheatersOpenWeek -0.27 -0.21 1.00 0.59 ## OpeningWeekend 0.14 0.20 0.59 1.00 ## BOAvgOpenWeekend 0.40 0.35 -0.37 0.13 ## Budget 0.06 0.13 0.59 0.72 ## DomesticGross 0.24 0.31 0.51 0.93 ## WorldGross 0.20 0.28 0.52 0.90 ## ForeignGross 0.17 0.25 0.49 0.84 ## Profitability 0.08 0.08 0.04 0.15 ## OpenProfit -0.10 -0.11 0.13 0.12 ## Year 0.04 -0.05 0.08 0.04 ## BOAvgOpenWeekend Budget DomesticGross WorldGross ForeignGross ## RottenTomatoes 0.40 0.06 0.24 0.20 0.17 ## AudienceScore 0.35 0.13 0.31 0.28 0.25 ## TheatersOpenWeek -0.37 0.59 0.51 0.52 0.49 ## OpeningWeekend 0.13 0.72 0.93 0.90 0.84 ## BOAvgOpenWeekend 1.00 0.08 0.27 0.25 0.22 ## Budget 0.08 1.00 0.70 0.78 0.79 ## DomesticGross 0.27 0.70 1.00 0.94 0.86 ## WorldGross 0.25 0.78 0.94 1.00 0.98 ## ForeignGross 0.22 0.79 0.86 0.98 1.00 ## Profitability 0.14 -0.12 0.17 0.15 0.13 ## OpenProfit -0.07 -0.14 0.07 0.02 -0.01 ## Year -0.04 0.01 0.03 0.04 0.04 ## Profitability OpenProfit Year ## RottenTomatoes 0.08 -0.10 0.04 ## AudienceScore 0.08 -0.11 -0.05 ## TheatersOpenWeek 0.04 0.13 0.08 ## OpeningWeekend 0.15 0.12 0.04 ## BOAvgOpenWeekend 0.14 -0.07 -0.04 ## Budget -0.12 -0.14 0.01 ## DomesticGross 0.17 0.07 0.03 ## WorldGross 0.15 0.02 0.04 ## ForeignGross 0.13 -0.01 0.04 ## Profitability 1.00 0.84 -0.01 ## OpenProfit 0.84 1.00 -0.03 ## Year -0.01 -0.03 1.00 The corrplot() function in the corrplot() package provides a visualization of the correlations. Larger, thicker circles indicate stronger correlations. library(corrplot) Corr &lt;- cor(select_if(HollywoodMovies, is.numeric), use=&quot;complete.obs&quot;) corrplot(Corr) We can also display a numeric version of the correlations by setting method=\"number\". Corr &lt;- cor(select_if(HollywoodMovies, is.numeric), use=&quot;complete.obs&quot;) corrplot(Corr, method=&quot;number&quot;) 1.2.14 Scatterplot Matrix A scatterplot matrix is a grid of plots. It can be created using the ggpairs() function in the GGally package. The scatterplot matrix shows us: Along the diagonal are density plots for quantitative variables, or bar graphs for categorical variables, showing the distribution of each variable. Under the diagonal are plots showing the relationships between the variables in the corresponding row and column. Scatterplots are used when both variables are quantitative, bar graphs are used when both variables are categorical, and boxplots are used when one variable is categorical, and the other is quantitative. Above the diagonal are correlations between quantitative variables. We need to remove the column with the movie names. This is done using select. library(GGally) ggpairs(MoviesSubset1 %&gt;% select(-Movie)) The scatterplot matrix is useful for helping us notice key trends in our data. However, the plot can hard to read as it is quite dense, especially when there are a large number of variables. These can help us look for trends from a distance, but we should then focus in on more specific plots. 1.2.15 Summary Tables group_by() and summarize() The group_by() and summarize() commands are useful for breaking categorical variables down by category. For example, let’s calculate number of films in each genre, and the mean, median, and standard deviation in film WorldGross by genre. MoviesSubset1 %&gt;% group_by(Genre) %&gt;% summarize(N = n(), Mean_Gross = mean(WorldGross, na.rm=TRUE), Median_Gross = median(WorldGross, na.rm=TRUE), StDev_Gross = sd(WorldGross, na.rm = TRUE)) %&gt;% arrange(desc(Mean_Gross)) ## # A tibble: 4 × 5 ## Genre N Mean_Gross Median_Gross StDev_Gross ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Action 162 371. 222. 393. ## 2 Horror 78 103. 79.6 113. ## 3 Comedy 160 85.1 65.0 77.0 ## 4 Drama 279 76.4 34.7 119. Notes: 1. The n() command calculates the number of observations in a category. 2. The na.rm=TRUE command removes missing values, so that summary statistics can be calculated. 3. arrange(desc(Mean_Gross)) arranges the table in descending order of Mean_Gross. To arrange in ascending order, use arrange(Mean_Gross). The kable() function in the knitr() package creates tables with professional appearance. library(knitr) MoviesTable &lt;- MoviesSubset1 %&gt;% group_by(Genre) %&gt;% summarize(N = n(), Mean_Gross = mean(WorldGross, na.rm=TRUE), Median_Gross = median(WorldGross, na.rm=TRUE), StDev_Gross = sd(WorldGross, na.rm = TRUE)) %&gt;% arrange(desc(Mean_Gross)) kable(MoviesTable) Genre N Mean_Gross Median_Gross StDev_Gross Action 162 370.85648 221.59 393.01213 Horror 78 102.55423 79.63 113.14402 Comedy 160 85.14469 64.99 77.00407 Drama 279 76.42573 34.72 119.08083 "],["introduction-to-statistical-models.html", "Chapter 2 Introduction to Statistical Models 2.1 Predicting House Prices 2.2 Variability Explained by a Model 2.3 Multiple Regression Model 2.4 Least-Squares Estimation 2.5 ANalysis Of VAriance 2.6 Models Involving Interaction 2.7 More on Interaction", " Chapter 2 Introduction to Statistical Models Learning Outcomes: Calculate sums of squares related to variability explained, including SST, SSR, and SSM., when given small datasets and/or summary statistics. Explain the meaning of SST, SSR, and SSM in a given context. Calculate \\(R^2\\) and ANOVA F-Statistics, when given small datasets and/or summary statistics. Intrepret \\(R^2\\) and F-statistics in context. Explain the process for estimating least-squares regression coefficients. Calculate predictions from linear regression models. Interpret regression coefficients for models involving quantitative and/or categorical variables in context, or explain why it is inappropriate to do so. Explain the meaning of interaction between quantitative and categorical explanatory variables. Apply graphical methods, statistical summaries, and background knowledge to argue for whether or not interaction term(s) should be used in a statistical model. Determine slopes, intercepts, and other regression coefficients for specific categories or values of an explanatory variable in models that involve interaction. 2.1 Predicting House Prices 2.1.1 House Prices in Ames IA Shown below are the prices of 10 houses sold in Ames, IA between 2006 and 2010. Houses$SalePrice ## [1] 187.00 163.99 235.00 113.00 110.00 84.90 123.00 176.50 150.00 137.00 ggplot(data=Houses, aes(x=SalePrice)) + geom_histogram(binwidth=60, fill=&quot;blue&quot;, color=&quot;white&quot;) + xlab(&quot;Price in thousands&quot;) + ylab(&quot;Frequency&quot;)+ theme_bw() + ggtitle(&quot;House Prices in Ames, IA&quot;) summary(Houses$SalePrice) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 84.9 115.5 143.5 148.0 173.4 235.0 Question: Suppose we know that a particular house sold in Ames during this time period, but know nothing else about the house. Based on this estimation, how might we predict the price of the house? How confident are you in this prediction? 2.1.2 Prices by Neighborhood Now, suppose that the houses are in one of three neighborhoods (College Creek, Edwards, North Ames). Prices of the houses in each neighborhood are shown below. p_House_Nbhd &lt;- ggplot(data=Houses, aes(x=Neighborhood, y=SalePrice)) +geom_point() + theme_bw() + ggtitle(&quot;Houses by Neighborhood&quot;) p_House_Nbhd kable(Houses %&gt;% select(Neighborhood, SalePrice)) Neighborhood SalePrice CollgCr 187.00 CollgCr 163.99 CollgCr 235.00 Edwards 113.00 Edwards 110.00 Edwards 84.90 NAmes 123.00 NAmes 176.50 NAmes 150.00 NAmes 137.00 Question: Suppose we know that a house sold in the College Creek neighborhood. How might we predict the price of the house? How confident are you in this prediction? 2.1.3 House Prices by Size The scatterplot shows the relationship between the size of a house in square feet, and its saleprice. ggplot(data=Houses, aes(x=SquareFeet, y=SalePrice)) + geom_point() Question: Suppose we know that another house, not in the original data, had 1200 square feet. How might we predict the saleprice of the house? How confident are you in this prediction? 2.1.4 Explanatory and Response Variables Now, suppose we want to use information about the house’s neighborhood in our predictions. We’ll predict a price of a house to be the average price among houses in the neightborhood. The variable we are trying to predict (price) is called the response variable (denoted \\(Y\\)). The variable(s) we use to help us make the prediction (neighborhood) is(are) called explanatory variables (denoted \\(X\\)). These are also referred to as predictor variables or covariates. 2.1.5 Prediction with No Explanatory Variables If we have the prices of \\(n\\) houses, \\(y_i, y_2, \\ldots, y_n\\), and want to predict the price of a new house, without information about any explanatory variables the ``statistically optimal\" prediction is the overall average price of all houses in the dataset. \\[ \\widehat{\\text{Price}} = \\bar{y}, \\text{where } \\bar{y}=\\frac{\\displaystyle\\sum_{i=1}^ny_i}{n}\\]. The symbol \\(\\widehat{\\text{Price}}\\), represents the predicted, or expected, price. ggplot(data=Houses, aes(x=1, y=SalePrice)) + geom_point() + ylab(&quot;Price in thousands&quot;) + xlab(&quot;&quot;)+ theme(axis.text.x = element_blank()) + stat_summary(fun = mean, geom = &quot;errorbar&quot;, aes(ymax = ..y.., ymin = ..y..), color=&quot;red&quot;) + annotate(&quot;text&quot;, y=145, x=1.25, label=&quot;sample mean&quot;, color=&quot;red&quot;) + theme_bw() + ggtitle(&quot;All 10 Houses&quot;) mean(Houses$SalePrice) ## [1] 148.039 Without information about any explanatory variables, we would predict the price of a house sold in Ames, IA during this time to be about 148 thousand dollars. 2.1.6 Simple Model in R M0 &lt;- lm(data=Houses, SalePrice~1) summary(M0) ## ## Call: ## lm(formula = SalePrice ~ 1, data = Houses) ## ## Residuals: ## Min 1Q Median 3Q Max ## -63.139 -32.539 -4.539 25.334 86.961 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 148.04 13.97 10.6 0.0000022 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 44.17 on 9 degrees of freedom Without information about any explanatory variables, we would predict the price of a house sold in Ames, IA during this time to be about 148 thousand dollars. 2.1.7 Prediction for a Categorical Explanatory Variable When we have a categorical explanatory variable (e.g. neighborhood), the “statistically optimal” prediction is the average response among all observations in the given category. p_House_Nbhd NbhdTbl &lt;- Houses %&gt;% group_by(Neighborhood) %&gt;% summarize(AveragePrice=mean(SalePrice)) kable(NbhdTbl) Neighborhood AveragePrice CollgCr 195.3300 Edwards 102.6333 NAmes 146.6250 We predict the price of a house in College Creek to be 195.33 thousand dollars, compared with 102.63 thousand dollars in Edwards, and 146.62 thousand dollars in Edwards. 2.1.8 Model by Neighborhood in R M_Nbhd &lt;- lm(data=Houses, SalePrice ~ Neighborhood) summary(M_Nbhd) ## ## Call: ## lm(formula = SalePrice ~ Neighborhood, data = Houses) ## ## Residuals: ## Min 1Q Median 3Q Max ## -31.340 -15.706 -2.477 9.617 39.670 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 195.33 14.89 13.118 0.00000349 *** ## NeighborhoodEdwards -92.70 21.06 -4.402 0.00315 ** ## NeighborhoodNAmes -48.70 19.70 -2.473 0.04267 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 25.79 on 7 degrees of freedom ## Multiple R-squared: 0.7348, Adjusted R-squared: 0.6591 ## F-statistic: 9.699 on 2 and 7 DF, p-value: 0.009603 For categorical explanatory variables, R treats the category that comes first alphabetically (in this case CCreek), as a baseline. The intercept gives the prediction for this category. We would expect a house in College Creek to cost 195.33 thousand dollars. Each of the other rows in the coefficients table represent the difference between the expected response (price) for that category (neighborhood), compared to the baseline. We would expect a house in Edwards to cost 92.70 thousand less than a house in College Creek. (hence costing 102.63 thousand) We would expect a house in North Ames to cost 48.71 thousand less than a house in College Creek. (hence costing 146.62 thousand) 2.1.9 Model Notation for Houses by Neighborhood The model can be expressed in the form: \\(\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{I}_{\\text{Edwards}} +b_2 \\times\\text{I}_{\\text{NAmes}}\\) \\(\\widehat{\\text{Price}}= 195.33+ -92.7 \\times\\text{I}_{\\text{Edwards}} +-48.7 \\times\\text{I}_{\\text{NAmes}}\\), where represents an indicator variables, taking on values 0 or 1. - Example: \\[ \\text{I}_{\\text{Edwards}} =\\begin{cases} 1 &amp; \\text{if house is in Edwards Neighborhood} \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] Predicted Prices: College Creek: \\(\\widehat{\\text{Price}}= 195.33+ -92.7 \\times0 +-48.7 \\times0 = 195.33\\) thousand. Edwards: \\(\\widehat{\\text{Price}}= 195.33+ -92.7 \\times1 +-48.7 \\times0 = 102.63\\) thousand. North Ames: \\(\\widehat{\\text{Price}}= 195.33+ -92.7 \\times0 +-48.7 \\times1 = 146.62\\) thousand. 2.1.10 Prediction for Quantitative Explanatory Variable For a quantitative explanatory variable like square feet, the “statistically optimal” prediction is found by fitting a “line of best fit” to the data (more details to come.) ggplot(data=Houses, aes(x=SquareFeet, y=SalePrice)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) For a house with 1200 square feet, predicted price is about $150 thousand. 2.1.11 Model using Square Feet M_SqFt &lt;- lm(data=Houses, SalePrice~SquareFeet) summary(M_SqFt) ## ## Call: ## lm(formula = SalePrice ~ SquareFeet, data = Houses) ## ## Residuals: ## Min 1Q Median 3Q Max ## -39.235 -14.309 2.052 10.966 43.971 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.82000 36.35455 0.188 0.85586 ## SquareFeet 0.12079 0.03022 3.997 0.00397 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 27.06 on 8 degrees of freedom ## Multiple R-squared: 0.6663, Adjusted R-squared: 0.6246 ## F-statistic: 15.97 on 1 and 8 DF, p-value: 0.003967 2.1.12 Model for SquareFeet and Interpretations In the model using both square feet and neighborhood, the regression equation is \\(\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{SquareFeet}\\) \\(\\widehat{\\text{Price}}= 6.82+ 0.121 \\times\\text{SquareFeet}\\) \\(\\widehat{\\text{Price}}\\) represents the expected, or predicted, price. The slope, \\(b_1\\) represents the expected change in price (in thousands) per one-unit increase in square feet. The price of a house is expected to increase by 121 dollars for each additional square foot. The intercept, \\(b_0\\) represents the expected price of a house with 0 square feet. - In this situation, this is not a meaningful interpretation. 2.1.13 Calculating Predicted Prices \\(\\widehat{\\text{Price}}= 6.82+ 0.121 \\times\\text{SquareFeet}\\) Predicted price for a house with 1200 square feet: \\(\\widehat{\\text{Price}}= 6.82+ 0.121 \\times 1200 = 151.8\\) thousand dollars. 2.2 Variability Explained by a Model 2.2.1 Quantifying Variability We’ve seen the “statistically optimal” way to make predictions for data with categorical and quantitative explanatory variables, but we shouldn’t expect these predictions to be exact. We see that prices of individual houses vary from one another, even if they are in the same neighborhood or have the same size. We can get a sense of how much variability we should expect in our prediction by looking at how much the values in our dataset differ from the predicted (mean) price. The difference between the true and predicted values (\\(y_i - \\hat{y}_i\\)) is called the \\(ith\\) residual. 2.2.2 Residuals for Three Models Model with No Explanatory Variables M0Resid &lt;-ggplot(data=Houses, aes(x = 1:10, y = SalePrice)) +geom_point() + geom_segment(aes(xend = 1:10, yend = M0$fitted.values), color=&quot;red&quot;) + geom_abline(slope=0, intercept=mean(M0$fitted.values)) + xlab(&quot;&quot;) + theme_bw() + theme(axis.text.x = element_blank()) M0Resid Model with Neighborhood as Explanatory Variable Model with Square Feet as Explanatory Variable ggplot(data=Houses, aes(x = SquareFeet, y = SalePrice)) + geom_segment(aes(xend = SquareFeet, yend = M_SqFt$fitted.values), color=&quot;red&quot;) + geom_point() + geom_point(aes(y = M_SqFt$fitted.values), shape = 1) + stat_smooth(method=&quot;lm&quot;, se=FALSE)+ theme_bw() 2.2.3 Quantifying Unexplained Variablility The residuals tell us how much variability in the response variable (sale price) is left unexplained by a model. For a model with given explanatory variable(s), we can calculate the proportion of variability in the response variable explained by the model by comparing the size of the residuals to those of a model with no explanatory variables. * \\(\\displaystyle\\sum_{i=1}^n (y_i - \\hat{y})=0\\), so this is not a helpful measure, but we can instead use: \\[ \\displaystyle\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\] 2.2.4 Total Sum of Squares (SST) We call the sum of squared residuals for the model with no explanatory variables the total sum of squares, abbreviated SST. In this model, \\(\\hat{y}_i = \\bar{y}\\). \\[SST = \\displaystyle\\sum_{i=1}^n (y_i - \\bar{y})^2 \\] SalePrice &lt;- Houses$SalePrice Predicted &lt;- M0$fitted.values SquareFeet &lt;- Houses$SquareFeet Residual &lt;- M0$residuals ResidSq &lt;- Residual^2 Residdf &lt;- data.frame(SquareFeet, SalePrice, Predicted, Residual, ResidSq) Residdf ## SquareFeet SalePrice Predicted Residual ResidSq ## 1 1499 187.00 148.039 38.961 1517.959521 ## 2 1456 163.99 148.039 15.951 254.434401 ## 3 1525 235.00 148.039 86.961 7562.215521 ## 4 672 113.00 148.039 -35.039 1227.731521 ## 5 1179 110.00 148.039 -38.039 1446.965521 ## 6 930 84.90 148.039 -63.139 3986.533321 ## 7 864 123.00 148.039 -25.039 626.951521 ## 8 1414 176.50 148.039 28.461 810.028521 ## 9 1144 150.00 148.039 1.961 3.845521 ## 10 1008 137.00 148.039 -11.039 121.859521 sum(Houses$SalePrice - mean(Houses$SalePrice)) ## [1] 0.0000000000001421085 sum((Houses$SalePrice - mean(Houses$SalePrice))^2) ## [1] 17558.52 knitr::include_graphics(&quot;SST.png&quot;) SST represents the total amount of variability in sale price (without accounting for any explanatory variables). 2.2.5 SSR and SSM When we fit a model with an explanatory variable (such as neighborhood), we can see how much the sum of squared residuals decreases, based on information introduced by that explanatory variable. SalePrice &lt;- Houses$SalePrice Predicted &lt;- M_Nbhd$fitted.values Residual &lt;- M_Nbhd$residuals ResidSq &lt;- Residual^2 Neighborhood &lt;- Houses$Neighborhood Residdf &lt;- data.frame(Neighborhood, SalePrice, Predicted, Residual, ResidSq) Residdf ## Neighborhood SalePrice Predicted Residual ResidSq ## 1 CollgCr 187.00 195.3300 -8.330000 69.38890 ## 2 CollgCr 163.99 195.3300 -31.340000 982.19560 ## 3 CollgCr 235.00 195.3300 39.670000 1573.70890 ## 4 Edwards 113.00 102.6333 10.366667 107.46778 ## 5 Edwards 110.00 102.6333 7.366667 54.26778 ## 6 Edwards 84.90 102.6333 -17.733333 314.47111 ## 7 NAmes 123.00 146.6250 -23.625000 558.14062 ## 8 NAmes 176.50 146.6250 29.875000 892.51562 ## 9 NAmes 150.00 146.6250 3.375000 11.39063 ## 10 NAmes 137.00 146.6250 -9.625000 92.64062 sum(M_Nbhd$residuals^2) ## [1] 4656.188 the variability in sale price remaining unexplained even after accounting for neighborhood is given by the sum of squared residuals. We abbreviate this SSR, for sum of squared residuals. \\[ \\text{SSR} = \\text{Variability Remaining}=\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2 \\] the difference, \\(\\text{SST} - \\text{SSR} = 17558.52 - 4656.188 = 12902.33\\) tells us how much variability in sale price is explained by the model using neighborhood as an explanatory variable. We call this quantity the sum of squares explained by the model, abbreviated SSM. The proportion of variability in sale price explained by the model using neighborhoods as an explanatory variable is \\[\\frac{SSM}{SST}=\\frac{12902.33}{17558.52} = 0.7348 \\] 73.5% of the variation in house price is explained by the model using neighborhood as an explanatory variable. The proportion of variability in the response variable explained by a model with given explanatory variables is called the coefficient of determination, and is given the symbol \\(R^2\\). Our value matches the value of “Multiple R-squared” in the 2nd last line of the R model summary. summary(M_Nbhd) ## ## Call: ## lm(formula = SalePrice ~ Neighborhood, data = Houses) ## ## Residuals: ## Min 1Q Median 3Q Max ## -31.340 -15.706 -2.477 9.617 39.670 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 195.33 14.89 13.118 0.00000349 *** ## NeighborhoodEdwards -92.70 21.06 -4.402 0.00315 ** ## NeighborhoodNAmes -48.70 19.70 -2.473 0.04267 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 25.79 on 7 degrees of freedom ## Multiple R-squared: 0.7348, Adjusted R-squared: 0.6591 ## F-statistic: 9.699 on 2 and 7 DF, p-value: 0.009603 2.2.6 Summary: SST, SSR, SSM, \\(R^2\\) the total variability in house prices is the sum of the squared differences between price and average price. \\[\\text{Total Variability in Price}= \\text{SST} =\\displaystyle\\sum_{i=1}^n(y_i-\\bar{y})^2\\] the variability remaining unexplained even after accounting for neighborhood is given by the sum of squared residuals. We abbreviate this SSR, for sum of squared residuals. \\[ \\text{SSR} = \\text{Variability Remaining}=\\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2 \\] the variability explained by the model, abbreviated SSM, is given by \\[ \\text{SSM} = \\text{SST} - \\text{SSR} \\] It can be shown that \\(\\text{SSM}=\\displaystyle\\sum_{i=1}^n(\\hat{y}_i-\\bar{y})^2\\). These abbreviations here vary across texts. Be careful! The coefficient of determination (abbreviated \\(R^2\\)) is defined as \\[R^2=\\frac{\\text{Variability Explained by Model}}{\\text{Total Variability}}=\\frac{\\text{SSM}}{\\text{SST}} =\\frac{\\displaystyle\\sum_{i=1}^n(\\hat{y}_i-\\bar{y})^2}{\\displaystyle\\sum_{i=1}^n(y_i-\\bar{y})^2}\\] 2.2.7 \\(R^2\\) Visually Blue Area = Total Variability (SST) Red Area = Variability Remaining Unexplained by Model (SSR) Blue Area - Red Area = Variability Explained by Model (SSM) \\(R^2 = \\frac{\\text{Area of Blue Squares} - \\text{Area of Red Squares}}{\\text{Area of Blue Squares}} = \\frac{\\text{SST}-\\text{SSR}}{\\text{SST}}= \\frac{\\text{SSM}}{\\text{SST}}\\) 2.2.8 Residuals for Square Feet Model ggplot(data=Houses, aes(x = SquareFeet, y = SalePrice)) + geom_segment(aes(xend = SquareFeet, yend = M_SqFt$fitted.values), color=&quot;red&quot;) + geom_point() + geom_point(aes(y = M_SqFt$fitted.values), shape = 1) + stat_smooth(method=&quot;lm&quot;, se=FALSE)+ theme_bw() SalePrice &lt;- Houses$SalePrice Predicted &lt;- M_SqFt$fitted.values Residual &lt;- M_SqFt$residuals ResidSq &lt;- Residual^2 SquareFeet &lt;- Houses$SquareFeet Residdf &lt;- data.frame(SquareFeet, SalePrice, Predicted, Residual, ResidSq) Residdf ## SquareFeet SalePrice Predicted Residual ResidSq ## 1 1499 187.00 187.88858 -0.8885824 0.7895786 ## 2 1456 163.99 182.69449 -18.7044870 349.8578356 ## 3 1525 235.00 191.02920 43.9708019 1933.4314183 ## 4 672 113.00 87.99284 25.0071576 625.3579304 ## 5 1179 110.00 149.23485 -39.2348498 1539.3734427 ## 6 930 84.90 119.15741 -34.2574142 1173.5704309 ## 7 864 123.00 111.18508 11.8149181 139.5922893 ## 8 1414 176.50 177.62118 -1.1211847 1.2570550 ## 9 1144 150.00 145.00710 4.9929021 24.9290718 ## 10 1008 137.00 128.57926 8.4207385 70.9088361 sum(M_SqFt$residuals^2) ## [1] 5859.068 2.2.9 Variation Explained by SquareFeet Model Created at http://www.rossmanchance.com/applets/RegShuffle.htm. Blue Area = Total Variability (SST) Red Area = Variability Remaining Unexplained by Model (SSR) Blue Area - Red Area = Variability Explained by Model (SSM) \\(R^2 = \\frac{\\text{Area of Blue Squares} - \\text{Area of Red Squares}}{\\text{Area of Blue Squares}} = \\frac{\\text{SST}-\\text{SSR}}{\\text{SST}}= \\frac{\\text{SSM}}{\\text{SST}}\\) 2.2.10 Variation Explained by Square Feet Model Total variability in house prices SST = 17,558.52 Variability remaining unexplained after accounting for square feet is SSR = 5,859.07 Variation explained by model accounting for square feet is \\[ \\text{SSM} = 17,558.52 - 5,859.07 = 11,699.45 \\] Proportion of variation explained by model accounting for square feet is \\[ R^2=\\frac{11,699.45}{17,558.52}\\approx0.6663\\] 66.6% of the variation in house price is explained by the model using square feet as an explanatory variable. summary(M_SqFt) ## ## Call: ## lm(formula = SalePrice ~ SquareFeet, data = Houses) ## ## Residuals: ## Min 1Q Median 3Q Max ## -39.235 -14.309 2.052 10.966 43.971 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.82000 36.35455 0.188 0.85586 ## SquareFeet 0.12079 0.03022 3.997 0.00397 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 27.06 on 8 degrees of freedom ## Multiple R-squared: 0.6663, Adjusted R-squared: 0.6246 ## F-statistic: 15.97 on 1 and 8 DF, p-value: 0.003967 2.2.11 Linear Correlation Coefficient ggplot(data=Houses, aes(x=SquareFeet, y=SalePrice)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) For linear models with a single quantitative variable, the linear correlation coefficient \\(r=\\sqrt{R^2}\\), or \\(r=-\\sqrt{R^2}\\) (with sign matching the sign on the slope of the line), provides information about the strength and direction of the linear relationship between the variables. \\(-1 \\leq r \\leq 1\\), and \\(r\\) close to \\(\\pm1\\) provides evidence of strong linear relationship, while \\(r\\) close to 0 suggests linear relationship is weak. \\(r\\) is only relevant for models with a single quantitative explanatory variable and a quantitative response variable, while \\(R^2\\) is relevant for any linear model with a quantitative response variable. cor(Houses$SalePrice,Houses$SquareFeet) ## [1] 0.8162794 2.3 Multiple Regression Model 2.3.1 Multiple Regression Model Suppose we have information on both the neighborhood and square feet in the houses. We can account for both of these together using a multiple regression model, i.e. a model with more than one explanatory variable. kable(Houses) Neighborhood SquareFeet SalePrice CollgCr 1499 187.00 CollgCr 1456 163.99 CollgCr 1525 235.00 Edwards 672 113.00 Edwards 1179 110.00 Edwards 930 84.90 NAmes 864 123.00 NAmes 1414 176.50 NAmes 1144 150.00 NAmes 1008 137.00 How can we predict the price of a: 848 square foot house in College Creek? 1200 square foot house in North Ames? 2314 square foot house in Edwards? 2.3.2 2-Variable Model Different Slopes We could try to lines to the houses in each neighborhood, independent of the other neighborhoods. ggplot(data=Houses, aes(x=SquareFeet, y=SalePrice, color=Neighborhood)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) Since there are only 3-4 houses per neighbhorhood, though, this isn’t a lot of information to go on. 2.3.3 2-Variable Model with Constant Slope Instead, we’ll assume the rate of increase wrt. square feet (i.e. slope) is the same in each neighborhood, but that some neighborhoods are more expensive than others. This allows us to use all 10 houses to estimate slope, while allowing intercepts to differ between neighborhoods. 2.3.4 House Price 2-Variable Model Summary M_Nbhd_SqFt &lt;- lm(data=Houses, SalePrice~SquareFeet+Neighborhood) summary(M_Nbhd_SqFt) ## ## Call: ## lm(formula = SalePrice ~ SquareFeet + Neighborhood, data = Houses) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.125 -9.050 -5.653 9.069 37.791 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 106.72593 68.92188 1.549 0.172 ## SquareFeet 0.05933 0.04517 1.314 0.237 ## NeighborhoodEdwards -59.09436 32.49761 -1.818 0.119 ## NeighborhoodNAmes -25.81232 25.59807 -1.008 0.352 ## ## Residual standard error: 24.55 on 6 degrees of freedom ## Multiple R-squared: 0.7941, Adjusted R-squared: 0.6911 ## F-statistic: 7.711 on 3 and 6 DF, p-value: 0.01757 2.3.5 MR Model for SquareFeet and Neighborhood In the model using both square feet and neighborhood, the regression equation is \\(\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{SquareFeet}+ b_2\\times\\text{I}_{Edwards} + b_3 \\times\\text{I}_{NAmes}\\) \\(\\widehat{\\text{Price}}= 106.73+ 0.06 \\times\\text{SquareFeet}+ -59.09 \\times\\text{I}_{Edwards} +-25.81 \\times\\text{I}_{NAmes}\\) The intercept \\(b_0\\) represents the expected price of a house in College Creek with 0 square feet. the intercept has no meaningful interpretation in this context \\(b_1\\) represents the expected change in price (in thousands) per one-unit increase in square feet, assuming neighborhood is the same. on average, we expect the price of a house to increase by $0.05933 thousand (i.e. $59.33) for each additional square foot, assuming the houses are in the same neighborhood. \\(b_2\\) and \\(b_3\\) represent the expected difference in price between a house in the Edwards (or North Ames) neighborhood, compared to the College Creek neighborhood, assuming square footage is the same. We expect a house in the Edwards neighborhood to cost $59.094 less than a house in the College Creek Neighborhood, assuming the houses are the same size. We expect a house in the North Ames Neighborhood to cost $25.812 less than a house in the College Creek Neighborhood, assuming the houses are the same size. 2.3.6 Predicting Price in MR Model \\(\\widehat{\\text{Price}}= 106.73+ 0.06 \\times\\text{SquareFeet}+ -59.09 \\times\\text{I}_{Edwards} +-25.81 \\times\\text{I}_{NAmes}\\) 848 square foot house in College Creek \\(\\widehat{\\text{Price}}= 106.73+ 0.06 \\times848+ -59.09 \\times0 +-25.81 \\times 0 =157.0378\\) thousand 1200 square foot house in North Ames \\(\\widehat{\\text{Price}}= 106.73+ 0.06 \\times1200+ -59.09 \\times0 +-25.81 \\times1 = 152.1096\\) thousand 2314 square foot house in Edwards \\(\\widehat{\\text{Price}}= 106.73+ 0.06 \\times\\text{SquareFeet}+ -59.09 \\times1 +-25.81 \\times 0 =184.9212\\) thousand 2.3.7 Risk of Extrapolation Note that 2314 square feet is well outside the range of our observed data. We should treat this prediction with caution, since we don’t know whether the trend we see in our data will continue. 2.3.8 Residuals for 2-Variable Model 2.3.9 Residuals for 2-Variable Model (cont.) SalePrice &lt;- Houses$SalePrice Predicted &lt;- M_Nbhd_SqFt$fitted.values Residual &lt;- M_Nbhd_SqFt$residuals ResidSq &lt;- M_Nbhd_SqFt$residuals^2 Residdf &lt;- data.frame(SalePrice, Predicted, Residual, ResidSq) Residdf ## SalePrice Predicted Residual ResidSq ## 1 187.00 195.6662 -8.666221 75.103383 ## 2 163.99 193.1149 -29.124898 848.259699 ## 3 235.00 197.2089 37.791119 1428.168680 ## 4 113.00 87.5034 25.496603 650.076744 ## 5 110.00 117.5853 -7.585270 57.536321 ## 6 84.90 102.8113 -17.911333 320.815835 ## 7 123.00 132.1774 -9.177395 84.224570 ## 8 176.50 164.8106 11.689410 136.642314 ## 9 150.00 148.7907 1.209343 1.462509 ## 10 137.00 140.7214 -3.721358 13.848508 sum(M_Nbhd_SqFt$residuals^2) ## [1] 3616.139 2.3.10 Variation Explained by 2-Variable Model Total Variation in house prices: SST=17,558.52 Variation remaining unexplained after accounting for square feet is SSR=3,616.139 Variation explained by model accounting for square feet is \\[SSM=SST-SSR=17,558.52 - 3,616.139 = 13,942.38\\] Proportion of variation in house prices explained by model is: \\[ R^2 = \\frac{13,942.38}{17,558.52}\\approx0.794 \\] 79.4% of the variation in house price is explained by the model using square feet and neighborhood as an explanatory variables. 2.3.11 Model Comparison Summary Model Variables Unexplained Variability Variability Explained \\(R^2\\) 0 None 17558.52489 0 0 1 Nbhd 4656.1875667 12902.3373233 0.734819 2 Sq. Ft. 5859.0678887 11699.4570013 0.6663121 3 Nbhd, Sq. Ft. 3616.1385638 13942.3863262 0.7940523 Comments on \\(R^2\\): \\(R^2\\) will never decrease when a new variable is added to a model. This does not mean that adding more variables to a model always improves its ability to make predictions on new data. \\(R^2\\) measures how well a model fits the data on which it was built. It is possible for a model with high \\(R^2\\) to “overfit” the data it was built from, and thus perform poorly on new data. We will discuss this idea extensively later in the course. On some datasets, there is a lot of “natural” variability in the response variable, and no model will achieve a high \\(R^2\\). That’s okay. Even a model with \\(R^2 = 0.10\\) or less can provide useful information. The task of a statistician is not to achieve a model that makes perfect predictions, but rather to be able to quantify the amount of uncertainty associated with the predictions we make. 2.4 Least-Squares Estimation 2.4.1 Line of Best Fit ggplot(data=Houses, aes(x=SquareFeet, y=SalePrice)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) The line \\(\\text{Price} = 6.82 + 0.12 \\times \\text{Square Feet}\\) is considered the “line of best fit” in the sense that it minimizes the sum of the squared residuals. This Rossman-Chance applet provides an illustration of the line of best fit. 2.4.2 Least-Squares Estimation in Simple Linear Regression Consider a simple linear regression(SLR) model, which is one with a singe quantitative explanatory variable. \\(\\hat{y}_i = b_0+b_1x_i\\) we need to choose the values of \\(b_0\\) and \\(b_1\\) that minimize: \\[ \\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2 =\\displaystyle\\sum_{i=1}^n(y_i-(b_0+b_1x_i))^2 \\] 2.4.3 Least-Squares Estimation in Simple Linear Regression (cont.) Using calculus, it can be shown that this quantity is minimized when \\(b_1=\\frac{\\displaystyle\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\displaystyle\\sum_{i=1}^{n}(x_i-\\bar{x})^2}=\\frac{\\displaystyle\\sum_{i=1}^{n} x_i y_i-\\frac{\\displaystyle\\sum_{i=1}^{n} x_i \\displaystyle\\sum_{i=1}^{n} y_i }{n}}{\\left(\\displaystyle\\sum_{i=1}^{n} x_i^2 -\\frac{\\left(\\displaystyle\\sum_{i=1}^{n} x_i\\right)^2}{n}\\right)}\\) \\(b_0=\\bar{y}-b_1\\bar{x}\\) (where \\(\\bar{y}=\\frac{\\displaystyle\\sum_{i=1}^{n}{y_i}}{n}\\), and \\(\\bar{x}=\\frac{\\displaystyle\\sum_{i=1}^{n}{x_i}}{n}\\)). 2.4.4 LS Estimation for One Categorical Variable Consider a model with a single categorical variable (such as neighborhood), with G+1 categories, numbered \\(g=0,2, \\ldots, G\\) Then \\(\\hat{y}_i = b_0 + b_1x_{i1} + \\ldots +b_{G}x_{iG}\\). we need to minimize \\[ \\displaystyle\\sum_{i=1}^n(y_i-\\hat{y}_i)^2 =\\displaystyle\\sum_{i=1}^n(y_i-(b_0 + b_1x_{i1} + \\ldots +b_{G}x_{iG}))^2. \\] It can be shown that this is achieved when \\(b_0 = \\bar{y_0}\\) (i.e. the average response in the “baseline group”), and \\(b_j = \\bar{y_j} - \\bar{y}_0\\) 2.4.5 LS Estimation More Generally For multiple regression models, the logic is the same. We need to choose \\(b_0, b_1, \\ldots, b_p\\) in order to minimize \\[ \\displaystyle\\sum_{i=1}^n (y_i -\\hat{y}_i)^2 = \\displaystyle\\sum_{i=1}^n (y_i -(b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_px_{ip}))^2 \\] The mathematics, however are more complicated and require inverting a matrix. This goes beyond the scope of this class, so we will let R do the estimation and use the results. More on least squares estimation in multiple regression can be found here. 2.5 ANalysis Of VAriance 2.5.1 Submodels Model Variables Unexplained Variability Variability Explained \\(R^2\\) 0 None 17558.52489 0 0 1 Nbhd. 4656.1875667 12902.3373233 0.734819 2 Sq. Ft 5859.0678887 11699.4570013 0.6663121 3 Nbhd, Sq. Ft. 3616.1385638 13942.3863262 0.7940523 Notice that Model 1 is a submodel of Model 3, since all variables used in Model 1 are also used in Model 3. Model 2 is also a submodel of Model 3. Model 0 is a submodel of Models 1, 2, and 3. Models 1 and 2 are not submodels of each other, since Model 1 contains a variable used in Model 2 and Model 2 contains a variable not used in Model 1. 2.5.2 Comparing Submodels When one model is a submodel of another, we can compare the amount of variability explained by the models, using a technique known as ANalysis Of VAriance (ANOVA). Reduced Model: \\(\\hat{y}_i = b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_qx_{iq}\\) Full Model: \\(\\hat{y}_i = b_0 + b_1x_{i1} + b_2x_{i2} + \\ldots + b_qx_{iq} + b_{q+1}x_{i{q+1}} \\ldots + b_px_{ip}\\) p = # variables in Full Model q = # variables in Reduced Model n = number of observations We calculate a statistic called F: \\[ \\begin{aligned} F &amp;= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} \\end{aligned} \\] Questions: 1. If the full model does a much better job explaining variability in the response variable than the reduced model, will the F-statistic be large or small? Can an F-statistic ever be negative? Why or why not? 2.5.3 Comments on F-Statistic The F-statistic measures the amount of variability explained by adding additional variable(s) to the model, relative to the total amount of unexplained variability. Large values of F indicate that adding the additional explanatory variables is helpful in explaining variability in the response variable Small values of F indicate that adding new explanatory variables variables does not make much of a difference in explaining variability in the response variable What counts as “large” is depends on \\(n, p,\\) and \\(q\\). We will revisit this later in the course. 2.5.4 ANOVA F-Statistic Let’s Calculate an ANOVA F-Statistic to compare Models 2 and 3. Reduced Model: \\(\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{SquareFeet}\\) Full Model: \\(\\widehat{\\text{Price}}= b_0+ b_1 \\times\\text{SquareFeet}+ b_2\\times\\text{I}_{Edwards} + b_3 \\times\\text{I}_{NAmes}\\) \\[ \\begin{aligned} F &amp;= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} \\\\ &amp;=\\frac{\\frac{5,859.07-3,616.14}{3-1}}{\\frac{3,616.13}{10-(3+1)}} \\\\ \\end{aligned} \\] SSR2 &lt;- sum(M_SqFt$residuals^2); SSR3 &lt;- sum(M_Nbhd_SqFt$residuals^2); ((SSR2-SSR3)/(3-1))/((SSR3)/(10-(3+1))) ## [1] 1.860766 2.5.5 ANOVA F-Statistic for M2 vs M3 in R anova(M_SqFt, M_Nbhd_SqFt) ## Analysis of Variance Table ## ## Model 1: SalePrice ~ SquareFeet ## Model 2: SalePrice ~ SquareFeet + Neighborhood ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 8 5859.1 ## 2 6 3616.1 2 2242.9 1.8608 0.2351 Notice the F-statistic has the same value. Later, we will examine what this tells us about adding Neighborhood to a model already containing square feet as an explanatory variable. 2.5.6 ANOVA F-Statistic for M1 vs M0 Now, let’s compare Models 0 and 1. Reduced Model: \\(\\widehat{\\text{Price}}_i = b_0\\) Full Model: \\(\\widehat{\\text{Price}}_i = b_0 + b_1\\text{I}_{\\text{Edwards}} + b_2\\text{I}_{\\text{NAmes}}\\) \\[ \\begin{aligned} F &amp;= \\frac{\\frac{\\text{SSR}_{\\text{Reduced}}-\\text{SSR}_{\\text{Full}}}{p-q}}{\\frac{\\text{SSR}_{\\text{Full}}}{n-(p+1)}} \\\\ &amp;=\\frac{\\frac{17558.52-4656.19}{2-0}}{\\frac{4656.19}{10-(2+1)}} \\end{aligned} \\] SSR0 &lt;- sum(M0$residuals^2); SSR1 &lt;- sum(M_Nbhd$residuals^2); ((SSR0-SSR1)/(2-0))/((SSR1)/(10-(2+1))) ## [1] 9.698531 2.5.7 ANOVA F-Statistic for M0 vs M1 in R anova(M0, M_Nbhd) ## Analysis of Variance Table ## ## Model 1: SalePrice ~ 1 ## Model 2: SalePrice ~ Neighborhood ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 9 17558.5 ## 2 7 4656.2 2 12902 9.6985 0.009603 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 2.5.8 ANOVA F-Statistic for Categorical Variables The difference between M1 and M0 is that M1 considers the house’s neighborhood, while M0 does not. If neighborhood is helpful in modeling house price, then we would expect to see a high F-statistic. Another way to think about this is that if the amount of variability in house prices between different neighborhoods is large, relative to the amount of variability within neighborhoods, then the F-statistic should be large. In fact, an alternative (an mathematically equivalent) way to calculate the F-statistic is to calculate the ratio of variability between different neighborhoods, relative to the amount of variability within neighborhoods. 2.5.9 F-Statistic for Categorical Variables Illustration An F-statistic compares the amount of variability between groups to the amount of variability within groups. Scenario 1 Scenario 2 variation between groups High Low variation within groups Low High F Statistic Large Small Result Evidence of Group Differences No evidence of differences Question: Suppose, in these scenarios, we perform an F-test comparing a model the includes group as an explanatory variable, to one the includes no explanatory variables? Which scenario (1 or 2) would you expect to result in a larger F-statistic? 2.5.10 Alternative F-Statistic Formula For a categorical variable with \\(g\\) groups, let \\(\\bar{y}_{1\\cdot}, \\ldots, \\bar{y}_{g\\cdot}\\) represent the mean response for each group. let \\(n_1, \\ldots, n_g\\) represent the sample size for each group Then \\(\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}n_i(y_{i\\cdot}-\\bar{y}_{\\cdot\\cdot})^2}{g-1}\\) gives a measure of how much the group means differ, and \\(\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i\\cdot})^2}{n-g}\\) gives a measure of how much individual observations differ within groups An alternative formula for this F-statistic is: \\[ F= \\frac{\\text{Variability between Neighborhoods}}{\\text{Variability within Neighborhoods}}= \\frac{\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}n_i(y_{i\\cdot}-\\bar{y}_{\\cdot\\cdot})^2}{g-1}}{\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i\\cdot})^2}{n-g}} \\] It can be shown that this statistic is equivalent to the one we saw previously. 2.5.11 Calculating F-Statistic for Categorical Variables We have seen previously that: \\(\\bar{y}_{\\cdot\\cdot}=148.039\\) (overall average price), and \\(n=10\\) \\(\\bar{y}_{1\\cdot}=195.330\\) (average price in College Creek), and \\(n_1=3\\) \\(\\bar{y}_{2\\cdot}=102.633\\) (average price in Edwards), and \\(n_2=4\\) \\(\\bar{y}_{3\\cdot}=146.625\\) (average price in North Ames), and \\(n_3=3\\) Then, \\(\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{i\\cdot}-\\bar{y}_{\\cdot\\cdot})^2}{g-1} = \\frac{3(195.330-148.039)^2+3(102.633-148.039)^2+4(146.625-148.039)^2}{3-1} = \\frac{12902}{2}\\), and \\(\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i\\cdot})^2}{n-g} = \\frac{(123.00-146.625)^2+ (187.00 - 195.33)^2 + \\ldots + (137.00-146.625)^2}{10-3} = \\frac{4656}{7}\\) 2.5.12 Calculating F-Statistic for Categorical Variables \\[ F= \\frac{\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}n_i(y_{i\\cdot}-\\bar{y}_{\\cdot\\cdot})^2}{g-1}}{\\frac{\\displaystyle\\sum_{i=1}^g\\sum_{j=1}^{n_i}(y_{ij}-\\bar{y}_{i\\cdot})^2}{n-g}} = \\frac{\\frac{(195.330-148.039)^2+(102.633-148.039)^2+(146.625-148.039)^2}{3-1}}{\\frac{(123.00-146.625)^2+ (187.00 - 195.33)^2 + \\ldots + (137.00-146.625)^2}{10-3}} = \\frac{\\frac{12902}{2}}{\\frac{4656}{7}} \\] Note that the quantity in the the quantity in the third line is equivalent to the sum of the squared residuals using M2. Thus, we can calculate F using: ((3*(195.330-148.039)^2+3*(102.633-148.039)^2+4*(146.625-148.039)^2)/(3-1))/(sum(M_Nbhd$residuals^2)/(10-3)) ## [1] 9.6986 2.5.13 Alternative Calculation in R This interpretation of the F-statistic can be seen using the AOV command in R. AOV_Nbhd &lt;- aov(data=Houses, SalePrice~Neighborhood) summary(AOV_Nbhd) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Neighborhood 2 12902 6451 9.699 0.0096 ** ## Residuals 7 4656 665 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The Neighborhood line represents the variability between neighborhoods The Residuals line represents the variability within neighborhoods The first two columns give the quantities we use in our formula. The third column, representing the ratio of the first two columns is called a mean square. 2.5.14 F-Statistic in R Output The last line in the summary output includes the F-statistic for the specified model, compared to a reduced model that includes only the intercept. Reduced Model: \\(\\widehat{Y}= b_0\\) Full Model: \\(\\widehat{Y}= b_0+ b_1 X_{i1}+ \\ldots+ b_p X_{ip}\\) This statistic addresses the question “Do any of the explanatory variables help explain variability in Y?”. When there is only one explanatory variable in the model, this statistic can be used to test whether there is evidence that this statistic is associated with \\(Y\\). 2.5.15 F-Statistic in R Output M1 The F-statistic compares a full model that includes neighborhood to a reduced model that predicts each price using the overall average. summary(M_Nbhd) ## ## Call: ## lm(formula = SalePrice ~ Neighborhood, data = Houses) ## ## Residuals: ## Min 1Q Median 3Q Max ## -31.340 -15.706 -2.477 9.617 39.670 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 195.33 14.89 13.118 0.00000349 *** ## NeighborhoodEdwards -92.70 21.06 -4.402 0.00315 ** ## NeighborhoodNAmes -48.70 19.70 -2.473 0.04267 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 25.79 on 7 degrees of freedom ## Multiple R-squared: 0.7348, Adjusted R-squared: 0.6591 ## F-statistic: 9.699 on 2 and 7 DF, p-value: 0.009603 2.5.16 F-Statistic in R Output M2 The F-statistic compares a full model that includes square feet to a reduced model that predicts each price using the overall average. summary(M_SqFt) ## ## Call: ## lm(formula = SalePrice ~ SquareFeet, data = Houses) ## ## Residuals: ## Min 1Q Median 3Q Max ## -39.235 -14.309 2.052 10.966 43.971 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.82000 36.35455 0.188 0.85586 ## SquareFeet 0.12079 0.03022 3.997 0.00397 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 27.06 on 8 degrees of freedom ## Multiple R-squared: 0.6663, Adjusted R-squared: 0.6246 ## F-statistic: 15.97 on 1 and 8 DF, p-value: 0.003967 2.5.17 F-Statistic in R Output M3 The F-statistic compares a full model that includes square feet and neighborhood to a reduced model that predicts each price using only the overall average. summary(M_Nbhd_SqFt) ## ## Call: ## lm(formula = SalePrice ~ SquareFeet + Neighborhood, data = Houses) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.125 -9.050 -5.653 9.069 37.791 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 106.72593 68.92188 1.549 0.172 ## SquareFeet 0.05933 0.04517 1.314 0.237 ## NeighborhoodEdwards -59.09436 32.49761 -1.818 0.119 ## NeighborhoodNAmes -25.81232 25.59807 -1.008 0.352 ## ## Residual standard error: 24.55 on 6 degrees of freedom ## Multiple R-squared: 0.7941, Adjusted R-squared: 0.6911 ## F-statistic: 7.711 on 3 and 6 DF, p-value: 0.01757 2.5.18 When to Use F-Statistics for Model Comparison We have used F-statistics to compare models 1 and 3, and models 0 and 2. We could also calculate F-statistics comparing models 2 and 3, models 0 and 1, and models 0 and 3. We cannot use an F-statistic to compare models 1 and 2, since neither is a submodel of the other. When comparing a model to the “intercept-only” model, we can use the model summary output. When comparing other to other submodels, use the aov() or anova() commands. 2.6 Models Involving Interaction 2.6.1 Bear Weights Dataset The Bolstad R package provides data on body measurements for a sample of 143 wild bears, who were anesthetized, measured and weighed, then released. Variables include: ID.- Indentification number Age - Bear’s age, in months. Note, wild bears are always born in January, so an expert can estimate the bear’s age without directly asking it how old it is. Month- Month when the measurement was made. 1 = Jan., 12 = Dec. Since bears hibernate in the winter, their body shape probably depends on the season. Sex - 1 = male 2 = female Head.L - Length of the head, in inches Head.W. - Width of the head, in inches Neck.G. - Girth (distance around) the neck, in inches Length. - Body length, in inches Chest.G. - Girth (distance around) the chest, in inches Weight - Weight of the bear, in pounds Obs.No - Observation number for this bear. For example, the bear with ID = 41 (Bertha) was measured on four occasions, in the months coded 7, 8, 11, and 5. The value of Obs.No goes from 1 to 4 for these observations. Name - The names of the bears given to them by the researchers Question of Interest: How quickly do bears gain weight as they grow? Do male and female bears gain weight at the same rate? 2.6.2 Exploring Bears Data library(Bolstad) data(bears) glimpse(bears) ## Rows: 143 ## Columns: 12 ## $ ID &lt;int&gt; 39, 41, 41, 41, 41, 43, 43, 45, 45, 48, 69, 83, 83, 83, 83, 91… ## $ Age &lt;int&gt; 19, 19, 20, 23, 29, 19, 20, 55, 67, 81, NA, 115, 117, 124, 140… ## $ Month &lt;int&gt; 7, 7, 8, 11, 5, 7, 8, 7, 7, 9, 10, 7, 9, 4, 8, 8, 4, 9, 7, 4, … ## $ Sex &lt;int&gt; 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1,… ## $ Head.L &lt;dbl&gt; 10.0, 11.0, 12.0, 12.5, 12.0, 11.0, 12.0, 16.5, 16.5, 15.5, 16… ## $ Head.W &lt;dbl&gt; 5.0, 6.5, 6.0, 5.0, 6.0, 5.5, 5.5, 9.0, 9.0, 8.0, 8.0, 10.0, 7… ## $ Neck.G &lt;dbl&gt; 15.0, 20.0, 17.0, 20.5, 18.0, 16.0, 17.0, 28.0, 27.0, 31.0, 32… ## $ Length &lt;dbl&gt; 45.0, 47.5, 57.0, 59.5, 62.0, 53.0, 56.0, 67.5, 78.0, 72.0, 77… ## $ Chest.G &lt;dbl&gt; 23.0, 24.0, 27.0, 38.0, 31.0, 26.0, 30.5, 45.0, 49.0, 54.0, 52… ## $ Weight &lt;int&gt; 65, 70, 74, 142, 121, 80, 108, 344, 371, 416, 432, 348, 476, 4… ## $ Obs.No &lt;int&gt; 1, 1, 2, 3, 4, 1, 2, 1, 2, 1, 1, 1, 2, 3, 4, 1, 1, 2, 1, 1, 2,… ## $ Name &lt;fct&gt; Allen, Berta, Berta, Berta, Berta, Clyde, Clyde, Doc, Doc, Qui… head(bears) ## ID Age Month Sex Head.L Head.W Neck.G Length Chest.G Weight Obs.No Name ## 1 39 19 7 1 10.0 5.0 15.0 45.0 23 65 1 Allen ## 2 41 19 7 2 11.0 6.5 20.0 47.5 24 70 1 Berta ## 3 41 20 8 2 12.0 6.0 17.0 57.0 27 74 2 Berta ## 4 41 23 11 2 12.5 5.0 20.5 59.5 38 142 3 Berta ## 5 41 29 5 2 12.0 6.0 18.0 62.0 31 121 4 Berta ## 6 43 19 7 1 11.0 5.5 16.0 53.0 26 80 1 Clyde 2.6.3 Bears Data Cleaning Notice that we have multiple observations on the same bears. The procedures we have learned so far require observations to be independent of each other. Thus, we’ll keep only the first observation on each bear. Bears_Subset &lt;- bears %&gt;% filter(Obs.No == 1) The variables Month and Sex are coded as integers, but it really makes more sense to think of these as categorical variables. Thus, we will convert them to factors. Bears_Subset$Month &lt;- as.factor(Bears_Subset$Month) Bears_Subset$Sex &lt;- as.factor(Bears_Subset$Sex) summary(Bears_Subset) ## ID Age Month Sex Head.L ## Min. : 39.0 Min. : 8.00 8 :23 1:62 Min. : 9.00 ## 1st Qu.:525.0 1st Qu.: 17.00 9 :20 2:35 1st Qu.:12.00 ## Median :579.0 Median : 34.00 10 :14 Median :13.00 ## Mean :537.6 Mean : 42.64 7 :11 Mean :13.29 ## 3rd Qu.:640.0 3rd Qu.: 57.25 11 : 9 3rd Qu.:14.50 ## Max. :911.0 Max. :177.00 4 : 8 Max. :18.50 ## NA&#39;s :41 (Other):12 ## Head.W Neck.G Length Chest.G ## Min. : 4.000 Min. :10.00 Min. :36.00 Min. :19.00 ## 1st Qu.: 5.000 1st Qu.:17.50 1st Qu.:54.50 1st Qu.:30.00 ## Median : 6.000 Median :20.00 Median :61.00 Median :34.00 ## Mean : 6.364 Mean :21.03 Mean :60.41 Mean :35.93 ## 3rd Qu.: 7.000 3rd Qu.:24.00 3rd Qu.:67.00 3rd Qu.:42.00 ## Max. :10.000 Max. :32.00 Max. :83.00 Max. :55.00 ## ## Weight Obs.No Name ## Min. : 26.0 Min. :1 Ian : 2 ## 1st Qu.:114.0 1st Qu.:1 Abe : 1 ## Median :154.0 Median :1 Addy : 1 ## Mean :187.2 Mean :1 Albert : 1 ## 3rd Qu.:236.0 3rd Qu.:1 Allen : 1 ## Max. :514.0 Max. :1 (Other):89 ## NA&#39;s : 2 2.6.4 Bear Weights and Ages Histogram of Bear Weights ggplot(data=Bears_Subset, aes(x=Weight)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;Weight&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Weights of Bears&quot;) We see that bears most commonly weigh between 100 and 200 lbs, and the distribution of weights is right-skewed. Histogram of Bear Ages ggplot(data=Bears_Subset, aes(x=Age)) + geom_histogram(color=&quot;white&quot;, fill=&quot;lightblue&quot;) + xlab(&quot;Age (in months)&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Ages of Bears (in months)&quot;) Most of the bears are less than 72 months (6 years) old, although there are a few older bears. Bears with Missing Ages Recall that 41 bears had missing ages. They will be ignored if we use age in our model. To see how this might impact predicted weights, let’s look at how weights compare for bears with and without missing ages. ggplot(data=Bears_Subset, aes(x=is.na(Age), y=Weight)) + geom_boxplot() + coord_flip() Bears with missing ages do not seem to be systematically different than those whose ages are recorded, with respect to weight, so the missing ages should not cause too much concern with out model results. Boxplot of Weight by Sex ggplot(data=Bears_Subset, aes(y=Weight, x=Sex)) + geom_boxplot() + xlab(&quot;Sex(1=M, 2=F)&quot;) + ylab(&quot;Weight in lbs&quot;) + ggtitle(&quot;Weight by Sex&quot;) + coord_flip() Boxplot of Age by Sex ggplot(data=Bears_Subset, aes(y=Age, x=Sex)) + geom_boxplot() + xlab(&quot;Sex(1=M, 2=F)&quot;) + ylab(&quot;Age in Months&quot;) + ggtitle(&quot;Age by Sex&quot;) + coord_flip() The median age for female bears is older than for male bears. There are 2 male bears that are much older than any others. Scatterplot of Age and Weight ggplot(data=Bears_Subset, aes(x=Age, y=Weight, color=Sex)) + geom_point() + xlab(&quot;Age in Months&quot;) We see that there is a positive, roughly linear, relationship between age and weight. We should note that this linear trend is not likely to continue outside the range of our observed ages. 2.6.5 Two Possible Models We’ll consider two possible models, based on different sets of assumptions. Model 1 Assumptions: * Assume weight increases linearly with age * Allow for differences in expected weight for male and female bears of same age * Assume male and female bears gain weight at the same rate as they age Model 2 Assumptions * Assumes weight increases linearly with age * Allows for differences in expected weight for male and female bears of same age * Allows male and female bears to gain weight at different rates as they age 2.6.6 Models with Interaction Notice that in Model 1, the effect of age on weight is the same for both sexes, while in Model 2, the effect of age on weight depends on sex. An interaction between two explanatory variables occurs when the effect of one explanatory variable on the response depends on the other explanatory variable. In Model 2, there is an interaction between age and sex. **Note that neigher Model 1 nor Model 2 is inherently “correct”. They are just different ways to represent and model growth of bears. We should rely on our intuition, and background knowledge, as well as the data to inform us which model is more appropriate in a given context. 2.6.7 Model Equations Model 1: \\(\\widehat{\\text{Weight}}= b_0+ b_1 \\times\\text{Age}+ b_2\\times\\text{I}_{Female}\\) Model 2: \\(\\widehat{\\text{Weight}}= b_0+ b_1 \\times\\text{Age}+ b_2\\times\\text{I}_{Female} + b_3\\times\\text{Age}\\times\\text{I}_{Female}\\) The term \\(b_3\\times\\text{Age}\\times\\text{I}_{Female}\\), involving a product of the explanatory variables is called an interaction term. 2.6.8 Expected Weight Equations Model 1: \\(\\widehat{\\text{Weight}}= b_0+ b_1 \\times\\text{Age}+ b_2\\times\\text{I}_{Female}\\) Sex Pred. Weight M \\(b_0 + b_1 \\times\\text{Age}\\) F \\((b_0 + b_2) + b_1 \\times\\text{Age}\\) Model 2: \\(\\widehat{\\text{Weight}}= b_0+ b_1 \\times\\text{Age}+ b_2\\times\\text{I}_{Female} + b_3\\times\\text{Age}\\times\\text{I}_{Female}\\) Sex Pred. Weight M \\(b_0 + b_1 \\times\\text{Age}\\) F \\((b_0 + b_2) + (b_1 + b_3) \\times\\text{Age}\\) Question: How should we interpret the coefficient \\(b_1\\) in Model 1? Is the interpretation the same in Model 2? Why or why not? 2.6.9 Interpretations of Interaction Model Coefficients Model 2 \\(\\widehat{\\text{Weight}}= b_0+ b_1 \\times\\text{Age}+ b_2\\times\\text{I}_{Female} + b_3\\times\\text{Age}\\times\\text{I}_{Female}\\) Sex Pred. Weight M \\(b_0 + b_1 \\times\\text{Age}\\) F \\((b_0 + b_2) + (b_1 + b_3) \\times\\text{Age}\\) Interpretations: \\(b_0\\): expected weight of a male bear at birth (caution:extrapolation) \\(b_1\\): expected weight gain per month for male bears \\(b_2\\): expected difference in weight between female and male bears at birth (caution:extrapolation) \\(b_3\\): expected difference in monthly weight gain for female bears, compared to male bears \\(b_0+b_2\\): expected weight of a female bear at birth (caution:extrapolation) \\(b1 + b3\\): expected weight gain per month for female bears 2.6.10 Model 1 R Output Bears_M_Age_Sex &lt;- lm(data=Bears_Subset, Weight ~ Age + Sex) summary(Bears_M_Age_Sex) ## ## Call: ## lm(formula = Weight ~ Age + Sex, data = Bears_Subset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -164.194 -48.483 -3.723 27.766 188.684 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 82.6049 16.4019 5.036 0.0000058437067215 *** ## Age 2.9242 0.2914 10.035 0.0000000000000744 *** ## Sex2 -79.8967 20.1416 -3.967 0.00022 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 71.33 on 53 degrees of freedom ## (41 observations deleted due to missingness) ## Multiple R-squared: 0.6679, Adjusted R-squared: 0.6554 ## F-statistic: 53.29 on 2 and 53 DF, p-value: 0.0000000000002061 2.6.11 Model 2 R Output To fit an interaction model in R, use * instead of + Bears_M_Age_Sex_Int &lt;- lm(data=Bears_Subset, Weight~ Age*Sex) summary(Bears_M_Age_Sex_Int) ## ## Call: ## lm(formula = Weight ~ Age * Sex, data = Bears_Subset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -207.583 -38.854 -9.574 23.905 174.802 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 70.4322 17.7260 3.973 0.000219 *** ## Age 3.2381 0.3435 9.428 0.000000000000765 *** ## Sex2 -31.9574 35.0314 -0.912 0.365848 ## Age:Sex2 -1.0350 0.6237 -1.659 0.103037 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 70.18 on 52 degrees of freedom ## (41 observations deleted due to missingness) ## Multiple R-squared: 0.6846, Adjusted R-squared: 0.6664 ## F-statistic: 37.62 on 3 and 52 DF, p-value: 0.0000000000004552 2.6.12 Model 1 Interpretations \\(\\widehat{\\text{Weight}}= 82.60 + 2.92 \\times\\text{Age} - 79.90\\times\\text{I}_{Female}\\) Male Bears: \\[ \\widehat{\\text{Weight}}= 70.43 + 2.92 \\times\\text{Age} \\] Female Bears: \\[ \\widehat{\\text{Weight}}= (82.60 -79.90) + (2.92)\\times\\text{Age} \\\\ = 2.7+2.92\\times Age \\] For bears of the same sex, weight is expected to increase by 2.92 lbs. each month. On average, a female bear is expected to weigh 79.90 lbs less than a male bear of the same age. Approximately 67% of the variation in bear weights is explained by the model using age and sex as explanatory variables. 2.6.13 Model 2 Interpretations \\(\\widehat{\\text{Weight}}= 70.43 + 3.24 \\times\\text{Age}- 31.96\\times\\text{I}_{Female} -1.04\\times\\text{Age}\\times\\text{I}_{Female}\\) For Model 2, we can write different equations for bears of each sex: Male Bears: \\[ \\widehat{\\text{Weight}}= 70.43 + 3.24 \\times\\text{Age} \\] Female Bears: \\[ \\widehat{\\text{Weight}}= (70.43 -31.96) + (3.24-1.04)\\times\\text{Age} \\\\ = 38.47+ 2.20\\times Age \\] On average, a male bear is expected to weigh \\(b_0=70.43\\) lbs at birth, and a female bear is expected to weigh 38.47 lbs at birth. We should treat these interpretation with caution, since all bears in the dataset were at least 8 months old. A male bear is expected to gain 3.24 lbs per month, on average. A female bear is expected to gain 2.2 lbs per month on average. Approximately 68.5% of the variation in bear weights is explained by the model using age and sex as explanatory variables. 2.6.14 Predicting Bear Weights Suppose Sally and Yogi are 25 month old bears, Sally is a female, Yogi a male. Model 1 Sally’s Predicted Weight: \\(\\widehat{\\text{Weight}}= 82.60 + 2.920 \\times 25 -78.90\\times 1 \\approx 75.8 \\text{ lbs.}\\) Yogi’s Predicted Weight: \\(\\widehat{\\text{Weight}}= 82.60 + 2.920 \\times 25 -78.90\\times 0 \\approx 155.7 \\text{ lbs.}\\) Model 2 Sally’s Predicted Weight: \\(\\widehat{\\text{Weight}}= 70.43+ 3.24 \\times 25- 31.96\\times1 -1.04\\times25\\times1 \\approx 93.55 \\text{ lbs.}\\) Yogi’s Predicted Weight: \\(\\widehat{\\text{Weight}}= 70.43+ 3.24 \\times 25- 31.96\\times0 -1.04\\times25\\times0 \\approx 151.38 \\text{ lbs.}\\) 2.6.15 predict Function in R We can use the predict function in R to calculate predictions. We first create a dataframe with the values and categories of explanatory variables for the cases we’re trying to predict. Sex &lt;- factor(c(1, 2)) Age &lt;- c(25, 25) NewBears &lt;- data.frame(Age, Sex) Model 1 predict(Bears_M_Age_Sex, newdata=NewBears) ## 1 2 ## 155.71061 75.81394 Model 2 predict(Bears_M_Age_Sex_Int, newdata=NewBears) ## 1 2 ## 151.38566 93.55306 2.6.16 Bears Weight Model Considerations \\(R^2\\) increased from 0.67 to 0.68 when the interaction term is added. This is a relatively small increase, so we might question whether the interaction term is needed. The constant slope model allows us to combine information across sexes to estimate the expected slope. The interaction model treats the two sexes completely separately, thus has less information to use for each estimate. Which model is preferable is not clear. In addition to the data, we should consider other relevent information. Do experts who study bears believe it is reasonable to assume that male and female bears grow at the same rate per month? While the models yield drastically different predictions for very young bears, the differences are not as big for bear 8 months or older. Regardless of model we use, we should be careful about making predictions for bears that are younger or older than those that we have data on. Both of these models contain assumptions that are probably unrealistic Both models assume that bears of the same sex gain weight linearly with age. A more realistic model might assume that bears gain weight more quickly when they are younger, and that the rate of growth slows once they reach adulthood. Are there variables not included in the model that might be predictive of a bear’s weight? Of course there is no statistical model that perfectly describes expected weight gain of bears. The question is whether we can find a model that provides an approximation that is reasonable enough to draw conclusions from. As statistician George Box famously said, “All models are wrong, but some are useful.” 2.7 More on Interaction 2.7.1 Two Categorical Variables In the previous section, we saw an example of an interaction involving a quantitative variable (age), and a categorical variable (sex). Interactions between two categorical variables (or two quantitative variables) also occur in practice. Recall the definition of an interaction is that the effect of one explanatory variable on the response variable depends on the other categorical variable. 2.7.2 Bears Weight By Season and Sex We’ll investigate whether bears tend to weigh more in certain seasons than others, and whether there effect of season is the same for male and female bears. We start by creating plots to explore the relationship between season and sex. Observations per Season Recall that the dataset contains the month the bear was observed. Let’s combine the months of April and May into a category called “Spring”, June, July, and August into “Summer”, and “September”, “October”, and “November”, into “Fall”. Bears_Subset &lt;- Bears_Subset %&gt;% mutate(Season = ifelse(Month %in% 4:5, &quot;Spring&quot;, ifelse(Month %in% 6:8, &quot;Summer&quot;, &quot;Fall&quot;))) Bears_Subset$Season &lt;- as.factor(Bears_Subset$Season) ggplot(data=Bears_Subset, aes(x=Season)) + geom_bar(color=&quot;white&quot;, fill=&quot;lightblue&quot;) Boxplot of Weight by Season ggplot(data=Bears_Subset, aes(x=Season, y=Weight)) + geom_boxplot() + geom_jitter() Boxplot of Weight by Sex ggplot(data=Bears_Subset, aes(y=Weight, x=Sex)) + geom_boxplot() + geom_jitter()+ xlab(&quot;Sex(1=M, 2=F)&quot;) + ylab(&quot;Weight&quot;) + ggtitle(&quot;Weight by Sex&quot;) + coord_flip() We see that male bears (Category 1) weigh more than female bears on average, and that there is more variability in the weights of male bears than female bears. 2.7.3 Two Models We’ll consider two models: Model 1 Assumptions (No interaction): Allows weights to differ by sex and season. Assumes difference between sexes is the same in each season and difference between seasons is the same for each sex. Model 2 Assumptions: (Interaction between sex and season) Allows for differences between seasons and sexes. Allows for differences between sexes to vary between seasons and difference between seasons to vary between sexes. 2.7.4 Model Equations Model 1 Equation (No Interaction) \\(\\widehat{\\text{Weight}} = b_0+b_1\\times\\text{I}_{\\text{Spring}}+b_2\\times\\text{I}_{\\text{Summer}} + b_3\\times\\text{I}_{\\text{Female}}\\) Season Male Female Fall \\(b_0\\) \\(b_0 + b_3\\) Spring \\(b_0 + b_1\\) \\(b_0 + b_1+ b_3\\) Summer \\(b_0 + b_2\\) \\(b_0 + b_2+ b_3\\) Model 2 Equation (Interaction) \\[\\widehat{\\text{Weight}} = b_0 + b_1 \\times\\text{I}_{\\text{Spring}} + b_2\\times\\text{I}_{\\text{Summer}} +b_3\\times\\text{I}_{\\text{Female}}\\\\ +b_4\\times\\text{I}_{\\text{Spring}}\\text{I}_{\\text{Female}} +b_5\\times\\text{I}_{\\text{Summer}}\\text{I}_{\\text{Female}}\\] Male Female Fall \\(b_0\\) \\(b_0+b_3\\) Spring \\(b_0+b_1\\) \\(b_0+b_1 +b_3+b_4\\) Summer \\(b_0+b_2\\) \\(b_0+b_2+b_3+b_5\\) \\(b_4\\) and \\(b_5\\) are called interaction effects. Notice that in Model 1, \\(b3\\) always represents the expected difference in weights between male and female bears, regardless of season, while in Model 2, the expected difference between male and female bears’ weights depends on the season. 2.7.5 Bears Season and Sex Interaction Model To fit an interaction model in R, use * instead of + Bears_M_Season_Sex_Int &lt;- lm(data=Bears_Subset, Weight~Season * Sex) summary(Bears_M_Season_Sex_Int) ## ## Call: ## lm(formula = Weight ~ Season * Sex, data = Bears_Subset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -181.14 -73.14 -13.07 58.81 292.86 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 221.14 20.28 10.905 &lt;0.0000000000000002 *** ## SeasonSpring -14.00 45.99 -0.304 0.762 ## SeasonSummer -17.95 29.49 -0.608 0.544 ## Sex2 -50.07 35.54 -1.409 0.162 ## SeasonSpring:Sex2 -29.08 68.34 -0.425 0.672 ## SeasonSummer:Sex2 -30.41 50.73 -0.599 0.550 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 109.2 on 91 degrees of freedom ## Multiple R-squared: 0.1064, Adjusted R-squared: 0.0573 ## F-statistic: 2.167 on 5 and 91 DF, p-value: 0.06458 2.7.6 Predictions for Season and Sex Interaction Model \\[\\widehat{\\text{Weight}} = b_0 + b_1 \\times\\text{I}_{\\text{Spring}} + b_2\\times\\text{I}_{\\text{Summer}} +b_3\\times\\text{I}_{\\text{Female}}\\\\ +b_4\\times\\text{I}_{\\text{Spring}}\\text{I}_{\\text{Female}} +b_5\\times\\text{I}_{\\text{Summer}}\\text{I}_{\\text{Female}}\\] \\[ \\begin{aligned} \\widehat{\\text{Weight}} &amp;= 221.14 -14.00 \\times\\text{I}_{\\text{Spring}} -17.95\\times\\text{I}_{\\text{Summer}} -50.07\\times\\text{I}_{\\text{Female}} \\\\ &amp;-29.08\\times\\text{I}_{\\text{Spring}}\\text{I}_{\\text{Female}} -30.41\\times\\text{I}_{\\text{Summer}}\\text{I}_{\\text{Female}} \\end{aligned} \\] Male Female Fall 221.14 -14.00(0) -17.95(0) -50.07(0) -29.08(0)(0) -30.41(0)(0)=221.14 221.14 -14.00(0) -17.95(0) -50.07(1) -29.08(0)(1) -30.41(0)(1) =171.07 Spring 221.14 -14.00(1) -17.95(0) -50.07(0) -29.08(1)(0) -30.41(0)(0) =207.14 211.375 + 8.01221.14 -14.00(1) -17.95(0) -50.07(1) -29.08(1)(1) -30.41(0)(1)=128.00 Summer 221.14 -14.00(0) -17.95(1) -50.07(0) -29.08(0)(0) -30.41(1)(0) =203.19 221.14 -14.00(0) -17.95(1) -50.07(1) -29.08(0)(1) -30.41(1)(1) =122.71 2.7.7 Season and Sex Interaction Model Interpretations \\[\\widehat{\\text{Weight}} = b_0 + b_1 \\times\\text{I}_{\\text{Spring}} + b_2\\times\\text{I}_{\\text{Summer}} +b_3\\times\\text{I}_{\\text{Female}}\\\\ +b_4\\times\\text{I}_{\\text{Spring}}\\text{I}_{\\text{Female}} +b_5\\times\\text{I}_{\\text{Summer}}\\text{I}_{\\text{Female}}\\] Male Female Fall \\(b_0\\) \\(b_0+b_3\\) Spring \\(b_0+b_1\\) \\(b_0+b_1 +b_3+b_4\\) Summer \\(b_0+b_2\\) \\(b_0+b_2+b_3+b_5\\) \\(b_0\\) represents expected weight of male bear in fall \\(b_1\\) represents difference between expected male bear weight in spring, compared to fall \\(b_2\\) represents difference between expected male bear weight in summer, compared to fall \\(b_3\\) represents difference between expected female bear weight, compared to male bear weight in fall \\(b_4\\) represents difference in expected weights between the sexes in the spring, compared to the difference in the fall \\(b_5\\) represents difference in expected weights between the sexes in the summer, compared to the difference in the fall 2.7.8 Interpretations for Season and Sex Interaction Model \\[ \\begin{aligned} \\widehat{\\text{Weight}} &amp;= 221.14 -14.00 \\times\\text{I}_{\\text{Spring}} -17.95\\times\\text{I}_{\\text{Summer}} -50.07\\times\\text{I}_{\\text{Female}} \\\\ &amp;-29.08\\times\\text{I}_{\\text{Spring}}\\text{I}_{\\text{Female}} -30.41\\times\\text{I}_{\\text{Summer}}\\text{I}_{\\text{Female}} \\end{aligned} \\] On average, male bears are expected to weigh 221.14 lbs in the fall. On average male bears are expected to weigh 14 lbs less in the spring than in the fall. On average, male bears are expected to weigh 17.95 lbs less in the summer than in the fall. On average, female bears are expected to weigh 50.07 lbs less than male bears in the fall. On average, the female bears are expected to weigh 29.08 lbs. less relative to male bears in the spring, compared to the expected difference the fall. Thus, female bears are expected to weigh 50.07 + 29.08 = 79.17 lbs less than male bears in the spring. On average, female bears are expected to weigh 30.41 lbs less, relative to male bears in the summer, compared to the expected difference in the fall. Thus, female bears are expected to weigh \\(50.07 + 30.41 = 80.48\\) lbs less than male bears in the summer. The interaction model explains about 10.6% of the variation in bear weights. 2.7.9 Predicting New Observations in R We can calculate predictions directly in R by putting the new data in a data.frame and calling the predict() function. Season &lt;- c(&quot;Fall&quot;, &quot;Fall&quot;, &quot;Spring&quot;, &quot;Spring&quot;, &quot;Summer&quot;, &quot;Summer&quot;) Sex &lt;- factor(c(1,2,1,2,1,2)) NewBears &lt;- data.frame(Season, Sex) predict(Bears_M_Season_Sex_Int, newdata=NewBears) ## 1 2 3 4 5 6 ## 221.1379 171.0714 207.1429 128.0000 203.1923 122.7143 2.7.10 Interaction Between Two Quantitative Variables Interactions are also possible between two quantitative variables. Suppose, for example, that we want to examine the relationship between age and length on weight of a bear. If we expect that the effect of an additional inch in length, on the weight of a bear, might be different depending on the bear’s age, then this would be an example of an interaction between age and length. 2.7.11 Models for Age and Length Model 1 Assumptions (No interaction): Allows weights to differ by age and length Assumes rate of change in weight with respect to age is the same, regardless of length, and rate of change in age with respect to length is same, regardless of age. Model 2 Assumptions: (Interaction between age and length) Allows for differences between seasons and sexes. Allows rate of change in weight with respect to age, to differ, depending on length, and rate of change in weight with respect to length, to differ, depending on age. 2.7.12 Model Equations Model 1 Equation (No Interaction) \\(\\widehat{\\text{Weight}} = b_0+b_1\\times\\text{Age}+b_2\\times\\text{Length}\\) Model 2 Equation (Interaction) \\(\\widehat{\\text{Weight}} = b_0+b_1\\times\\text{Age}+b_2\\times\\text{Length} + b_3 \\times\\text{Age}\\times\\text{Length}\\) \\(b_3\\) is an interaction effects. 2.7.13 6 and 24 Month Old Bears In the interaction model (Model 2), the effect of length on weight depends on age: For a 6-month old bear: \\[ \\begin{aligned} \\widehat{\\text{Weight}} &amp; = b_0+6b_1+b_2\\times\\text{Length} + 6b_3 \\times\\text{Length} \\\\ &amp; = (b_0+6b_1)+(b_2+6b_3)\\times\\text{Length} \\end{aligned} \\] For each additional inch in length, the weight of a 6-month old bear is expected to increase by \\(b_2+6b_3\\) pounds. For a 24-month old bear: \\[ \\begin{aligned} \\widehat{\\text{Weight}} &amp; = b_0+6b_1+b_2\\times\\text{Length} + 6b_3 \\times\\text{Length} \\\\ &amp; = (b_0+24b_1)+(b_2+24b_3)\\times\\text{Length} \\end{aligned} \\] For each additional inch in length, the weight of a 24-month old bear is expected to increase by \\(b_2+24b_3\\) pounds. 2.7.14 Interaction Model in R Bears_M_Age_Length_Int &lt;- lm(data=Bears_Subset, Weight~ Age*Length) summary(Bears_M_Age_Length_Int) ## ## Call: ## lm(formula = Weight ~ Age * Length, data = Bears_Subset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -121.93 -29.90 -7.37 16.25 130.22 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -165.82578 54.76390 -3.028 0.003826 ** ## Age -7.66320 1.97286 -3.884 0.000292 *** ## Length 5.62418 1.00480 5.597 0.000000826 *** ## Age:Length 0.12577 0.02848 4.416 0.000051090 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 49.1 on 52 degrees of freedom ## (41 observations deleted due to missingness) ## Multiple R-squared: 0.8456, Adjusted R-squared: 0.8367 ## F-statistic: 94.94 on 3 and 52 DF, p-value: &lt; 0.00000000000000022 2.7.15 Age, Length Interaction Interpretations For a 6-month old bear: \\[ \\begin{aligned} \\widehat{\\text{Weight}} &amp;= (-165.83+6(-7.66))+(5.62+6(0.13))\\times\\text{Length} \\\\ &amp; = -211.79+6.4\\times\\text{Length} \\end{aligned} \\] For a 24-month old bear: \\[ \\begin{aligned} \\widehat{\\text{Weight}} &amp;= (-165.83+24(-7.66))+(5.62+24(0.13))\\times\\text{Length} \\\\ &amp; = -349.67+8.65\\times\\text{Length} \\end{aligned} \\] For each additional inch in length, the weight of a 6-month old bear is expected to increase by \\(5.62+6(0.13)=6.4\\) pounds. For each additional inch in length, the weight of a 24-month old bear is expected to increase by \\(5.62+24(0.13)=8.65\\) pounds. The interaction model explains about 85% of total variability in bear weight. We could, of course, perform similar calculations for bears of any age/length. The key is that the effect of length on weight depends on age (and likewise, the effect of age on weight depends on length). 2.7.16 Interaction vs Correlation It is easy to confuse the concept of interaction with that of correlation (seen in Section 1.2). A correlation between two variables means that as one increases, the other is more likely to increase or decrease. An interaction between two explanatory variables means that the effect of one on the response depends on the other. For example, we might expect that bigger cars will get lower gas mileage. This is an example of a (negative) correlation between gas mileage and size of the car. In the example, there would only be an interaction, if the effect of gas mileage on a response variable (say price) depends on the size of the car. If for example, better gas mileage was associated with higher price for small cars, but had no impact on price for large cars, then this would be an example of an interaction between size and gas mileage. "],["hypothesis-testing-via-permutation.html", "Chapter 3 Hypothesis Testing via Permutation 3.1 Test for Difference in Means 3.2 Test for Difference in Standard Deviation 3.3 Test for Regression Slope 3.4 Test for Comparing Multiple Groups", " Chapter 3 Hypothesis Testing via Permutation Learning Outcomes: State null and alternative hypotheses associated with models involving categorical and quantitative explanatory variables. Explain how to use permutation tests for hypotheses involving means, medians, F-statistics, slopes, and other regression coefficients, as well as functions of these statistics. Interpret p-values in context. Explain the conclusions we should draw from from a hypothesis test, while accounting for other information available in a dataset. Explain how to simultaneously test for differences between multiple groups. Distinguish between statistical significance and practical importance. 3.1 Test for Difference in Means 3.1.1 Mercury Levels in Florida Lakes A 2004 study by Lange, T., Royals, H. and Connor, L. examined Mercury accumulation in large-mouth bass, taken from a sample of 53 Florida Lakes. If Mercury accumulation exceeds 0.5 ppm, then there are environmental concerns. In fact, the legal safety limit in Canada is 0.5 ppm, although it is 1 ppm in the United States. Figure 3.1: https://www.maine.gov/ifw/fish-wildlife/fisheries/species-information/largemouth-bass.html 3.1.2 Florida Lakes Dataset data(&quot;FloridaLakes&quot;) glimpse(FloridaLakes) ## Rows: 53 ## Columns: 12 ## $ ID &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1… ## $ Lake &lt;chr&gt; &quot;Alligator&quot;, &quot;Annie&quot;, &quot;Apopka&quot;, &quot;Blue Cypress&quot;, &quot;Bri… ## $ Alkalinity &lt;dbl&gt; 5.9, 3.5, 116.0, 39.4, 2.5, 19.6, 5.2, 71.4, 26.4, 4… ## $ pH &lt;dbl&gt; 6.1, 5.1, 9.1, 6.9, 4.6, 7.3, 5.4, 8.1, 5.8, 6.4, 5.… ## $ Calcium &lt;dbl&gt; 3.0, 1.9, 44.1, 16.4, 2.9, 4.5, 2.8, 55.2, 9.2, 4.6,… ## $ Chlorophyll &lt;dbl&gt; 0.7, 3.2, 128.3, 3.5, 1.8, 44.1, 3.4, 33.7, 1.6, 22.… ## $ AvgMercury &lt;dbl&gt; 1.23, 1.33, 0.04, 0.44, 1.20, 0.27, 0.48, 0.19, 0.83… ## $ NumSamples &lt;int&gt; 5, 7, 6, 12, 12, 14, 10, 12, 24, 12, 12, 12, 7, 43, … ## $ MinMercury &lt;dbl&gt; 0.85, 0.92, 0.04, 0.13, 0.69, 0.04, 0.30, 0.08, 0.26… ## $ MaxMercury &lt;dbl&gt; 1.43, 1.90, 0.06, 0.84, 1.50, 0.48, 0.72, 0.38, 1.40… ## $ ThreeYrStdMercury &lt;dbl&gt; 1.53, 1.33, 0.04, 0.44, 1.33, 0.25, 0.45, 0.16, 0.72… ## $ AgeData &lt;int&gt; 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1… We are interested in whether mercury levels are higher or lower, on average, in Northern Florida compared to Southern Florida. We’ll divide the state along route 50, which runs East-West, passing through Northern Orlando. Figure 3.2: from Google Maps We add a variable indicating whether each lake lies in the northern or southern part of the state. library(Lock5Data) data(FloridaLakes) #Location relative to rt. 50 FloridaLakes$Location &lt;- as.factor(c(&quot;S&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;S&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;S&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;N&quot;,&quot;S&quot;,&quot;S&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;S&quot;,&quot;N&quot;,&quot;S&quot;,&quot;S&quot;,&quot;S&quot;,&quot;S&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;,&quot;N&quot;)) print.data.frame(data.frame(FloridaLakes%&gt;% select(Lake, Location, AvgMercury)), row.names = FALSE) ## Lake Location AvgMercury ## Alligator S 1.23 ## Annie S 1.33 ## Apopka N 0.04 ## Blue Cypress S 0.44 ## Brick S 1.20 ## Bryant N 0.27 ## Cherry N 0.48 ## Crescent N 0.19 ## Deer Point N 0.83 ## Dias N 0.81 ## Dorr N 0.71 ## Down S 0.50 ## Eaton N 0.49 ## East Tohopekaliga S 1.16 ## Farm-13 N 0.05 ## George N 0.15 ## Griffin N 0.19 ## Harney N 0.77 ## Hart S 1.08 ## Hatchineha S 0.98 ## Iamonia N 0.63 ## Istokpoga S 0.56 ## Jackson N 0.41 ## Josephine S 0.73 ## Kingsley N 0.34 ## Kissimmee S 0.59 ## Lochloosa N 0.34 ## Louisa S 0.84 ## Miccasukee N 0.50 ## Minneola N 0.34 ## Monroe N 0.28 ## Newmans N 0.34 ## Ocean Pond N 0.87 ## Ocheese Pond N 0.56 ## Okeechobee S 0.17 ## Orange N 0.18 ## Panasoffkee N 0.19 ## Parker S 0.04 ## Placid S 0.49 ## Puzzle N 1.10 ## Rodman N 0.16 ## Rousseau N 0.10 ## Sampson N 0.48 ## Shipp S 0.21 ## Talquin N 0.86 ## Tarpon S 0.52 ## Tohopekaliga S 0.65 ## Trafford S 0.27 ## Trout S 0.94 ## Tsala Apopka N 0.40 ## Weir N 0.43 ## Wildcat N 0.25 ## Yale N 0.27 3.1.3 Comparing Northern and Southern Lakes We are interested in investigating whether average mercury levels are higher in either Northern Florida or Southern Florida than the other. LakesBP &lt;- ggplot(data=FloridaLakes, aes(x=Location, y=AvgMercury, fill=Location)) + geom_boxplot() + geom_jitter() + ggtitle(&quot;Mercury Levels in Florida Lakes&quot;) + xlab(&quot;Location&quot;) + ylab(&quot;Mercury Level&quot;) + theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + coord_flip() LakesBP LakesTable &lt;- FloridaLakes %&gt;% group_by(Location) %&gt;% summarize(MeanHg=mean(AvgMercury), StDevHg=sd(AvgMercury), N=n()) kable(LakesTable) Location MeanHg StDevHg N N 0.4245455 0.2696652 33 S 0.6965000 0.3838760 20 3.1.4 Model for Northern and Southern Lakes \\(\\widehat{\\text{Hg}} = b_0 +b_1\\text{I}_{\\text{South}}\\) \\(b_0\\) represents the mean mercury level for lakes in North Florida, and \\(b_1\\) represents the mean difference in mercury level for lakes in South Florida, compared to North Florida 3.1.5 Model for Lakes R Output Lakes_M &lt;- lm(data=FloridaLakes, AvgMercury ~ Location) summary(Lakes_M) ## ## Call: ## lm(formula = AvgMercury ~ Location, data = FloridaLakes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.65650 -0.23455 -0.08455 0.24350 0.67545 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.42455 0.05519 7.692 0.000000000441 *** ## LocationS 0.27195 0.08985 3.027 0.00387 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3171 on 51 degrees of freedom ## Multiple R-squared: 0.1523, Adjusted R-squared: 0.1357 ## F-statistic: 9.162 on 1 and 51 DF, p-value: 0.003868 3.1.6 Interpreting Lakes Regression Output \\(\\widehat{\\text{Hg}} = 0.4245455 +0.2719545\\text{I}_{\\text{South}}\\) \\(b_1 = 0.27915= 0.6965 - 0.4245\\) is equal to the difference in mean mercury levels between Northern and Southern lakes. (We’ve already seen that for categorical variables, the least-squares estimate is the mean, so this makes sense.) We can use \\(b_1\\) to assess the size of the difference in mean mercury concentration levels. Since the lakes we observed are only a sample of all lakes, we cannot assume the difference in mercury concentrations is exactly 0.4245 for all Northern vs Southern Florida lakes. 3.1.7 Evidence of Difference? Do these results provide evidence that among all Florida lakes, the mean mercury level is higher in the South than in the North? Possible Explanations: 1. There really is a difference in average mercury level between lakes in Northern and Southern Florida. 2. There really is no difference in average mercury levels between lakes in Northern and Southern Florida, and we just happened, by chance, to select more lakes with higher mercury concentrations in Southern Florida. Question: Which of these explanations do you think is more reasonable? 3.1.8 Permutation Test Key Question: How likely is it that we would have observed a difference in means (i.e. a value of \\(b_1\\)) as extreme as 0.6965-0.4245 = 0.27195 ppm, merely by chance, if there is really no relationship between location and mercury level? We can answer the key question using a procedure known as a permutation test. In a permutation test, we randomly permute our data to simulate a situation where there is no relationship between our explanatory and response variable. We observe whether it is plausible to observe values of a statistic (in this case the difference in means) as extreme or more extreme than what we saw in the actual data. We’ll simulate situations where there is no relationship between location and mercury level, and see how often we observe a difference in means (\\(b_1\\)) as extreme as 0.27195. Procedure: Randomly shuffle the locations of the lakes, so that any relationship between location and mercury level is due only to chance. Calculate the difference in mean mercury levels (i.e. value of \\(b_1\\)) in “Northern” and “Southern” lakes, using the shuffled data. Repeat steps 1 and 2 many (say 10,000) times, recording the difference in means (i.e. value of \\(b_1\\)) each time. Analyze the distribution of mean differences, simulated under the assumption that there is no relationship between location and mercury level. Look whether the actual difference we observed is consistent with the simulation results. 3.1.9 Applet for Permutation Tests An Art of Stat Web App performs the steps listed above. Follow these steps to perform the simulation. 1. Open the app. 2. Under “Enter Data”, Choose “Provide Own”. 3. For “Response Variable”, type in “Mercury Concentration.” 4. For “Group 1 Label”, enter “North”, and for “Group 2 Label”, enter “South”. 5. Copy/paste the following values into the “Group 1 Data” box. These are the mercury concentrations in the lakes in North Florida. NLakesHg &lt;- as.data.frame(FloridaLakes %&gt;% filter(Location==&quot;N&quot;))$AvgMercury kable(NLakesHg, fill = getOption(&quot;width&quot;)) x 0.04 0.27 0.48 0.19 0.83 0.81 0.71 0.49 0.05 0.15 0.19 0.77 0.63 0.41 0.34 0.34 0.50 0.34 0.28 0.34 0.87 0.56 0.18 0.19 1.10 0.16 0.10 0.48 0.86 0.40 0.43 0.25 0.27 Copy/paste the following values into the “Group 2 Data” box. These are the mercury concentrations in the lakes in North Florida. NLakesHg &lt;- as.data.frame(FloridaLakes %&gt;% filter(Location==&quot;S&quot;))$AvgMercury kable(NLakesHg, fill = getOption(&quot;width&quot;)) x 1.23 1.33 0.44 1.20 0.50 1.16 1.08 0.98 0.56 0.73 0.59 0.84 0.17 0.04 0.49 0.21 0.52 0.65 0.27 0.94 For the question: “Select how many permutations you want to generate:” Choose 1. Click “Generate” The app will randomly assign the lakes to groups, with sizes matching the original data, and calculate the mean for each group. Observe means in the “Permuted Sample” and the difference in means. Is the difference as extreme as we observed in the actual data? STOP HERE UNTIL FURTHER INSTRUCTED. Now generate 5 more permutations. Did you get any differences in means as extreme as we observed in the original data? What does this tell us about the likelihood of observing a difference in means as extreme as 0.27 by chance? STOP HERE UNTIL FURTHER INSTRUCTED. Now generate 10,000 permutations. How often do you get a difference as extreme as we observed in the original data? What does this tell us about the likelihood of observing a difference in means as extreme as 0.27 by chance? 3.1.10 Five Permutations in R We’ll use R to perform permutation tests in the same manner as is done in the Art of Stat App. First Permutation Recall these groups were randomly assigned, so the only differences in averages are due to random chance. ShuffledLakes &lt;- FloridaLakes ## create copy of dataset ShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] Shuffle1df &lt;- data.frame(FloridaLakes$Lake, FloridaLakes$Location, FloridaLakes$AvgMercury, ShuffledLakes$Location) names(Shuffle1df) &lt;- c(&quot;Lake&quot;, &quot;Location&quot;, &quot;AvgMercury&quot;, &quot;Shuffled Location&quot;) kable(head(Shuffle1df)) Lake Location AvgMercury Shuffled Location Alligator S 1.23 N Annie S 1.33 S Apopka N 0.04 S Blue Cypress S 0.44 N Brick S 1.20 N Bryant N 0.27 N LakesPerm &lt;- ggplot(data=Shuffle1df, aes(x=`Shuffled Location`, y=AvgMercury, fill=`Shuffled Location`)) + geom_boxplot() + geom_jitter() + ggtitle(&quot;Mercury Levels in Florida Lakes&quot;) + xlab(&quot;Location&quot;) + ylab(&quot;Mercury Level&quot;) + theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + coord_flip() LakesPerm LakesPermTable &lt;- Shuffle1df %&gt;% group_by(`Shuffled Location`) %&gt;% summarize(MeanHg=mean(AvgMercury), StDevHg=sd(AvgMercury), N=n()) kable(LakesPermTable) Shuffled Location MeanHg StDevHg N N 0.5460606 0.3552986 33 S 0.4960000 0.3225784 20 Second Permutation ShuffledLakes &lt;- FloridaLakes ## create copy of dataset ShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] kable(head(Shuffle1df)) Lake Location AvgMercury Shuffled Location Alligator S 1.23 N Annie S 1.33 S Apopka N 0.04 S Blue Cypress S 0.44 N Brick S 1.20 N Bryant N 0.27 N Shuffle1df &lt;- data.frame(FloridaLakes$Lake, FloridaLakes$Location, FloridaLakes$AvgMercury, ShuffledLakes$Location) names(Shuffle1df) &lt;- c(&quot;Lake&quot;, &quot;Location&quot;, &quot;AvgMercury&quot;, &quot;Shuffled Location&quot;) LakesPerm &lt;- ggplot(data=Shuffle1df, aes(x=`Shuffled Location`, y=AvgMercury, fill=`Shuffled Location`)) + geom_boxplot() + geom_jitter() + ggtitle(&quot;Mercury Levels in Florida Lakes&quot;) + xlab(&quot;Location&quot;) + ylab(&quot;Mercury Level&quot;) + theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + coord_flip() LakesPerm LakesPermTable &lt;- Shuffle1df %&gt;% group_by(`Shuffled Location`) %&gt;% summarize(MeanHg=mean(AvgMercury), StDevHg=sd(AvgMercury), N=n()) kable(LakesPermTable) Shuffled Location MeanHg StDevHg N N 0.4839394 0.3316431 33 S 0.5985000 0.3527975 20 Third Permutation ShuffledLakes &lt;- FloridaLakes ## create copy of dataset ShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] kable(head(Shuffle1df)) Lake Location AvgMercury Shuffled Location Alligator S 1.23 S Annie S 1.33 S Apopka N 0.04 N Blue Cypress S 0.44 S Brick S 1.20 N Bryant N 0.27 S Shuffle1df &lt;- data.frame(FloridaLakes$Lake, FloridaLakes$Location, FloridaLakes$AvgMercury, ShuffledLakes$Location) names(Shuffle1df) &lt;- c(&quot;Lake&quot;, &quot;Location&quot;, &quot;AvgMercury&quot;, &quot;Shuffled Location&quot;) LakesPerm &lt;- ggplot(data=Shuffle1df, aes(x=`Shuffled Location`, y=AvgMercury, fill=`Shuffled Location`)) + geom_boxplot() + geom_jitter() + ggtitle(&quot;Mercury Levels in Florida Lakes&quot;) + xlab(&quot;Location&quot;) + ylab(&quot;Mercury Level&quot;) + theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + coord_flip() LakesPerm LakesPermTable &lt;- Shuffle1df %&gt;% group_by(`Shuffled Location`) %&gt;% summarize(MeanHg=mean(AvgMercury), StDevHg=sd(AvgMercury), N=n()) kable(LakesPermTable) Shuffled Location MeanHg StDevHg N N 0.5324242 0.3863777 33 S 0.5185000 0.2583607 20 Fourth Permutation ShuffledLakes &lt;- FloridaLakes ## create copy of dataset ShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] kable(head(Shuffle1df)) Lake Location AvgMercury Shuffled Location Alligator S 1.23 N Annie S 1.33 N Apopka N 0.04 N Blue Cypress S 0.44 S Brick S 1.20 N Bryant N 0.27 S Shuffle1df &lt;- data.frame(FloridaLakes$Lake, FloridaLakes$Location, FloridaLakes$AvgMercury, ShuffledLakes$Location) names(Shuffle1df) &lt;- c(&quot;Lake&quot;, &quot;Location&quot;, &quot;AvgMercury&quot;, &quot;Shuffled Location&quot;) LakesPerm &lt;- ggplot(data=Shuffle1df, aes(x=`Shuffled Location`, y=AvgMercury, fill=`Shuffled Location`)) + geom_boxplot() + geom_jitter() + ggtitle(&quot;Mercury Levels in Florida Lakes&quot;) + xlab(&quot;Location&quot;) + ylab(&quot;Mercury Level&quot;) + theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + coord_flip() LakesPerm LakesPermTable &lt;- Shuffle1df %&gt;% group_by(`Shuffled Location`) %&gt;% summarize(MeanHg=mean(AvgMercury), StDevHg=sd(AvgMercury), N=n()) kable(LakesPermTable) Shuffled Location MeanHg StDevHg N N 0.4833333 0.3314803 33 S 0.5995000 0.3527109 20 Fifth Permutation ShuffledLakes &lt;- FloridaLakes ## create copy of dataset ShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] kable(head(Shuffle1df)) Lake Location AvgMercury Shuffled Location Alligator S 1.23 N Annie S 1.33 S Apopka N 0.04 N Blue Cypress S 0.44 N Brick S 1.20 N Bryant N 0.27 S Shuffle1df &lt;- data.frame(FloridaLakes$Lake, FloridaLakes$Location, FloridaLakes$AvgMercury, ShuffledLakes$Location) names(Shuffle1df) &lt;- c(&quot;Lake&quot;, &quot;Location&quot;, &quot;AvgMercury&quot;, &quot;Shuffled Location&quot;) LakesPerm &lt;- ggplot(data=Shuffle1df, aes(x=`Shuffled Location`, y=AvgMercury, fill=`Shuffled Location`)) + geom_boxplot() + geom_jitter() + ggtitle(&quot;Mercury Levels in Florida Lakes&quot;) + xlab(&quot;Location&quot;) + ylab(&quot;Mercury Level&quot;) + theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + coord_flip() LakesPerm LakesPermTable &lt;- Shuffle1df %&gt;% group_by(`Shuffled Location`) %&gt;% summarize(MeanHg=mean(AvgMercury), StDevHg=sd(AvgMercury), N=n()) kable(LakesPermTable) Shuffled Location MeanHg StDevHg N N 0.5642424 0.3259508 33 S 0.4660000 0.3647551 20 3.1.11 R Code for Permutation Test We’ll write a for loop to perform 10,000 permutations and record the value of \\(b_1\\) (the difference in sample means) for each simulation. b1 &lt;- Lakes_M$coef[2] ## record value of b1 from actual data ## perform simulation b1Sim &lt;- rep(NA, 10000) ## vector to hold results ShuffledLakes &lt;- FloridaLakes ## create copy of dataset for (i in 1:10000){ #randomly shuffle locations ShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] ShuffledLakes_M&lt;- lm(data=ShuffledLakes, AvgMercury ~ Location) #fit model to shuffled data b1Sim[i] &lt;- ShuffledLakes_M$coef[2] ## record b1 from shuffled model } NSLakes_SimulationResults &lt;- data.frame(b1Sim) #save results in dataframe 3.1.12 Permutation Tests Results NSLakes_SimulationResultsPlot &lt;- ggplot(data=NSLakes_SimulationResults, aes(x=b1Sim)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + geom_vline(xintercept=c(b1, -1*b1), color=&quot;red&quot;) + xlab(&quot;Lakes: Simulated Value of b1&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Distribution of b1 under assumption of no relationship&quot;) NSLakes_SimulationResultsPlot It appears unlikely that we would observe a difference in sample mean (\\(b_1\\)) as extreme as 0.27195 ppm by chance, if there is really no relationship between location and mercury level. 3.1.13 Conclusions Number of simulations (out of 10,000) resulting in difference in means more extreme than 0.27195. sum(abs(b1Sim) &gt; abs(b1)) ## [1] 39 Proportion of simulations resulting in difference in means more extreme than 0.27195. mean(abs(b1Sim) &gt; abs(b1)) ## [1] 0.0039 The probability of observing a difference in means as extreme as 0.27195 by chance, when there is no relationship between location and mercury level is very low. There is strong evidence of a relationship between location and mercury level. In this case, there is strong evidence that mercury level is higher in Southern Lakes than northern Lakes. 3.1.14 Hypothesis Testing Terminology We can think of the simulation as a test of the following hypotheses: Hypothesis 1: Among all Florida lakes, average mercury level is the same for lakes in Northern Florida, as in Southern Florida. (Thus the difference of 0.27 we observed in our data occurred just by chance). Hypothesis 2: Among all Florida lakes, there is a difference in average mercury level between lakes in Northern Florida and Southern Florida. The “no difference,” or “chance alone” hypothesis is called the null hypothesis. The other hypothesis is called the alternative hypothesis. We used \\(b_1\\) to measure difference in average mercury levels between the locations in our observed data. We found that the probability of observing a difference in means as extreme as 0.27 when Hypothesis 1 is true is very low (approximately 0.0023) The statistic used to measure the difference or relationship we are interested in is called a test statistic. In this case, the test statistic is the difference in sample means (\\(b_1\\)) The p-value is the probability of observing a test statistic as extreme or more extreme than we did due to chance, when the null hypothesis is true. - A low p-value provides evidence against the null hypothesis. - A high p-value means that the data could have plausibly been obtained when the null hypothesis is true, and thus the null hypothesis cannot be ruled out. - A high p-value does not mean that the null hypothesis is true or probably true. A p-value can only tell us the strength of evidence against the null hypothesis, and should never be interpreted as support for the null hypothesis. 3.1.15 How Low Should the p-value Be knitr::include_graphics(&quot;pvals.png&quot;) 3.1.16 Practical Importance A low p-value tells us that the difference in average Mercury levels that we saw in our sample is unlikely to have occurred by chance, providing evidence that there is indeed a difference in average Mercury levels between Northern and Southern lakes. The p-value does not tell us anything about the size of the difference! If the difference is really small (say 0.001 ppm), perhaps there is no need to worry about it. It’s possible to get a small p-value even when the true difference is very small (especially when our sample size is large). In addition to a p-value, we should consider whether a difference is big enough to be meaningful in a practical way, before making any policy decisions. For now, we can use the difference in sample means of 0.27 ppm as an estimate of the size of the difference. Based on our limited knowledge of mercury levels, this does seem big enough to merit further investigation, and possible action. 3.2 Test for Difference in Standard Deviation 3.2.1 Standard Deviation Northern and Southern Lakes Recall that in our sample, the standard deviation was higher for the lakes in Southern Florida than Northern Florida. Note: for a sample of \\(n\\) observations, \\(y_1, \\ldots, y_n\\), standard deviation is a measure of spread, is calculated using the formula: \\[ SD=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(y_i-\\bar{y})^2} \\] LakesBP &lt;- ggplot(data=FloridaLakes, aes(x=Location, y=AvgMercury, fill=Location)) + geom_boxplot() + geom_jitter() + ggtitle(&quot;Mercury Levels in Florida Lakes&quot;) + xlab(&quot;Location&quot;) + ylab(&quot;Mercury Level&quot;) + theme(axis.text.x = element_text(angle = 90)) + ylim(c(0, 1.5)) + coord_flip() LakesBP LakesTable &lt;- FloridaLakes %&gt;% group_by(Location) %&gt;% summarize(MeanHg=mean(AvgMercury), StDevHg=sd(AvgMercury), N=n()) kable(LakesTable) Location MeanHg StDevHg N N 0.4245455 0.2696652 33 S 0.6965000 0.3838760 20 Does this provide evidence that there is really more variability in mercury levels for lakes in Southern Florida than in Northern Florida, or could we have just by chance picked lakes with more variability in South Florida? 3.2.2 Hypotheses Null Hypothesis: Standard deviation in mercury levels among all lakes in Northern Florida is the same as the standard deviation in mercury levels among all lakes in Southern Florida. Alternative Hypothesis: Standard deviation in mercury levels among all lakes in Northern Florida is different than the standard deviation in mercury levels among all lakes in Southern Florida. 3.2.3 Permutation Test Steps Procedure: Randomly shuffle the locations of the lakes, so that any relationship between location and mercury level is due only to chance. Calculate the difference in standard deviation in mercury levels (i.e. value of \\(b_1\\)) in “Northern” and “Southern” lakes, using the shuffled data. Repeat steps 1 and 2 many (say 10,000) times, recording the difference in standard deviations each time. Analyze the distribution of differences in standard deviation, simulated under the assumption that there is no relationship between location and mercury level. Look whether the actual difference we observed is consistent with the simulation results. Question: Looking back at the 5 simulations performed in the previous section, does it seem plausible that we could have observed a difference in standard deviations as extreme as \\(0.3839-0.2697 = 0.1142\\) by chance? 3.2.4 R Code for Permutation Test SDTab &lt;- FloridaLakes %&gt;% group_by(Location) %&gt;% summarize(SD=sd(AvgMercury)) DiffSD &lt;- SDTab$SD[2] - SDTab$SD[1] ## perform simulation DiffSim &lt;- rep(NA, 10000) ## vector to hold results ShuffledLakes &lt;- FloridaLakes ## create copy of dataset for (i in 1:10000){ #randomly shuffle locations ShuffledLakes$Location &lt;- ShuffledLakes$Location[sample(1:nrow(ShuffledLakes))] SDTabSim &lt;- ShuffledLakes %&gt;% group_by(Location) %&gt;% summarize(SD=sd(AvgMercury)) DiffSim[i] &lt;- SDTabSim$SD[2] - SDTabSim$SD[1] #record difference in SD for simulated data } NSLakes_SDSimResults &lt;- data.frame(DiffSim) #save results in dataframe 3.2.5 Permutation Tests Results NSLakes_SDSimResultsPlot &lt;- ggplot(data=NSLakes_SDSimResults, aes(x=DiffSim)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + geom_vline(xintercept=c(DiffSD, -1*DiffSD), color=&quot;red&quot;) + xlab(&quot;Lakes: Difference in SD&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Distribution of Difference in SD under assumption of no relationship&quot;) NSLakes_SDSimResultsPlot Number of simulations (out of 10,000) resulting in standard deviations greater the 0.1142. sum(abs(DiffSim) &gt; abs(DiffSD)) ## [1] 601 Proportion of simulations (out of 10,000) resulting in standard deviations greater the 0.1142. mean(abs(DiffSim) &gt; abs(DiffSD)) ## [1] 0.0601 This p-value represents the probability of observing a difference in sample standard deviations as extreme as 0.1142 in a samples of size 33 and 20 by chance, if in fact, the standard deviation in mercury concentration levels is the same for lakes in Northern Florida as in Southern Florida. 3.2.6 Conclusions It is unlikely that we would observe a difference in standard deviations as extreme as 0.1142 by chance. There is evidence that lakes in Southern Florida exhibit more variability in mercury levels than lakes in Northern Florida (though the evidence is not as strong as it was when we were testing for a difference in means). Again, a p-value does not tell us whether a difference is practically meaningful. Without knowing a lot about mercury levels, and their impact on the ecosystem, it’s harder to tell wheter an estimated difference in standard deviations of 0.11 ppm is meaningful or not. It would be good to consult a biologist before making any decisions based on these results. 3.3 Test for Regression Slope 3.3.1 2015 Cars Dataset We consider data from the Kelly Blue Book, pertaining to new cars, released in 2015. We’ll investigate the relationship between price, length, and time it takes to accelerate from 0 to 60 mph. data(Cars2015) glimpse(Cars2015) ## Rows: 110 ## Columns: 20 ## $ Make &lt;fct&gt; Chevrolet, Hyundai, Kia, Mitsubishi, Nissan, Dodge, Chevrole… ## $ Model &lt;fct&gt; Spark, Accent, Rio, Mirage, Versa Note, Dart, Cruze LS, 500L… ## $ Type &lt;fct&gt; Hatchback, Hatchback, Sedan, Hatchback, Hatchback, Sedan, Se… ## $ LowPrice &lt;dbl&gt; 12.270, 14.745, 13.990, 12.995, 14.180, 16.495, 16.170, 19.3… ## $ HighPrice &lt;dbl&gt; 25.560, 17.495, 18.290, 15.395, 17.960, 23.795, 25.660, 24.6… ## $ Drive &lt;fct&gt; FWD, FWD, FWD, FWD, FWD, FWD, FWD, FWD, FWD, FWD, FWD, AWD, … ## $ CityMPG &lt;int&gt; 30, 28, 28, 37, 31, 23, 24, 24, 28, 30, 27, 27, 25, 27, 30, … ## $ HwyMPG &lt;int&gt; 39, 37, 36, 44, 40, 35, 36, 33, 38, 35, 33, 36, 36, 37, 39, … ## $ FuelCap &lt;dbl&gt; 9.0, 11.4, 11.3, 9.2, 10.9, 14.2, 15.6, 13.1, 12.4, 11.1, 11… ## $ Length &lt;int&gt; 145, 172, 172, 149, 164, 184, 181, 167, 179, 154, 156, 180, … ## $ Width &lt;int&gt; 63, 67, 68, 66, 67, 72, 71, 70, 72, 67, 68, 69, 70, 68, 69, … ## $ Wheelbase &lt;int&gt; 94, 101, 101, 97, 102, 106, 106, 103, 104, 99, 98, 104, 104,… ## $ Height &lt;int&gt; 61, 57, 57, 59, 61, 58, 58, 66, 58, 59, 58, 58, 57, 58, 59, … ## $ UTurn &lt;int&gt; 34, 37, 37, 32, 37, 38, 38, 37, 39, 34, 35, 38, 37, 36, 37, … ## $ Weight &lt;int&gt; 2345, 2550, 2575, 2085, 2470, 3260, 3140, 3330, 2990, 2385, … ## $ Acc030 &lt;dbl&gt; 4.4, 3.7, 3.5, 4.4, 4.0, 3.4, 3.7, 3.9, 3.4, 3.9, 3.9, 3.7, … ## $ Acc060 &lt;dbl&gt; 12.8, 10.3, 9.5, 12.1, 10.9, 9.3, 9.8, 9.5, 9.2, 10.8, 11.1,… ## $ QtrMile &lt;dbl&gt; 19.4, 17.8, 17.3, 19.0, 18.2, 17.2, 17.6, 17.4, 17.1, 18.3, … ## $ PageNum &lt;int&gt; 123, 148, 163, 188, 196, 128, 119, 131, 136, 216, 179, 205, … ## $ Size &lt;fct&gt; Small, Small, Small, Small, Small, Small, Small, Small, Smal… 3.3.2 Car Price and Acceleration Time LowPrice represents the price of a standard (non-luxury) model of a car. Acc060 represents time it takes to accelerate from 0 to 60 mph. data(Cars2015) CarsA060 &lt;- ggplot(data=Cars2015, aes(x=Acc060, y=LowPrice)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) CarsA060 3.3.3 Modeling Price using Acc060 \\(\\widehat{Price} = b_0 + b_1\\times\\text{Acc. Time}\\) Model assumes expected price is a linear function of acceleration time. Interpretations: \\(b_0\\) represents intercept of regression line, i.e. expected price of a car that can accelerate from 0 to 60 mph in no time. This is not a meaningful interpretation in context. \\(b_1\\) represents slope of regression line, i.e. expected change in price for each additional second it takes to accelerate from 0 to 60 mph. 3.3.4 Modeling for Car Price and Acceleration Cars_M_A060 &lt;- lm(data=Cars2015, LowPrice~Acc060) summary(Cars_M_A060) ## ## Call: ## lm(formula = LowPrice ~ Acc060, data = Cars2015) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.512 -6.544 -1.265 4.759 27.195 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 89.9036 5.0523 17.79 &lt;0.0000000000000002 *** ## Acc060 -7.1933 0.6234 -11.54 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.71 on 108 degrees of freedom ## Multiple R-squared: 0.5521, Adjusted R-squared: 0.548 ## F-statistic: 133.1 on 1 and 108 DF, p-value: &lt; 0.00000000000000022 3.3.5 Acc060 Model Interpretations \\(\\widehat{Price} = b_0 + b_1\\times\\text{Acc. Time}\\) \\(\\widehat{Price} = 89.90 - 7.193\\times\\text{Acc. Time}\\) Intercept \\(b_0\\) might be interpreted as the price of a car that can accelerate from 0 to 60 in no time, but this is not a meaningful interpretation since there are no such cars. \\(b_1=-7.1933\\) tells us that on average, the price of a car is expected to decrease by 7.19 thousand dollars for each additional second it takes to accelerate from 0 to 60 mph. \\(R^2 = 0.5521\\) tells us that 55% of the variation in price is explained by the linear model using acceleration time as the explanatory variable. 3.3.6 Is Car Price Associated with Acceleration Time? Is it possible that there is really no relationship between price and acceleration time, and we just happened to choose a sample that led to a slope of -7.1933, by chance? Is it possible that among all cars, the picture looks like the one below, and we just happened to draw a sample of 110 cars, showing a downward trend by chance? 3.3.7 Acc060 Key Question and Hypotheses If there is really no relationship between price and acceleration time, then we would expect a slope (i.e value of \\(b_1\\)) equal to 0. Key Question: How likely is it that we would have observed a slope (i.e. a value of \\(b_1\\)) as extreme as -7.1933 merely by chance, if there is really no relationship between price and acceleration time? Null Hypothesis: Among all 2015 cars, there is no relationship between price and acceleration time, and the slope we observed occurred merely by chance. Alternative Hypothesis: The slope we observed is due to more than chance, and there is a relationship between price and acceleration time among all 2015 cars. 3.3.8 Permutation Test for Slope Procedure: Randomly shuffle the acceleration times, so that any relationship between acceleration time and price is due only to chance. Fit a regression line to the shuffled data and record the slope of the regression line. Repeat steps 1 and 2 many (say 10,000) times, recording the slope (i.e. value of \\(b_1\\)) each time. Analyze the distribution of slopes, simulated under the assumption that there is no relationship between price and acceleration time. Look whether the actual slope we observed is consistent with the simulation results. 3.3.9 Five Permutations First Permutation ShuffledCars &lt;- Cars2015 ## create copy of dataset ShuffledCars$Acc060 &lt;- ShuffledCars$Acc060[sample(1:nrow(ShuffledCars))] Shuffle1df &lt;- data.frame(Cars2015$Make, Cars2015$Model, Cars2015$LowPrice, Cars2015$Acc060, ShuffledCars$Acc060) names(Shuffle1df) &lt;- c(&quot;Make&quot;, &quot;Model&quot;, &quot;LowPrice&quot;, &quot;Acc060&quot;, &quot;ShuffledAcc060&quot;) kable(head(Shuffle1df)) Make Model LowPrice Acc060 ShuffledAcc060 Chevrolet Spark 12.270 12.8 6.8 Hyundai Accent 14.745 10.3 10.1 Kia Rio 13.990 9.5 9.4 Mitsubishi Mirage 12.995 12.1 9.7 Nissan Versa Note 14.180 10.9 8.1 Dodge Dart 16.495 9.3 7.2 ggplot(data=Shuffle1df, aes(x=ShuffledAcc060, y=LowPrice)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) Slope of regression line from permuted data: M_Cars_Shuffle &lt;- lm(data=ShuffledCars, LowPrice~Acc060) summary(M_Cars_Shuffle)$coef[2] ## [1] 0.5801725 Second Permutation ShuffledCars &lt;- Cars2015 ## create copy of dataset ShuffledCars$Acc060 &lt;- ShuffledCars$Acc060[sample(1:nrow(ShuffledCars))] Shuffle1df &lt;- data.frame(Cars2015$Make, Cars2015$Model, Cars2015$LowPrice, Cars2015$Acc060, ShuffledCars$Acc060) names(Shuffle1df) &lt;- c(&quot;Make&quot;, &quot;Model&quot;, &quot;LowPrice&quot;, &quot;Acc060&quot;, &quot;ShuffledAcc060&quot;) kable(head(Shuffle1df)) Make Model LowPrice Acc060 ShuffledAcc060 Chevrolet Spark 12.270 12.8 6.9 Hyundai Accent 14.745 10.3 8.0 Kia Rio 13.990 9.5 10.5 Mitsubishi Mirage 12.995 12.1 7.2 Nissan Versa Note 14.180 10.9 8.4 Dodge Dart 16.495 9.3 11.0 ggplot(data=Shuffle1df, aes(x=ShuffledAcc060, y=LowPrice)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) Slope of regression line from permuted data: M_Cars_Shuffle &lt;- lm(data=ShuffledCars, LowPrice~Acc060) summary(M_Cars_Shuffle)$coef[2] ## [1] -0.9289392 Third Permutation ShuffledCars &lt;- Cars2015 ## create copy of dataset ShuffledCars$Acc060 &lt;- ShuffledCars$Acc060[sample(1:nrow(ShuffledCars))] Shuffle1df &lt;- data.frame(Cars2015$Make, Cars2015$Model, Cars2015$LowPrice, Cars2015$Acc060, ShuffledCars$Acc060) names(Shuffle1df) &lt;- c(&quot;Make&quot;, &quot;Model&quot;, &quot;LowPrice&quot;, &quot;Acc060&quot;, &quot;ShuffledAcc060&quot;) kable(head(Shuffle1df)) Make Model LowPrice Acc060 ShuffledAcc060 Chevrolet Spark 12.270 12.8 6.8 Hyundai Accent 14.745 10.3 8.7 Kia Rio 13.990 9.5 6.7 Mitsubishi Mirage 12.995 12.1 8.8 Nissan Versa Note 14.180 10.9 7.2 Dodge Dart 16.495 9.3 5.5 ggplot(data=Shuffle1df, aes(x=ShuffledAcc060, y=LowPrice)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) Slope of regression line from permuted data: M_Cars_Shuffle &lt;- lm(data=ShuffledCars, LowPrice~Acc060) summary(M_Cars_Shuffle)$coef[2] ## [1] -1.428531 3.3.10 Fourth Permutation ShuffledCars &lt;- Cars2015 ## create copy of dataset ShuffledCars$Acc060 &lt;- ShuffledCars$Acc060[sample(1:nrow(ShuffledCars))] Shuffle1df &lt;- data.frame(Cars2015$Make, Cars2015$Model, Cars2015$LowPrice, Cars2015$Acc060, ShuffledCars$Acc060) names(Shuffle1df) &lt;- c(&quot;Make&quot;, &quot;Model&quot;, &quot;LowPrice&quot;, &quot;Acc060&quot;, &quot;ShuffledAcc060&quot;) kable(head(Shuffle1df)) Make Model LowPrice Acc060 ShuffledAcc060 Chevrolet Spark 12.270 12.8 7.4 Hyundai Accent 14.745 10.3 7.0 Kia Rio 13.990 9.5 7.7 Mitsubishi Mirage 12.995 12.1 7.6 Nissan Versa Note 14.180 10.9 6.2 Dodge Dart 16.495 9.3 8.7 ggplot(data=Shuffle1df, aes(x=ShuffledAcc060, y=LowPrice)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) Slope of regression line from permuted data: M_Cars_Shuffle &lt;- lm(data=ShuffledCars, LowPrice~Acc060) summary(M_Cars_Shuffle)$coef[2] ## [1] 0.01197376 Fifth Permutation ShuffledCars &lt;- Cars2015 ## create copy of dataset ShuffledCars$Acc060 &lt;- ShuffledCars$Acc060[sample(1:nrow(ShuffledCars))] Shuffle1df &lt;- data.frame(Cars2015$Make, Cars2015$Model, Cars2015$LowPrice, Cars2015$Acc060, ShuffledCars$Acc060) names(Shuffle1df) &lt;- c(&quot;Make&quot;, &quot;Model&quot;, &quot;LowPrice&quot;, &quot;Acc060&quot;, &quot;ShuffledAcc060&quot;) kable(head(Shuffle1df)) Make Model LowPrice Acc060 ShuffledAcc060 Chevrolet Spark 12.270 12.8 8.8 Hyundai Accent 14.745 10.3 7.0 Kia Rio 13.990 9.5 7.5 Mitsubishi Mirage 12.995 12.1 7.8 Nissan Versa Note 14.180 10.9 8.1 Dodge Dart 16.495 9.3 7.9 ggplot(data=Shuffle1df, aes(x=ShuffledAcc060, y=LowPrice)) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=FALSE) Slope of regression line from permuted data: M_Cars_Shuffle &lt;- lm(data=ShuffledCars, LowPrice~Acc060) summary(M_Cars_Shuffle)$coef[2] ## [1] -0.4476822 3.3.11 R Code for Permutation Test b1 &lt;- Cars_M_A060$coef[2] ## record value of b1 from actual data ## perform simulation b1Sim &lt;- rep(NA, 10000) ## vector to hold results ShuffledCars &lt;- Cars2015 ## create copy of dataset for (i in 1:10000){ #randomly shuffle acceleration times ShuffledCars$Acc060 &lt;- ShuffledCars$Acc060[sample(1:nrow(ShuffledCars))] ShuffledCars_M&lt;- lm(data=ShuffledCars, LowPrice ~ Acc060) #fit model to shuffled data b1Sim[i] &lt;- ShuffledCars_M$coef[2] ## record b1 from shuffled model } Cars_A060SimulationResults &lt;- data.frame(b1Sim) #save results in dataframe 3.3.12 Permutation Test Results b1 &lt;- Cars_M_A060$coef[2] ## record value of b1 from actual data Cars_A060SimulationResultsPlot &lt;- ggplot(data=Cars_A060SimulationResults, aes(x=b1Sim)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + geom_vline(xintercept=c(b1, -1*b1), color=&quot;red&quot;) + xlab(&quot;Simulated Value of b1&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Distribution of b1 under assumption of no relationship&quot;) Cars_A060SimulationResultsPlot It is extremely unlikely that we would observe a value of \\(b_1\\) as extreme as -7.1933 by chance, if there is really no relationship between price and acceleration time. 3.3.13 P-value and Conclusion Proportion of simulations resulting in simulation value of \\(b_2\\) more extreme than -7.1933. mean(abs(b1Sim) &gt; abs(b1)) ## [1] 0 The p-value represents the probability of observing a slope as extreme or more extreme than -7.1933 by chance when there is actually no relationship between price and acceleration time. The probability of observing a slope as extreme as -7.1933 by chance, when there is no relationship between location and mercury level practically zero. There is very strong evidence of a relationship between price and acceleration time. A low p-value tells us only that there is evidence of a relationship, not that it is practically meaningful. But an estimated difference of more than $7 thousand for each additional second seems pretty important and would likely influence a buyer’s decision. 3.4 Test for Comparing Multiple Groups 3.4.1 Relationship Price and Car Size Continuing with the sample of 110 cars, seen in the previous section, let’s compare prices of small, midsized, and large cars. ggplot(data=Cars2015, aes(x=Size, y=LowPrice, fill=Size)) + geom_boxplot() + geom_jitter() + coord_flip() Cars2015 %&gt;% group_by(Size) %&gt;% summarize(MeanPrice = mean(LowPrice), StDevPrice=sd(LowPrice), N=n()) ## # A tibble: 3 × 4 ## Size MeanPrice StDevPrice N ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Large 42.3 17.9 29 ## 2 Midsized 33.2 12.0 34 ## 3 Small 26.7 14.4 47 3.4.2 Cars Questions of Interest Do the data provide evidence of a relationship between price and size of vehicle? Is there evidence of a difference in average price between… large and midsized cars? large and small cars? small and midsized cars? 3.4.3 Cars Price and Size Model Cars_M_Size = lm(data=Cars2015, LowPrice~Size) summary(Cars_M_Size) ## ## Call: ## lm(formula = LowPrice ~ Size, data = Cars2015) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.516 -11.190 -4.005 9.064 57.648 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.311 2.737 15.460 &lt; 0.0000000000000002 *** ## SizeMidsized -9.098 3.725 -2.442 0.0162 * ## SizeSmall -15.659 3.480 -4.499 0.0000174 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.74 on 107 degrees of freedom ## Multiple R-squared: 0.1593, Adjusted R-squared: 0.1436 ## F-statistic: 10.14 on 2 and 107 DF, p-value: 0.00009271 3.4.4 Relationship between Size and Price Do the data provide evidence of a relationship between price and size of vehicle? \\(\\widehat{\\text{Price}} = b_0 +b_1\\times\\text{I}_{\\text{Midsized}}+ b_2\\times\\text{I}_{\\text{Large}}\\) \\(b_0\\) represents expected price of large cars. \\(b_1\\) represents expected difference in price between large and midsized cars. \\(b_2\\) represents expected difference in price between large and small cars. Unfortunately, none of these measure whether there is an overall relationship between price and size. Question:What statistic can we use to assess the size of differences between more than two groups? 3.4.5 Test Statistic for Car Size and Price Cars_A_Size &lt;- aov(data=Cars2015, LowPrice~Size) summary(Cars_A_Size) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Size 2 4405 2202.7 10.14 0.0000927 *** ## Residuals 107 23242 217.2 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 3.4.6 Key Question in Car Size Investigation Null Hypothesis: Average price, among all 2015 cars, is the same between small, midsized, and large cars. Alternative Hypothesis: Average price among all 2015 cars differs between at least two of these sizes. Key Question: How likely is it that we would have obtained an F-statistic as extreme as 10.14 by chance, if there is really no difference in price between small, medium, and large sized cars, among all 2015 cars? 3.4.7 Simulation-Based Test for F-Statistic We’ll simulate situations where there is no relationship between size and price, and see how often we observe an F-statistic as extreme as 10.14. Procedure: Randomly shuffle the sizes of the vehicles, so that any relationship between size and price is due only to chance. Fit a model, using the shuffled data, with price as the response variable, and size as the explanatory variable. Record the F-statistic. Repeat steps 1 and 2 many (say 10,000) times, recording the F-statistic each time. Analyze the distribution of F-statistics, simulated under the assumption that there is no relationship between size and price. Look whether the actual F-statistic we observed is consistent with the simulation results. 3.4.8 Five Permutations First Permutation ShuffledCars &lt;- Cars2015 ## create copy of dataset ShuffledCars$Size &lt;- ShuffledCars$Size[sample(1:nrow(ShuffledCars))] Shuffle1df &lt;- data.frame(Cars2015$Make, Cars2015$Model, Cars2015$LowPrice, Cars2015$Size, ShuffledCars$Size) names(Shuffle1df) &lt;- c(&quot;Make&quot;, &quot;Model&quot;, &quot;LowPrice&quot;, &quot;Size&quot;, &quot;ShuffledSize&quot;) kable(head(Shuffle1df)) Make Model LowPrice Size ShuffledSize Chevrolet Spark 12.270 Small Small Hyundai Accent 14.745 Small Large Kia Rio 13.990 Small Large Mitsubishi Mirage 12.995 Small Midsized Nissan Versa Note 14.180 Small Midsized Dodge Dart 16.495 Small Small Recall this model was fit under an assumption of no relationship between price and size. ggplot(data=ShuffledCars, aes(x=Size, y=LowPrice, fill=Size)) + geom_boxplot() + geom_jitter() + coord_flip() + ggtitle(&quot;Shuffled Cars&quot;) Cars_A_Size_Shuffle &lt;- aov(data=ShuffledCars, LowPrice~Size) summary(Cars_A_Size_Shuffle) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Size 2 746 372.9 1.483 0.232 ## Residuals 107 26902 251.4 Second Permutation ShuffledCars &lt;- Cars2015 ## create copy of dataset ShuffledCars$Size &lt;- ShuffledCars$Size[sample(1:nrow(ShuffledCars))] Shuffle1df &lt;- data.frame(Cars2015$Make, Cars2015$Model, Cars2015$LowPrice, Cars2015$Size, ShuffledCars$Size) names(Shuffle1df) &lt;- c(&quot;Make&quot;, &quot;Model&quot;, &quot;LowPrice&quot;, &quot;Size&quot;, &quot;ShuffledSize&quot;) kable(head(Shuffle1df)) Make Model LowPrice Size ShuffledSize Chevrolet Spark 12.270 Small Large Hyundai Accent 14.745 Small Small Kia Rio 13.990 Small Large Mitsubishi Mirage 12.995 Small Large Nissan Versa Note 14.180 Small Midsized Dodge Dart 16.495 Small Large ggplot(data=ShuffledCars, aes(x=Size, y=LowPrice, fill=Size)) + geom_boxplot() + geom_jitter() + coord_flip() + ggtitle(&quot;Shuffled Cars&quot;) Cars_A_Size_Shuffle &lt;- aov(data=ShuffledCars, LowPrice~Size) summary(Cars_A_Size_Shuffle) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Size 2 688 344.1 1.366 0.26 ## Residuals 107 26960 252.0 Third Permutation ShuffledCars &lt;- Cars2015 ## create copy of dataset ShuffledCars$Size &lt;- ShuffledCars$Size[sample(1:nrow(ShuffledCars))] Shuffle1df &lt;- data.frame(Cars2015$Make, Cars2015$Model, Cars2015$LowPrice, Cars2015$Size, ShuffledCars$Size) names(Shuffle1df) &lt;- c(&quot;Make&quot;, &quot;Model&quot;, &quot;LowPrice&quot;, &quot;Size&quot;, &quot;ShuffledSize&quot;) kable(head(Shuffle1df)) Make Model LowPrice Size ShuffledSize Chevrolet Spark 12.270 Small Small Hyundai Accent 14.745 Small Large Kia Rio 13.990 Small Small Mitsubishi Mirage 12.995 Small Small Nissan Versa Note 14.180 Small Midsized Dodge Dart 16.495 Small Midsized ggplot(data=ShuffledCars, aes(x=Size, y=LowPrice, fill=Size)) + geom_boxplot() + geom_jitter() + coord_flip() + ggtitle(&quot;Shuffled Cars&quot;) Cars_A_Size_Shuffle &lt;- aov(data=ShuffledCars, LowPrice~Size) summary(Cars_A_Size_Shuffle) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Size 2 42 21.01 0.081 0.922 ## Residuals 107 27606 258.00 Fourth Permutation ShuffledCars &lt;- Cars2015 ## create copy of dataset ShuffledCars$Size &lt;- ShuffledCars$Size[sample(1:nrow(ShuffledCars))] Shuffle1df &lt;- data.frame(Cars2015$Make, Cars2015$Model, Cars2015$LowPrice, Cars2015$Size, ShuffledCars$Size) names(Shuffle1df) &lt;- c(&quot;Make&quot;, &quot;Model&quot;, &quot;LowPrice&quot;, &quot;Size&quot;, &quot;ShuffledSize&quot;) kable(head(Shuffle1df)) Make Model LowPrice Size ShuffledSize Chevrolet Spark 12.270 Small Small Hyundai Accent 14.745 Small Small Kia Rio 13.990 Small Midsized Mitsubishi Mirage 12.995 Small Small Nissan Versa Note 14.180 Small Small Dodge Dart 16.495 Small Midsized Recall this model was fit under an assumption of no relationship between price and size. ggplot(data=ShuffledCars, aes(x=Size, y=LowPrice, fill=Size)) + geom_boxplot() + geom_jitter() + coord_flip() + ggtitle(&quot;Shuffled Cars&quot;) Cars_A_Size_Shuffle &lt;- aov(data=ShuffledCars, LowPrice~Size) summary(Cars_A_Size_Shuffle) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Size 2 885 442.6 1.77 0.175 ## Residuals 107 26763 250.1 Fifth Permutation ShuffledCars &lt;- Cars2015 ## create copy of dataset ShuffledCars$Size &lt;- ShuffledCars$Size[sample(1:nrow(ShuffledCars))] Shuffle1df &lt;- data.frame(Cars2015$Make, Cars2015$Model, Cars2015$LowPrice, Cars2015$Size, ShuffledCars$Size) names(Shuffle1df) &lt;- c(&quot;Make&quot;, &quot;Model&quot;, &quot;LowPrice&quot;, &quot;Size&quot;, &quot;ShuffledSize&quot;) kable(head(Shuffle1df)) Make Model LowPrice Size ShuffledSize Chevrolet Spark 12.270 Small Midsized Hyundai Accent 14.745 Small Small Kia Rio 13.990 Small Small Mitsubishi Mirage 12.995 Small Midsized Nissan Versa Note 14.180 Small Midsized Dodge Dart 16.495 Small Large ggplot(data=ShuffledCars, aes(x=Size, y=LowPrice, fill=Size)) + geom_boxplot() + geom_jitter() + coord_flip() + ggtitle(&quot;Shuffled Cars&quot;) Cars_A_Size_Shuffle &lt;- aov(data=ShuffledCars, LowPrice~Size) summary(Cars_A_Size_Shuffle) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Size 2 745 372.3 1.481 0.232 ## Residuals 107 26903 251.4 3.4.9 R Code For Permutation Test We’ll simulate 10,000 permutations and record the F-statistic for each set of permuted data. Fstat &lt;- summary(Cars_M_Size)$fstatistic[1] ## record value of F-statistic from actual data ## perform simulation FSim &lt;- rep(NA, 10000) ## vector to hold results ShuffledCars &lt;- Cars2015 ## create copy of dataset for (i in 1:10000){ #randomly shuffle acceleration times ShuffledCars$Size &lt;- ShuffledCars$Size[sample(1:nrow(ShuffledCars))] ShuffledCars_M&lt;- lm(data=ShuffledCars, LowPrice ~ Size) #fit model to shuffled data FSim[i] &lt;- summary(ShuffledCars_M)$fstatistic[1] ## record F from shuffled model } CarSize_SimulationResults &lt;- data.frame(FSim) #save results in dataframe 3.4.10 F-statistic for Size Simulation Results CarSize_SimulationResults_Plot &lt;- ggplot(data=CarSize_SimulationResults, aes(x=FSim)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + geom_vline(xintercept=c(Fstat), color=&quot;red&quot;) + xlab(&quot;Simulated Value of F&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Distribution of F under assumption of no relationship&quot;) CarSize_SimulationResults_Plot p-value: mean(FSim &gt; Fstat) ## [1] 0 The p-value represents the probability of observing an F-statistic as extreme as 10.14 by chance, in samples of size 29, 34, and 47, if in fact there is no relationship between price and size of car. The data provide strong evidence of a relationship between price and size. 3.4.11 Differences Between Different Sizes Now that we have evidence that car price is related to size, we might want to know which sizes differ from each other. Is there evidence of a difference in average price between… a) large and midsized cars? b) large and small cars? c) small and midsized cars? 3.4.12 Regression Coefficients for Tests Between Sizes \\(\\widehat{\\text{Price}} = b_0 +b_1\\times\\text{I}_{\\text{Midsized}}+ b_2\\times\\text{I}_{\\text{Small}}\\) \\(b_0\\) represents expected price of large cars. \\(b_1\\) represents expected difference in price between large and midsized cars. \\(b_2\\) represents expected difference in price between large and small cars. Thus, we can answer each question by looking at the appropriate regression coefficient. a) large and midsized cars? (\\(b_1\\)) b) large and small cars? (\\(b_2\\)) c) small and midsized cars? (\\(b_1-b_2\\)) 3.4.13 Simulation for Differences between Types of Cars We’ll simulate situations where there is no relationship between size and price, and see how often we observe results for \\(b_1\\), \\(b_2\\), and \\(b_1-b_2\\) as extreme as we did in the actual data. Procedure: Randomly shuffle the sizes of the vehicles, so that any relationship between size and price is due only to chance. Fit a model, using the shuffled data, with price as the response variable, and size as the explanatory variable. Record the values of \\(b_1\\), \\(b_2\\), and \\(b_1-b_2\\). Repeat steps 1 and 2 many (say 10,000) times, recording the values of \\(b_1\\), \\(b_2\\), and \\(b_1-b_2\\) each time. Analyze the distribution of \\(b_1\\), \\(b_2\\), \\(b_1-b_2\\), simulated under the assumption that there is no relationship between size and price. Look whether the actual values we observed are consistent with the simulation results. 3.4.14 Code for Simulation-Based Test of Prices by Size b1 &lt;- Cars_M_Size$coefficients[2] #record b1 from actual data b2 &lt;- Cars_M_Size$coefficients[3] #record b2 from actual data ## perform simulation b1Sim &lt;- rep(NA, 10000) ## vector to hold results b2Sim &lt;- rep(NA, 10000) ## vector to hold results ShuffledCars &lt;- Cars2015 ## create copy of dataset for (i in 1:10000){ #randomly shuffle acceleration times ShuffledCars$Size &lt;- ShuffledCars$Size[sample(1:nrow(ShuffledCars))] ShuffledCars_M&lt;- lm(data=ShuffledCars, LowPrice ~ Size) #fit model to shuffled data b1Sim[i] &lt;- ShuffledCars_M$coefficients[2] ## record b1 from shuffled model b2Sim[i] &lt;- ShuffledCars_M$coefficients[3] ## record b2 from shuffled model } Cars_Size2_SimulationResults &lt;- data.frame(b1Sim, b2Sim) #save results in dataframe 3.4.15 Car Size Simulation-Based Results for \\(b_1\\) Cars_Size2_SimulationResultsPlot_b1 &lt;- ggplot(data=Cars_Size2_SimulationResults, aes(x=b1Sim)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + geom_vline(xintercept=c(b1, -1*b1), color=&quot;red&quot;) + xlab(&quot;Simulated Value of b1&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Large vs Midsize Cars: Distribution of b1 under assumption of no relationship&quot;) Cars_Size2_SimulationResultsPlot_b1 p-value: mean(abs(b1Sim)&gt;abs(b1)) ## [1] 0.0243 The p-value represents the probability of observing a difference in mean prices as extreme as 9.1 by chance, in samples of size 29 and 34 cars, if in fact there is no difference in average prices of large and midsized cars. 3.4.16 Car Size Simulation-Based Results for \\(b_2\\) Cars_Size2_SimulationResultsPlot_b2 &lt;- ggplot(data=Cars_Size2_SimulationResults, aes(x=b2Sim)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + geom_vline(xintercept=c(b2, -1*b2), color=&quot;red&quot;) + xlab(&quot;Simulated Value of b2&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Large vs Small Cars: Distribution of b2 under assumption of no relationship&quot;) Cars_Size2_SimulationResultsPlot_b2 p-value: mean(abs(b2Sim)&gt;abs(b2)) ## [1] 0 The p-value represents the probability of observing a difference in mean prices as extreme as 15.4 by chance, in samples of size 29 and 47 cars, if in fact there is no difference in average prices of large and small cars. 3.4.17 Car Size Simulation-Based Results for \\(b_1-b_2\\) Cars_Size2_SimulationResultsPlot_b1_b2 &lt;- ggplot(data=Cars_Size2_SimulationResults, aes(x=b1Sim-b2Sim)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + geom_vline(xintercept=c(b1-b2, -1*(b1-b2)), color=&quot;red&quot;) + xlab(&quot;Simulated Value of b1-b2&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Small vs Midsize Cars: Distribution of b1-b2 under assumption of no relationship&quot;) Cars_Size2_SimulationResultsPlot_b1_b2 p-value: mean(abs(b1Sim-b2Sim)&gt;abs(b1-b2)) ## [1] 0.0666 The p-value represents the probability of observing a difference in mean prices as extreme as 6.5 by chance, in samples of size 34 and 47 cars, if in fact there is no difference in average prices of midsized and small cars. 3.4.18 Bonferroni Correction We might normally conclude that there is evidence of differences in group means in the p-value is less than 0.05. However, since we are performing multiple tests simultaneously, there is an increased chance that at least one of them will yield a small p-value just by chance. Thus, we should be more strict in deciding what constitutes evidence against the null hypothesis. A commone rule (known as the Bonferroni correction) is to divide the values usually used as criteria for evidence by the number of tests. In this example: - Here, we would say there is some evidence of differences between groups if the p-value is less than 0.10/3=0.0333. - We would say there is strong evidence of differences if the p-value is less than 0.05/3=0.0167 Comparison Coefficient p-value Evidence of Difference large vs midsize \\(b_1\\) 0.0243 Some evidence large vs small \\(b_2\\) 0 Strong evidence small vs midsize \\(b_1-b_2\\) 0.0666 No evidence 3.4.19 Summary of Tests Between Multiple Groups When testing for differences between more than two groups: Perform an overall test, using the F-statistic. A large F-statistic and small p-value tell us there is evidence of differences between at least some of the groups. If the F-tests yields evidence evidence of differences, perform tests on individual model coefficients to determine which groups differ. Use a more strict cutoff criteria, such as the Bonferroni correction. 3.4.20 Bear Weights by Season ggplot(data=Bears_Subset, aes(y=Weight, x=Season, fill=Season)) + geom_boxplot() + geom_jitter() + coord_flip() 3.4.21 Bear Weights by Season Model Bears_M_Season &lt;- lm(data=Bears_Subset, Weight~Season) summary(Bears_M_Season) ## ## Call: ## lm(formula = Weight ~ Season, data = Bears_Subset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -178.84 -79.84 -29.02 54.98 309.16 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 204.84 17.16 11.939 &lt;0.0000000000000002 *** ## SeasonSpring -37.27 34.62 -1.076 0.284 ## SeasonSummer -29.81 24.71 -1.206 0.231 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 112.5 on 94 degrees of freedom ## Multiple R-squared: 0.02034, Adjusted R-squared: -0.0005074 ## F-statistic: 0.9757 on 2 and 94 DF, p-value: 0.3807 3.4.22 F-Statistic for Bear Weights by Season Bears_A_Season &lt;- aov(data=Bears_Subset, Weight~Season) summary(Bears_A_Season) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Season 2 24699 12350 0.976 0.381 ## Residuals 94 1189818 12658 3.4.23 Hypotheses for Bears Seasons F-Test Null Hypothesis: Among all bears of this type, mean weight is the same in each season. Alternative Hypothesis: Among all bears of this type, mean weight differs between at least two of the seasons. Key Question: What is the probability of observing an F-statistic as extreme as 0.976 if there is really no relationship between weight and season? 3.4.24 Simulation-Based Test for Bears F-Statistic We’ll simulate situations where there is no relationship between size and price, and see how often we observe an F-statistic as extreme as 0.976. Procedure: Randomly shuffle the seasons, so that any relationship between weight and season is due only to chance. Fit a model, using the shuffled data, with weight as the response variable, and season as the explanatory variable. Record the F-statistic. Repeat steps 1 and 2 many (say 10,000) times, recording the F-statistic each time. Analyze the distribution of F-statistics, simulated under the assumption that there is no relationship between season and weight Look whether the actual F-statistic we observed is consistent with the simulation results. 3.4.25 Bears F-Statistic Simulation Fstat &lt;- summary(Bears_M_Season)$fstatistic[1] ## record value of F-statistic from actual data ## perform simulation FSim &lt;- rep(NA, 10000) ## vector to hold results ShuffledBears &lt;- Bears_Subset ## create copy of dataset for (i in 1:10000){ #randomly shuffle acceleration times ShuffledBears$Season &lt;- ShuffledBears$Season[sample(1:nrow(ShuffledBears))] ShuffledBears_M&lt;- lm(data=ShuffledBears, Weight ~ Season) #fit model to shuffled data FSim[i] &lt;- summary(ShuffledBears_M)$fstatistic[1] ## record F from shuffled model } Bears_Seasons_SimulationResults &lt;- data.frame(FSim) #save results in dataframe 3.4.26 F-statistic for Bears Season Simulation Bears_Seasons_SimulationResultsPlot &lt;- ggplot(data=Bears_Seasons_SimulationResults, aes(x=FSim)) + geom_histogram(fill=&quot;lightblue&quot;, color=&quot;white&quot;) + geom_vline(xintercept=c(Fstat), color=&quot;red&quot;) + xlab(&quot;Simulated Value of F&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;Distribution of F under assumption of no relationship&quot;) Bears_Seasons_SimulationResultsPlot mean(FSim &gt; Fstat) ## [1] 0.3762 The p-value represents the probability of observing and F-statistic as extreme as 0.976 by chance, in a sample of 97, if in fact there is no difference in average weights of bears between seasons. It is not at all unusual to observe an F-statistic as extreme or more extreme than we did if there is really no relationship between weight and season. There is no evidence that average bear weights differ between seasons. 3.4.27 Don’t Accept Null Hypothesis In the previous example, we concluded that there is no evidence that average bear weights differ between seasons. This is different than saying that bear weights are the same in each season. Why would it be inappropriate to say this? 3.4.28 Comparison of Weights by Season Bears_Season_Table &lt;- Bears_Subset %&gt;% group_by(Season) %&gt;% summarize(MeanWeight = mean(Weight), StDevWeight = sd(Weight), N=n()) kable(Bears_Season_Table) Season MeanWeight StDevWeight N Fall 204.8372 125.71414 43 Spring 167.5714 108.74155 14 Summer 175.0250 97.70796 40 3.4.29 Don’t Accept Null Hypothesis (Cont.) The data do show differences in average weight between seasons. It’s just that we can’t rule out the possibility that these differences are due to chance alone. A hypothesis test can only tell us the strength of evidence against the null hypothesis. The absence of evidence against the null hypothesis should not be interpreted as evidence for the null hypothesis. We should never say that the data support/prove/confirm the null hypothesis. We can only say that the data do not provide evidence against the null hypothesis. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
